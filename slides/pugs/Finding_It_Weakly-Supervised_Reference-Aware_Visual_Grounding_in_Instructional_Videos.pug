+slide
section#Finding_It_Weakly-Supervised_Reference-Aware_Visual_Grounding_in_Instructional_Videos
  .paper-abstract
    .title Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos
    .info
      .authors De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, Juan Carlos Niebles
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p 言語的な文脈の中で指示語からそれが何であるかを特定する問題（Visual Grounding; 「それを取ってください」の「それ」を動画中から探索するなど）を扱う論文である。この問題に対してMIL（Multiple Instance Learning）を参考にした弱教師付き学習であるReference-aware MIL（RA-MIL）を用いて解決する。
    .item2
      .text
        p
          img(src=`${figpath}180518VisualGrounding.png`,alt="180518VisualGrounding")
    .item3
      .text
        h1 新規性・結果・なぜ通ったか？
        p 画像に対するVisual Groundingが空間的な関係性を捉えるのに対して、Visual Groundingは時間的な関係性を捉える課題である。YouCookII/RoboWatch datasetにて処理を行った結果、弱教師付き学習であるRA-MILを適用するとVisual Groundingに対して精度向上することを明らかにした。
    .item4
      .text
        h1 コメント・リンク集
        p Language and Visionの課題はすでに動画にまで及んでいる。Visual Groundingのみならず、新規問題設定を試みた論文として精読してもよいかも？それと視覚と言語のサーベイ論文は読んでみたい
        ul
          li
            a(href="http://ai.stanford.edu/~dahuang/papers/cvpr18-ramil.pdf") 論文
          li
            a(href="http://ai.stanford.edu/~dahuang/") 著者
          li
            a(href="http://aclweb.org/anthology/D15-1021") 視覚と言語のサーベイ論文
    .slide_index #{getSlideIndex()}
    .timestamp 2018.5.18 16:30:52
