+slide
section#ID_Quantization_and_Training_of_Neural_Networks_for_Efficient_Integer-Arithmetic-Only_Inference
  .paper-abstract
    .title Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
    .info
      .authors Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p Integer(int)演算によるニューラルネットの効率的な量子化および学習の提案である。Int演算でFloat演算よりも効率的な計算を可能とした。同様に、End-to-End学習についても精度を保持しつつ演算の高速化にも成功、accuracy/latencyのトレードオフについても効率的な解決策となった。関連研究であるMobileNetについても効率化に成功し、ImageNet/MSCOCOにてCPU実装をデモした。
    .item2
      .text
        p
          img(src=`${figpath}180723IntOnly.png`,alt="180723IntOnly")
    .item3
      .text
        h1 新規性・結果・なぜ通ったか？
        p 8ビットInt型の演算のみでニューラルネットの学習を実施。学習/推論においてFloat型の精度/速度を凌駕する性能を発揮した。さらに、MobileNet等の効率化されたアーキテクチャについてもより効率化を実現した。
    .item4
      .text
        h1 コメント・リンク集
        p さすがにGoogleは保有データのみでなく、アルゴリズム面においてもトップを行っている。データあり、資源あり、人ありでその上分野を網羅的に攻めることができている。
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf") 論文
          li
            a(href="https://www.slideshare.net/DeepLearningJP2016/dlquantization-and-training-of-neural-networks-for-efficient-integerarithmeticonly-inference") SlideShare
          li
            a(href="https://www.tensorflow.org/") TensorFlow
    .slide_index #{getSlideIndex()}
    .timestamp 2018.7.23 14:22:37
