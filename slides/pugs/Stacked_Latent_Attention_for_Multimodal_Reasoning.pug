+slide
section#ID_Stacked_Latent_Attention_for_Multimodal_Reasoning
  .paper-abstract
    .title Stacked Latent Attention for Multimodal Reasoning
    .info
      .authors Haoqi Fan, Jiatong Zhou
      .conference CVPR 2018
    .slide_editor: a(href="http://hirokatsukataoka.net/" target="blank") Hirokatsu Kataoka

    .item1
      .text
        h1 概要
        p アテンションモデルの改善を行い、VQAに適用する。現在のアテンションに関する弱点は（１）中間層では対応関係といった理由づけに関する情報を除去してしまう（２）StackedAttentionでは局所最適解に陥ってしまうことを挙げた。本論文ではこの問題を解決するため、明示的に中間的な理由づけに関する構造を加えたStacked Latent Attention Modelを提案。マルチモーダルのReasoningに有効であることがわかり、VQAにおいても効果的な手法となった。
    .item2
      .text
        p
          img(src=`${figpath}180704StackedLatentAttentionModel.png`,alt="180704StackedLatentAttentionModel")
    .item3
      .text
        h1 新規性・結果・なぜ通ったか？
        p 構造をスタックしてより良好なアテンションにしていくモデルを構築した。空間的な理由づけ（Reasoning）を潜在的に行うモデルであり、マルチモーダルであるVQAや画像説明文にも効果的である。
    .item4
      .text
        h1 コメント・リンク集
        p アテンションは論文数増加していて、各方面に広がってきた。
        ul
          li
            a(href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf") 論文
          li
            a(href="https://research.fb.com/publications/stacked-latent-attention-for-multimodal-reasoning/") Project
    .slide_index #{getSlideIndex()}
    .timestamp 2018.7.4 08:35:08


