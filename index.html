<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>cvpaper.challenge</title><link rel="stylesheet" href="css/reveal.css"><link rel="stylesheet" href="css/theme/white.css"><link rel="stylesheet" href="css/layout.css"><link rel="stylesheet" href="lib/css/zenburn.css"></head><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-118576057-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-118576057-1');
</script><body><div class="reveal"><div class="slides"><section id="Can_Spatiotemporal_3D_CNNs_Retrace_the_History_of_2D_CNNs_and_ImageNet"><div class="paper-abstract"><div class="title">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</div><div class="info"><div class="authors">Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh</div><div class="conference">CVPR 2018</div><div class="paper_id">arXiv:1711.09577</div></div><div class="slide_editor">Tenga Wakamiya</div><div class="item1"><div class="text"><h1>概要</h1><p>動画データセット上の比較的浅いものから非常に深いものまでの様々な3DCNNの構造を調べた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Can_Spatiotemporal_3D_CNNs_Retrace_the_History_of_2D_CNNs_and_ImageNet.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ResNet-18の学習は，UCF-101，HMDB-51，およびActivityNetの過学習していて，Kineticsは過学習しなかった．</li><li>Kineticsのデータセットは，深い層の3DCNNで学習するために十分なデータがあり，ImageNetの2D ResNetsと同様に，最大152のResNets層の学習を可能にし，ResNeXt-101は，Kineticsのテストセットで平均78.4％の精度がある．</li><li>UCF-101およびHMDB-51上の複雑な2DアーキテクチャよりもKineticsの事前学習されたシンプルな3Dアーキテクチャが優れていて，UCF-101およびHMDB-51でそれぞれ94.5％および70.2％を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09577">論文</a></li></ul></div></div><div class="slide_index">[#1]</div><div class="timestamp">2018.5.29 15:59:46</div></div></section><section id="Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation"><div class="paper-abstract"><div class="title">Conditional Generative Adversarial Network for Structured Domain Adaptation</div><div class="info"><div class="authors">W.Hong, Z.Wang, M.Yang and J.Yuan</div><div class="conference">CVPR2018</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>コンピュータによって学習用のアノテーションを生成し，実画像のような合成画像として用いることが流行．しかし，ドメインの不一致という問題が起きる．それを解決するために，GANをFCNフレームワークに統合することでSemanticSegmentationのためのドメイン適用のための手法を提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>合成画像の特徴を実画像のように変換する条件付きジェネ−レータとディスクリメーターを学習</li><li>ジェネレータは合成画像を実画像のようにディスクリメーターを騙すように学習させることでFCNのパラメータを更新．</li><li>本手法である実際のラベルを用いずに実験を行い，CityscapesデータセットのIoU平均が12〜20上回りSoTA．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>FCN＋GANでSemanticSegmentation</li><li><a href="https://weixianghong.github.io/publications/papers/CVPR_18.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#2]</div><div class="timestamp">2018.5.28 15:36:39</div></div></section><section id="Learning_to_Sketch_with_Shortcut_Cycle_Consistency"><div class="paper-abstract"><div class="title">Learning to Sketch with Shortcut Cycle Consistency</div><div class="info"><div class="authors">Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像からスケッチのストロークを取得する手法の提案。人間が画像からスケッチをすると、同じ画像に対しても様々なバリエーションが生じてしまう。
そこで、教師有学習と教師無学習を組み合わせることによって画像からスケッチの取得を実現する。
教師有学習は、画像からスケッチもしくはスケッチから画像という変換を学習する。
教師無学習は、オートエンコーダのように画像もしくはスケッチを符号化し、元に戻すという処理を学習する。
その際、CycleGANのようにドメイン変換を繰り返すのではなく、符号化したものをそのまま復号化する(Shortcut Cycle)。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Sketch_with_Shortcut_Cycle_Consistency.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Pix2pixやCycleGANなどの手法と比較を行い、いずれの手法と比較してもスケッチとして抽象化されつつもセマンティックな特徴を捉えていることを確認した。また、数値評価としてスケッチの認識及び検索タスクを行って評価した。
どちらのタスクにおいても、従来手法と比較して高い精度でスケッチへの変換ができていることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.00247">論文</a></li></ul></div></div><div class="slide_index">[#3]</div><div class="timestamp">2018.5.29 14:10:18</div></div></section><section id="Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration"><div class="paper-abstract"><div class="title">Show Me a Story: Towards Coherent Neural Story Illustration</div><div class="info"><div class="authors">Hareesh Ravi, Lezi Wang, Carlos M. Muniz, Leonid Sigal, Dimitris N. Metaxas, Mubbasir Kapadia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数の文で構成されたテキストの内容を表す画像シークエンスを検索する手法を提案。文章から抽出される特徴と画像から抽出された特徴を対応付けることにより、各文に対して1枚の画像を選択する。
その際、文章特徴はGRUによって前後の文章との関係を含めて抽出する。
また、heやitなどの代名詞が何を指しているかを明らかにするために、テキスト全体としての一貫性を測るcoherence vectorを導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ベースラインとなる手法では、文単位で画像の検索を行っているために画像シークエンスとしての一貫性が損なわれてしまう。そこで、GRU及びcoherence vectorによって前後の文で登場した単語などを考慮することが可能となり、テキスト全体を表す画像シークエンスの検索が可能となった。
ユーザースタディにより、ベースライン、coherence vector無し、coherence vector有りの比較を行い、coherence vector有りが最も好まれる結果を得た。
また、画像シークエンスがテキストに合っているかは主観的な評価であるため、saliencyベースの新たな評価指標を提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.cs.ubc.ca/~lsigal/Publications/cvpr2018ravi.pdf">論文URL</a></li><li><a href="https://arxiv.org/abs/1511.06361">ベースライン</a></li></ul></div></div><div class="slide_index">[#4]</div><div class="timestamp">2018.5.29 12:03:27</div></div></section><section id="SO-Net_Self_Organizing_Network_for_Point_Cloud_Analysis"><div class="paper-abstract"><div class="title">SO-Net: Self-Organizing Network for Point Cloud Analysis</div><div class="info"><div class="authors">Jiaxin Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>順序構造に対して不変な３次元 Point Cloud のための deep learning アーキテクチャー SO-Net を提案. Self-Organizing Map (SOM) を作ることで点群の空間分布をモデル化し, SOMのノードを用いて階層的な特徴量の抽出を行う. Point Cloud のクラス分類やセグメンテーションなどのタスクを用いた評価実験では, 先行研究と同等以上の結果をより短い学習時間で達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png" alt="fukuhara-fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SOM を用いることで Point Cloud を複数の Point Cloud の部分集合に分割し, 各部分集合ごとの特徴量を抽出した後, 全体の特徴量を階層的に抽出する.</li><li>初期ノードの位置を固定し, 学習を batch 単位で行うことで, SOM の学習が順序構造に対して不変となるようにしている.</li><li>様々なタスクの事前学習として用いるための Point Cloud の autoencoder を提案. </li><li>ネットワークの構造が単純かつ並列計算可能なため, 先行研究よりも短時間で学習をすることが可能.</li><li>point cloud reconstruction, classification, object part segmentation, shape retrieval などの複数のタスクを用いて評価実験を行った.</li><li>評価実験の結果では Point-Net++ や Kd-Net などとの先行研究と同等以上の結果を半分以下の学習時間で達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04249" target="blank">[論文] SO-Net: Self-Organizing Network for Point Cloud Analysis</a></li><li><a href="https://github.com/lijx10/SO-Net" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#5]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="Large-scale_Point_Cloud_Semantic_Segmentation_with_Superpoint_Graphs"><div class="paper-abstract"><div class="title">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</div><div class="info"><div class="authors">Yoshihiro Fukuhara et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模(数百万規模)な point clouds データに対して効率的に Semantic Segmentation を行う研究. まず, point clouds 全体を形状が単純で, 意味的に同じ点が属する部分集合(superpoint）に分類し, superpoint が作るグラフ（SPG）に graph convorution を適用することで segmentation を行う. Semantic3D と S3DIS dataset を用いた評価実験では先行研究よりも良い結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png" alt="fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>superpoint の構成は先行研究(Guinard+17)で提案された, Global Energy を用いて行う.</li><li>各 superpoint の特徴量を PointNet を用いて抽出する. (大規模なデータを扱うため, 各 superpoint 内でダウンサンプリングを行っている.) </li><li>抽出された各 superpoint の特徴量に対して Gated Recurrent Unit (GRU) を用いた graph convorution を適用することで, 各 superpoint のクラス分類を行う.</li><li>Semantic3D と S3DIS dataset を用いた評価実験では, ShapeNet などの先行研究と比較して複数の評価尺度で最も優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09869" target="blank">[論文] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</a></li><li><a href="https://github.com/loicland/superpoint_graph" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#6]</div><div class="timestamp">2018.5.28 00:47:55</div></div></section><section id="FoldingNet_Point_Cloud_Auto_encoder_via_Deep_Grid_Deformation"><div class="paper-abstract"><div class="title">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</div><div class="info"><div class="authors">Yaoqing Yang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>3次元点群処理のための autoencoder を提案. Folding という新しい decoding 演算を導入することで, 2次元グリッド上の点から3次元点群の表面上への射影を教師なしで学習した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png" alt="fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新しい end-to-end な3次元点群処理のための deep autoencoder を提案した.</li><li>提案手法のdecoderのパラメータ数は既存手法の7%であるが, これで2次元グリッドと任意の3次元点群表面への写像が構成できることを理論的に証明した.</li><li>MN40 や MN10 dataset を用いた classification タスクの評価実験では, 最先端の教師あり手法（Achlioptas+17）などと同等の精度を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07262" target="blank">[論文] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</a></li><li><a href="https://www.youtube.com/watch?v=csC6SodV6vk" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#7]</div><div class="timestamp">2018.5.22 12:19:55</div></div></section><section id="FFNet_Video_Fast-Forwarding_via_Reinforcement_Learning"><div class="paper-abstract"><div class="title">FFNet: Video Fast-Forwarding via Reinforcement Learning</div><div class="info"><div class="authors">Shuyue Lan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video Fast-forwarding のタスクを MDP(Markov Decision Process) として定式化し, 強化学習を用いて解く方法を提案. 評価実験では精度と効率の両方に置いて先行研究よりも優れた結果を示した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png" alt="fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video Fast-forwarding を MDP (Markov Decision Process) として定式化した.</li><li>現在の Frame の特徴量を状態, スキップする Frame 数を行動として, Q-learningで強化学習を行う. </li><li>報酬はスキップした Frame の中に重要なものがどの程度含まれていたかに基づいて計算される.</li><li>Tour20 や TVSum dataset を用いた先行研究との比較実験では, 主観評価と定量的評価の両方に置いて最も良い結果となった.(6-20%程度、重要なframeを含んでいる割合が増加)</li><li>先行研究と比較して80%近く処理するフレーム数を削減し, 効率化することに成功した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vcg.engr.ucr.edu/publications/Shuyue_CVPR.pdf" target="blank">[論文] FFNet: Video Fast-Forwarding via Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#8]</div><div class="timestamp">2018.5.17 17:25:55</div></div></section><section id="Egocentric_Activity_Recognition_on_a_Budget"><div class="paper-abstract"><div class="title">Egocentric Activity Recognition on a Budget</div><div class="info"><div class="authors">Rafael Possas et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>ウェアラブルデバイスのような使用可能な電力が限られる状況において, 電力消費と精度を強化学習を用いてバランスするフレームワークを提案. 複数のセンサー情報を用いた行動認識のタスクにおいて, 高精度・高電力消費な predictor と低精度・低電力消費な predictor を強化学習の結果に基づいて適宜切り替えることで少ない消費電力で先行研究と同等の精度を達成した. また, 一人称視点動画行動認識のための新しいデータセットを作成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png" alt="fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ウェアラブルカメラの情報を用いた高精度・高コストな predictor とモーションセンサーの情報を用いた低精度・低コストな predictor のどちらを使用して推定を行うべきかを A3C の agent が判断する.</li><li>どちらのセンサーの情報を用いても正しい推定結果となるような状況では低精度・低コストな predictor を使用した場合に大きな報酬が得られるように agent の学習を行う.</li><li>提案手法では報酬についてのパラメータ１つを調整する事で精度と消費電力の簡単なトレードオフが可能.</li><li>一人称視点動画行動認識のための新しいデータセット（DataEgo）を作成.</li><li>Multimodal egocentric dataset を用いた評価実験では従来手法(Song+16)とほぼ同等の精度を少ない消費電力で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://web.it.usyd.edu.au/~framos/Publications_files/egocentric-activity-recognition%20%282%29.pdf" target="blank">[論文] Egocentric Activity Recognition on a Budget</a></li></ul></div></div><div class="slide_index">[#9]</div><div class="timestamp">2018.5.19 13:40:55</div></div></section><section id="A2-RL_Aesthetics_Aware_Reinforcement_Learning_for_Image_Cropping"><div class="paper-abstract"><div class="title">A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</div><div class="info"><div class="authors">Debang Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習 (A3C) を用いて Image cropping を行う手法を提案. 従来の sliding winodow に基づく手法のように膨大な数の cropping 候補を評価する必要がないため, 先行研究よりも短時間で結果の計算が可能. また, 評価実験では精度についても先行研究よりも優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png" alt="fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Image cropping を sequential decision-making process として定式化した. (14種類の cropping を action として, Markov 過程としてモデル化.)</li><li>上記の問題を A3C を用いた強化学習を用いて解いた. </li><li>報酬については学習済みの View Finding Network (Chen＋2017）を使用.</li><li>各ステップで候補となる cropping の種類の数が少ないため, 先行研究と比較して非常に短い計算時間で結果を出力することが可能となった.</li><li>Flickr Cropping Dataset, CUHK Image Cropping Dataset, Human Cropping Dataset を用いて行った評価実験ではいずれも先行研究よりも優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1709.04595.pdf" target="blank">[論文] A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</a></li><li><a href="https://github.com/wuhuikai/TF-A2RL" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#10]</div><div class="timestamp">2018.5.22 18:27:55</div></div></section><section id="Bonnet_An_Open-Source_Training_and_Deployment_Framework_for_Semantic_Segmentation_in_Robotics_using_CNNs"><div class="paper-abstract"><div class="title">Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs</div><div class="info"><div class="authors">A. Miliot and C. Stachniss</div><div class="conference">CVPR2018</div><div class="paper_id">177</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>現在，ロボットの制御に不可欠な周辺環境を認識するシステムとしてCNNs(Convolution Neural Networks)が重要視されている．しかし，CNNsの利用はCV系の専門ではない人々にとってとても扱いづらいというギャップが存在した．
そこで，CV分野とロボット工学とのギャップを埋めることを目標として，CNNsを利用したSemantic Segmantationを扱えるオープンソースのフレームワーク, Bonnetを提供した．
PythonでTensorflowを利用し，データセットからピクセル単位のSemanticクラスを推測するトレーニングを行う．
そして既存のC++アプリケーション，またはROS(Robot Operating System)が使用できるロボットで，訓練モデルから推測することができるC++インターフェースを用いて実際のロボットプラットフォームに展開する．本手法の評価はCityscapes，Synthia，Crop-Weedの3種類のデータセットでの正解率や実行時間を評価している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180177_Bonnet.png" alt="Item3Image"><img src="slides/figs/180177_frameworks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CVにおける新規性は少ないが，ロボット工学において最先端のセマンティックセグメンテーションなどを容易に扱うことを可能とした．これにより，ロボット工学だけでなく他分野でのCNNsの利用が容易となり，より新たな考えを生み出すことができるだけでなく，CVでの様々な手法が利用される可能性が高まると考えられる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.08960">arXiv</a></li><li><a href="http://www.ipb.uni-bonn.de/2018/03/2018-03-code-available-bonnet-tensorflow-convolutional-semantic-segmentation-pipeline-by-andres-milioto-and-cyrill-stachniss/">Project Page</a></li><li><a href="https://www.youtube.com/watch?v=lBCT5eMiDqA">Bonnet: Prediction of Cityscapes(Youtube)</a></li></ul></div></div><div class="slide_index">[#11]</div><div class="timestamp">2018.5.26 15:36:41</div></div></section><section id="Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs"><div class="paper-abstract"><div class="title">Good View Hunting: Learning Photo Composition from Dense View Pairs</div><div class="info"><div class="authors">Zijun Wei, Jianming Zhang, Xiaohui Shen, Zhe Lin, Radomir Mech, Minh Hoai, Dimitris Samaras</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像の構図の良し悪しを評価するComparative Photo Compositionデータセットを構築。10800枚の画像から24の構図の画像を作成し、クラウドソーシングによって2つの構図のどちらがいいかをアノテーションした。
また、入力画像をどのようにクロッピングすると良い構図になるかを提示するシステムを構築した。
その際、IOUを評価尺度にすると構図的に評価が低いものも高いスコアになるため、画像を評価するネットワークから得られるスコアを指標とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のデータセットでは画像に対してスコアがついていたのに対して、構図の異なる2枚の画像どちらがいいかを100万ペアアノテーションを行った。構図推薦システムは、ユーザースタディの結果従来手法よりも良いと感じる人が多いことを確認した。
また、計算速度も従来手法と比べはるかに向上した(75FPS+)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.zijunwei.org/VPN_CVPR2018.html">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#12]</div><div class="timestamp">2018.5.28 00:50:47</div></div></section><section id="DVQA_Understanding_Data_Visualization_via_Question_Answering"><div class="paper-abstract"><div class="title">DVQA: Understanding Data Visualization via Question Answering</div><div class="info"><div class="authors">Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan</div><div class="conference">CVPR 2018</div><div class="paper_id">694</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規なバーグラフに対して質問回答タスクDVQA及びデータセットの提案．</li><li>バーグラフが情報の一つとしてより豊かな統計的な情報を表現できる．提案手法がバーグラフを対象としたDVQAを提案し，バーグラフの自動的情報抽出と理解を可能にした．</li><li>大規模なバーグラフQAデータセットDVQAを提案した．DVQAが3Mのグラフ‐質問ペアから構成され，バーグラフに対し３種類の質問(構造理解，データ検索，reasoning)を設定した．また，全部の質問がopen-endedである．</li><li>DVQAタスクにおいて，2種類のネットワーク構造を提案した．①MOM:グラフの局所領域を抽出し文章を生成ことにより回答できる問題を対応するネットワークboundingbox OCR及びグラフの局所領域を抽出せずに回答する一般的な問題を対応するClassifierの二つのサブネットから構成される．どのネットにより回答するかを2クラス分類問題として取り扱っている②SANDY:従来手法SANにダイナミックエンコーディングモデルを用いて，質問文中のchart-specific単語をエンコーディングし，それをベースに直接chart-specificな回答文を生成できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DVQA.png" alt="DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用性が高い新規なバーグラフに対し質問回答タスクを提案．</li><li>提案データセットDVQAに対し5種類の従来のVQA手法と提案のMOM,SANDYの比較実験を行った．一般的問題・chart-specific問題の両方に対し提案のSANDYモデルが最も良い精度を達成した．</li><li>提案のデータセットDVQAがバーグラフの理解と質問文・回答文によりバーグラフ自動生成に用いられる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VQAタスクのVを画像からバーグラフに変更し実用性が高い提案である．</li><li>類似した考えで従来の”V”か“Q”か“A”を同じ処理で別の似た概念に変更する研究をするも面白そう</li><li><a href="https://arxiv.org/abs/1801.08163">論文</a></li></ul></div></div><div class="slide_index">[#13]</div><div class="timestamp">2018.5.25 17:28:12</div></div></section><section id="RotationNet_Joint_Object_Categorization_and_Pose_Estimation_Using_Multiviews_from_Unsupervised_Viewpoints"><div class="paper-abstract"><div class="title">RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints</div><div class="info"><div class="authors">Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida</div><div class="conference">CVPR 2018</div><div class="paper_id">628</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>物体のマルチ視点の画像からジョイントで3D姿勢推定及び物体認識を行う手法RotationNetの提案．</li><li>3D MFPにより作成されたマルチ視点画像データセットMIROを提案した．(12classes, 10 instances/class,160viewpoints)</li><li>物体を観測する視点及び物体のカテゴリをジョイントで推定した方がより良い精度を達成できると指摘し，更にトレーニングする際に物体を観測する視点をlatent variablesとして取り扱い，視点unalignedな学習データセットからunsupervisedで物体の姿勢推定を学習する．</li><li>また，視点-specificな特徴をクラス内だけではなく，異なるクラス間の姿勢アライメントを行う．</li><li>RotationNetのネットワーク構造はマルチ視点の画像から画像ごとにそ全部の視点の確率(その画像がその視点であるか)及び物体カテゴリを予測し，全部の画像から予測した結果から正解ラベルのクラスの確率＊視点の確率の統合を最大化するように学習する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RotationNet.png" alt="RotationNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体認識においてはSHREC’17のnormalデータに対し優勝した．また，ModelNet-10,ModelNet-40に対し従来のマルチ視点・ポイントクラウド・ボクセルベースな様々な手法より良い精度を達成．</li><li>物体姿勢推定において，無監督な方法で従来の監督方法レベルな結果が得られた．</li><li>実環境で，良い姿勢な画像をと撮影できるとは限らない．RotationNetで物体の姿勢及び認識を行う際，画像枚数（＞＝１）で動作でき，観測が更新したら予測結果を更新する．そのため，RotationNetはAR応用などの実環境の応用に適応する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラス間のViewpoint-specificな特徴を学習することが面白い．可視化手法を加えて学習済みモデルに対しどういうようにアライメントしているのかを知りたい．また，問題定義を詳細的に考える必要がありそう</li><li>疑問点としては予測したそれぞれの視点の結果の統合は平均をとる？</li><li><a href="https://arxiv.org/abs/1603.06208">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">コード</a></li></ul></div></div><div class="slide_index">[#14]</div><div class="timestamp">2018.5.25 17:21:58</div></div></section><section id="Visual_to_Sound_Generating_Natural_Sound_for_Videos_in_the_Wild"><div class="paper-abstract"><div class="title">Visual to Sound: Generating Natural Sound for Videos in the Wild</div><div class="info"><div class="authors">Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara Berg</div><div class="conference">CVPR 2018</div><div class="paper_id">435</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>ビデオからリアルな音声を生成する(waveformな)手法及びビデオ―音声データセットを提案した．</li><li>人がビジョンとサウンド間の関連性をある程度把握できる．そこで，in-the-wildビデオから音声(waveform型)を自動生成するタスクを提案し，また，このタスクのためのデータセットVEGASを提案した．VEGASはAudioSetデータセットをAMTよりクリーンし，10カテゴリのビデオ及び対応した音声28109ペアから構成される．データセットのビデオの総時間が55時間となる．</li><li>提案タスクに対応したフレームワークはビデオエンコーダー及び音声ジェネレータから構成される．音声ジェネレータは階層的RNNを用いた．ビデオエンコーダーに対し:①frame-to-frame②sequence-to-sequence③flow-basedの３種類の設計を用いた．3種類モデルの生成結果に対し定量評価及びヒューマンテストを用いて評価し，flow-based構造が最も良い性能とヒューマン評価を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualToSound_InTheWild.png" alt="VisualToSound_InTheWild"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のビデオから音声を生成する手法はビデオに対し拘束条件を加えている．提案手法は初めてのin-the-wildビデオから音声を生成する手法．</li><li>ビデオから音声を自動生成する手法の応用場面が広い．(VRシステムでの没入感の増強，音声編集作業の自動化，視覚障害の人に視覚体験を聴覚体験として提供)</li><li>ヒューマンテスト (ビデオがリアルかフェクか)に対し，ビデオエンコーダーをflow-basedな構造を用いた場合，平均73.36%の生成音声がリアル音声と評価された．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・視覚情報の抽出機に更にコンテンツと物体relationなどを重視したネットワークを用いたら更なる良い結果が得られそう・逆設定として，音声情報からビデオの予測も面白そう</p><ul><li><a href="https://arxiv.org/abs/1712.01393">論文</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/562">コード</a></li></ul></div></div><div class="slide_index">[#15]</div><div class="timestamp">2018.5.25 17:15:56</div></div></section><section id="Functional_Map_of_the_World"><div class="paper-abstract"><div class="title">Functional Map of the World</div><div class="info"><div class="authors">Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee</div><div class="conference">CVPR 2018</div><div class="paper_id">795</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>建物や土地などの機能的目的を予測するタスクに用いられる大規模な衛星画像データセットfMoWの提案(bounding box, 時系列，カテゴリ，メタ情報などのアノテーションがあり)</li><li>データセットの具体的な統計情報は①200以上の国の１,047,691 枚画像②63カテゴリ③一枚の画像1つ以上のバウンディングボクス定義④時系列画像が大量に含む．</li><li>このデータセットに対応した新たなタスクを設定した：連続な時系列画像によりバウンディングボクス内の物体を認識する．提案データセットfMoWを用いて5つのネットワーク構造:LSTM-M,CNN-I,CNN-IM,LSTM-I,LSTM-IM(I:画像M:メタ特徴)に対し比較実験を行た．平均F1スコアにおいてLSTM-IMが最も高い精度を示したので，時系列情報及びメタ情報をジョイントでreasoningするアプローチの有効性を証明した</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FunctionalMapOfTheWorld.png" alt="FunctionalMapOfTheWorld"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>公開されている最も大規模な衛星画像データセット．</li><li>異なる国・撮影時間・撮影年代などで撮影された画像から構成され，提案データセットを統計比較などにも用いられる．</li><li>従来の衛星画像データセットは主にbrief momentsの情報だけをキャプチャーし，メタ情報(ロケーション，時間，太陽角度など)がアノテーションされていない．提案データセットはメタ情報をアノテーションし，様々な応用を可能にした．(例：パーキングエリアの時系列駐車量の統計・影と時間情報によりオブジェクトの高さ推定など)</li><li>検出と識別タスクの間に位置付ける新たな問題設定“時系列画像のバウンディングボックス内の物体識別”をして，更に実験を通してメタ情報と時系列情報をジョイントで処理することの重要性を示した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>地理情報に関する分析の研究に用いられるデータセット</p></li><li><p>国のバリエーションが豊かなデータセットなので，国ごと上空シーン特徴の比較などにも用いられる</p></li><li><p><a href="https://arxiv.org/abs/1711.07846">論文</a></p></li><li><p><a href="https://github.com/fMoW">コード</a></p></li><li><p><a href="https://github.com/fMoW/dataset">データセット</a></p></li><li><p><a href="https://www.iarpa.gov/index.php/working-with-iarpa/prize-challenges/1015-functional-map-of-the-world-fmow">fMoW Challenge</a></p></li></ul></div></div><div class="slide_index">[#16]</div><div class="timestamp">2018.5.25 17:05:45</div></div></section><section id="Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift"><div class="paper-abstract"><div class="title">Deep Cocktail Networks: Multi-source Unsupervised Domain Adaptation with Category Shift</div><div class="info"><div class="authors">Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメインのラベル付きデータセットが複数ある場合のunsupervised domain adaptation(UDA)であるmultiple domain adaptation(MDA)によってターゲットドメインのクラシフィケーションを行う
Deep Cocktail Network(DCTN)を提案。MDAではUDAで問題視されるドメインシフトに加えて、
ソースドメインのデータセット間で全てのカテゴリが共有されていないカテゴリシフトが存在する。
DCTNでは、k番目のソースドメインのデータセットとターゲットドメインのデータセットを入力として
discriminatorによってperplexity scoreを算出することでどのソースドメインのデータセットの分布に近いかを算出し、
これを全てのソースドメインのデータセットに対して行い、perplexity scoreを重み付けるすることで最終的な識別結果を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>discriminatorによってターゲットドメインがソースドメインのデータセットのうちどのデータの分布に近いかを計算することで、MDAに取り組むDCTNを提案。</li><li>3つのベンチマークにおいてUDAのstate-of-the-artと比較し他結果、提案手法が最も高い精度を達成。</li><li>カテゴリシフトを解決できているかどうかを確認するために、ターゲットドメイン内でカテゴリの重複あり/なしにおける識別結果を比較したところ、
state-of-the-artと同等以上の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>discriminatorが算出したperplexity scoreによって重み付けをするというシンプルな手法だが、UDAに取り組むstate-of-the-artよりも高い精度を達成している。</li><li><a href="https://arxiv.org/abs/1803.00830">論文</a></li></ul></div></div><div class="slide_index">[#17]</div></div></section><section id="Unsupervised_Correlation_Analysis"><div class="paper-abstract"><div class="title">Unsupervised Correlation Analysis</div><div class="info"><div class="authors">Yedid Hoshen, Lior Wolf</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>2つのドメインを結合する手法であるCanonical Correlation Analysis(CCA、正準相関分析)を教師なし学習に対して行うUnsupervised Correlation Analysis(UCA)を提案。
既存のCCAは教師あり学習かつ2つのドメインが何らかの対応関係を持っていることを前提としていたが、
UCAは教師なし学習かつ2つのドメインに対応関係がない場合を想定している。
教師あり学習とは異なり、トレーニング時に2つのドメインにおける相関係数を計算することができないため、入力する2つのドメインと、
ネットワークによって射影された潜在変数空間の3つのドメイン間の射影、逆射影がうまくいくように様々なロスをとることで学習を行う。
ロスに対するablationも行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Correlation_Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>教師なしかつ2つのドメインに対応関係がない状況におけるCCAの拡張であるUCAを提案。</li><li>評価尺度として潜在変数空間における相関係数、AUCを用いて以下の5つの状況で実験を行なった。1.MNISTの画像とそのミラー画像、2.MNISTの上半分の画像と下半分の画像、3.鳥の画像とそのキャプション、4.花の画像とそのキャプション、5.Flickerの画像とそれに付随する5つの文章。
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li><li>教師なし学習の結果をGANと比較しており、全ての実験においてGANよりも高い精度を達成。</li><li>教師あり学習をUCAで行なった結果も乗せられており、実験3、４、5において通常のCCAよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>現状のネットワークを見ると、それぞれのドメインにおける直交性と、それぞれのドメインの射影先が同じ空間になるように様々なロスをとっているだけなので、
もう少しアップデートすることができるかもしれない。</li><li>CCAの特徴であるL_Orthだけを除いた場合に、どれほどの影響が出るのかが気になった。</li><li><a href="https://arxiv.org/abs/1804.00347">論文</a></li></ul></div></div><div class="slide_index">[#18]</div></div></section><section id="Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification"><div class="paper-abstract"><div class="title">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</div><div class="info"><div class="authors">Jingya Wang, Xiatian  Zhu, Shaogang Gong, Wei Li </div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなしデータセットにおいてperson re-identification(re-id)を教師なしで行うために、ラベルありデータセットからdomain adaptationを行うTransferable Joint Attribute-Identity 
Deep Learning(TJ-AIDL)を提案。person re-idとは、街中の監視カメラのような異なる視点、
重複のない領域を撮影された映像内の同一人物を探すことである。
TJ-AIDLにはアイデンティティーを推定するIdentity branch、アトリビュートを推定するAttribute branch、
アトリビュートからアイデンティティーを推定するモジュールであるIdentity Inferred Attirbute(IIA)からなる。
domain adaptationの際には、Attribute branch、IIAの更新のみを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>domain adaptationを用いて教師なしでperson re-idを行うために、画像のアトリビュートからアイデンティティーを推定するTJ-AIDLを提案。</li><li>personn re-idのベンチマークである4つのデータセットを使用しており、Rank-1mAPにおいてre-idを教師なしで行うstate-of-the-artよりも高い精度を達成。</li><li>TJ-AIDLにおいてアトリビュート/アイデンティティーのみ学習した際の結果、adaptation有り/無しの結果についても議論しており、提案したTJ-AIDLが最も高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.09786">論文</a></li></ul></div></div><div class="slide_index">[#19]</div></div></section><section id="Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Duplex Generative Adversarial Network for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Lanqing Hu, Meina Kan, Shiguang Shan, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>同一カテゴリのdomain間におけるadaptation, transferをラベル識別と2つのdiscriminatorを用いるネットワークDupGANを提案。target domainにはラベルがない状況である教師なし学習を対象としている。
DupGANはencoderでそれぞれのドメインの潜在変数をエンコードし、generatorでデコードを行い、
2つのdiscriminatorでそれぞれのドメインに対してfake/realとラベルの認識を行う。
結果はdomain transferされた数字画像のラベル認識・生成結果、物体認識の精度において比較を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル認識と2つのdiscriminatorによってdomain adaptaion/transferをおこなうDupGANを提案。</li><li>既存手法である<a href="http://jmlr.org/papers/volume17/15-239/15-239.pdf">DANN</a>、<a href="https://arxiv.org/abs/1702.05464">ADDA</a>はadversarial lossを使用してtarget→source のマッピングを行うが、
これらの手法ではマッピングされたtarget domainの分布が歪んでいないことは保証できない。
一方DupGANではラベルの認識を行わせることでカテゴリ構造を保つことができる。また提案手法では画像の生成も可能である。</li><li>state-of-the-artと比較して、数字画像データセットであるMNIST、USPS、SVHN、SVHN-extraそれぞれのデータセット間におけるdomain transferに対するラベル認識の結果、
最も高い精度を達成。またdomain transferによる画像も生成することが可能。</li><li>31種類のラベル、3つのドメインを持つOffice-31データセットにおける物体認識結果がstate-of-the-artよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラシフィケーション生成された画像ではなくはエンコードされた潜在変数に対して行われている。</li><li>画像の生成力はそこまで高くなく、実際Office31に対する画像生成は難しかったと主張している。</li><li><a href="http://vipl.ict.ac.cn/uploadfile/upload/2018041610083083.pdf">論文</a></li></ul></div></div><div class="slide_index">[#20]</div></div></section><section id="Pixels_voxels_and_views_A_study_of_shape_representations_for_single_view_3D_object_shape_prediction"><div class="paper-abstract"><div class="title">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</div><div class="info"><div class="authors">Daeyun Shin, Charless Fowlkes, Derek Hoiem</div><div class="conference">CVPR 2018</div><div class="paper_id">384</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚の画像から3次元形状を推定するタスクにおいて，異なる形状representation及びcoordinate framesを用いた場合，精度がどのように変化するのかの徹底的比較実験に関する研究．</li><li>従来形状推定タスクにおいて異なる設計の比較分析の研究がないので，著者達が異なる設計を比較できるフレームワーク及び具体的な実験を行った．</li><li>比較実験は具体的に，a.RGB画像b.デプス画像からの形状推定タスクにおいて，“①マルチサーフェス画像VS volumetricデータ表示②viewer-centered VS object-centeredな座標”などの設定に対し，定量的及び定性的な比較実験を行った．</li><li>提案の比較用フレームワークはencoder-decoderベースなネットワークを用いて，decoderに変更を加えることで， マルチサーフェス画像及び volumetricデータの2種類を生成できるようにした．また，coordinate frameをスイッチすることにより，viewer/object centeredを変更できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Study_Of_Shape_Representations.png" alt="A_Study_Of_Shape_Representations"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3次元形状推定タスクにおいて，異なる設定の比較実験を行った．</li><li>形状representationの設定において，Multi-surfaceの方がvoxel と比べunseenクラスにおいてより良い性能を達成した． Multi-surfaceの方が高い解像度をエンコーディングできるのが理由な可能性があると指摘した．</li><li>coordinate framesの設定において，意外なことに従来広く採用されているobject-centeredはviewer-centeredと比べunseenクラスにおいて精度が劣っていて， object-centeredの方がカテゴリ認識に対応が強いのが原因となることを指摘した．</li><li>以上の結論を元に，object centeredなsurface-basedな1枚の画像から3次元形状推定の手法3D-R2N2を提案し，PASCAL 3D+データセットにおいてmean IoU0.414を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>比較をしていない設計(Oct-tree based representationなど)もあるので，そういった構造に対して比較実験を行うのも面白い．</p></li><li><p>3次元あたりの徹底的比較を行って，何らかの結論を出すような研究がまだ少ないので，研究テーマを沢山作れるかも？</p></li><li><p><a href="https://arxiv.org/abs/1804.06032">論文</a></p></li></ul></div></div><div class="slide_index">[#21]</div><div class="timestamp">2018.5.24 18:20:50</div></div></section><section id="PlaneNet_Piece-wise_Planar_Reconstruction_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image</div><div class="info"><div class="authors">Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa</div><div class="conference">CVPR 2018</div><div class="paper_id">336</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚のRGB画像から“piece-wise planar depthmap”を推定するend-to-endなネットワークを提案した．提案手法を用いてRGB画像から平面パラメータ及び平面セグメンテーションマスク及びデプスマップを同時に推定できる．</li><li>画像からpiece-wiseな平面を検出するタスクはARの応用に一つ重要なタスクとなっている．しかし従来，デプス推定とpiece-wiseな平面検出を同時に行う研究がない．著者達が新たにこのタスク及びタスクに対応できるネットワークを定義した．</li><li>提案フレームワークは:①DRNs(Dilated Residual Networks)を用いて入力画像から特徴抽出を行う②平面パラメータ推定・non-planarデプスマップ推定・セグメンテーションマスク推定の3つの推定ネットワークを用いる③推定した3つの結果から“piece-wise planar depthmap”を生成する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PlaneNet_PieceWise_PlaneEstimation.png" alt="PlaneNet_PieceWise_PlaneEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規な問題定義．実験で提案手法が部屋のレイアウト推定・ARアプリ(テクスチャー編集・バーチャルルーラーなど)に応用できることを指摘した．</li><li>51,000枚ほどの学習データを作成した．(これが大変そう)</li><li>plane segmentationタスクにおいてNYUデータセットでの精度が従来の三つの手法より優れている(比較している手法は2009年，2009年，2012年の手法だけど。。)</li><li>デプスマップ推定タスクにおいてNYUv2データセットにおいて前述した３つの手法より精度良い</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>ARアプリに応用できるところから考えると単純なデプス推定より実用性が高い</p></li><li><p>平面検出も同時に行うので，部屋レイアウト推定に良い精度を達成したのが理解できる．しかし，疑問としては提案手法が平面検出＋デプス推定だけで部屋の幾何構造実際は学習していないので，デプス推定＋平面パーツ検出の従来研究と比べると新規性と技術的の難しさがどこなのかちょっとわからない</p></li><li><p><a href="https://www.cse.wustl.edu/~chenliu/planenet/paper.pdf">論文</a></p></li><li><p><a href="https://github.com/art-programmer/PlaneNet">コード</a></p></li><li><p><a href="http://www.cse.wustl.edu/~chenliu/planenet.html">プロジェクト</a></p></li></ul></div></div><div class="slide_index">[#22]</div><div class="timestamp">2018.5.24 18:13:54</div></div></section><section id="PointNetVLAD_Deep_Point_Cloud_Based_Retrieval_for_Large-Scale_Place_Recognition"><div class="paper-abstract"><div class="title">PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition</div><div class="info"><div class="authors">Mikaela Angelina Uy, Gim Hee Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">573</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>PointNetとNetVLADを用いたポイントクラウドベースな“場所検索”ネットワークPointNetVLAD及びデータセットの提案．</li><li>従来の自動運転などに用いられる場所検索技術では2次元画像ベースで行われている．しかし，照明条件などに対しロバスト性が低い．ポイントクラウドベースな場所検索が従来良いグローバル特徴抽出機がないため，まだ研究されていない．近年PointNetなどの良いポイントクラウド特徴抽出機が提案され，そこで著者達がPointNetとNetVLADを用いたLiDARで撮ったポイントクラウドをベースとした場所検索手法を提案した．</li><li>提案データセットの収集過程は:①Oxford RobotCar などのdatasetからフルールートを選択する②フルールートから局所を選択する③選択した局所ポイントクラウドをダウンサンプルと正規処理を行う．また，Oxford RobotCar 以外，3種類の他のデータセットからデータを集めた．</li><li>fixedサイズなポイントクラウドからグローバル特徴を抽出できるPointNet，NetVLADと全結合層をコンバインたend-to-endなグローバル特徴抽出機を構築した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointNetVLAD.png" alt="PointNetVLAD"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規なポイントクラウドベースな場所検索及び場所検索3次元ポイントクラウドデータセットの提案．</li><li>従来の2次元画像ベースな場所検索と比べ，提案したポイントクラウドベースな場所検索が照明条件にロバストである．</li><li>PointNetとNetVLADを用いているので，ポイントクラウドの無順序性及びpermulationを対応できる．</li><li>新規なロス関数Lazy quadrupletを定義した．</li><li>提案データセットにおいて，PointNetとModelNetなどの従来手法と比べ良い検索精度達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>PointNet，PointNet++, Kd-networkなどのポイントクラウドデータを扱えるネットワークでポイントクラウドから情報抽出を利用した研究がこれからまだ増えるのかな？</p></li><li><p>ポイントクラウドデータを直接処理できるネットワークがいくつかあるが，主にPointNet，PointNet++が引用されていそう．ほかの手法があまり使われていない理由が知りたい</p></li><li><p><a href="https://arxiv.org/abs/1804.03492">論文</a></p></li><li><p><a href="https://github.com/mikacuy/pointnetvlad">コード</a></p></li></ul></div></div><div class="slide_index">[#23]</div><div class="timestamp">2018.5.24 18:03:54</div></div></section><section id="Pix3D_Dataset_and_Methods_for_3D_Object_Modeling_from_a_Single_Image"><div class="paper-abstract"><div class="title">Pix3D: Dataset and Methods for 3D Object Modeling from a Single Image</div><div class="info"><div class="authors">Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Tianfan Xue, Joshua Tenenbaum, William Freeman</div><div class="conference">CVPR 2018</div><div class="paper_id">375</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>大規模なピクセルレベルに対応付けられたimage-shape pairsデータセットPix3Dの提案及び画像から同時に三次元形状及び姿勢を推定するネットワークの提案．</li><li>従来のimage-shape pairsデータセットは①合成データセットを用いる②image-shapeの対応が精密ではない③データセット規模が小さいなどの問題点がある．そこで，著者達が大規模なピクセルレベルに対応付けられたデータセットを提案した．Pix3Dは395個の3次元物体モデル(9カテゴリ)，10069ペアの画像―形状ペアから構成される．画像と形状のペアはピクセルレベルの精密的に対応付けられている．</li><li>データセットの収集段階では:①IKEA及び自撮りで大量な画像―形状ペアを集める②AMTにより画像からキーポイントをアノテーションする③Efficient PnP及びLevenberg-Marquardtを用いて粗い・精密なposeを求める．</li><li>更に，提案手法は画像から同時に姿勢及び3次元形状を予測できるネットワークを提案した．提案ネットワークはまず画像から2.5Dスケッチを推定し，推定したスケッチをエンコーディングする．また，デコーディングにより3次元形状を推定し，同時にview estimatorネットワークにより姿勢を推定する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Pix3D.png" alt="Pix3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のデータセットではCGモデルで合成されている方が多く，提案のデータセットが実物体を用い，更にピクセルレベルな精密度の画像―形状対応付けアノテーションがある．</li><li>画像から同時に形状姿勢を推定するフレームワークの定量化結果は提案したデータセットでは3D-VAE-GAN,MarrNetなどの従来手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>現在の学習データアノテーション段階でAmazon Mechanical Turkを用いている．Semantic Keypointの自動的検出を用いたら自動化できることはデータセットの更なる拡大化につなぎられそう</p></li><li><p><a href="https://arxiv.org/abs/1804.04610">論文</a></p></li><li><p><a href="https://github.com/xingyuansun/pix3d">コード</a></p></li></ul></div></div><div class="slide_index">[#24]</div><div class="timestamp">2018.5.24 17:57:53</div></div></section><section id="Learning_to_Look_Around_Intelligently_Exploring_Unseen_Environments_for_Unknown_Tasks"><div class="paper-abstract"><div class="title">Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks</div><div class="info"><div class="authors">Dinesh Jayaraman, Kristen Grauman</div><div class="conference">CVPR 2018</div><div class="paper_id">152</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規な問題設定“シーンや物体を有効的に観測できる視点を学習する”及びこの問題を対応できる “アクティブ観測補完”ネットワークの提案．</li><li>従来のCVタスクは主に与えられた観測(画像・ビデオ・ポイントクラウドなど)から視覚性質(クラス分類・検出など)の分析を行う．しかし，リアルな知能はまず環境から目的を達成するための観測を取得することから始まる．また，異なる観測から得られる情報量も異なる．そこで，著者達が“active observation completion”タスクを提案し，未知なシーンかオブジェクトからシーン及び物体のより多く3次元情報が含めた数が限られた観測視点の推定を目標とする．</li><li>提案手法は強化学習を用いる．RNNベースなネットワークを用いて選択された視点からシーンか物体のパーツ情報を統合する．また，統合されたモデルから推定できるunobserved視点とgt間の誤差をベースにロス関数を設定した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LearningToLookAround.png" alt="LearningToLookAround"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>学習データを手動でラベリングする必要がないので，大量な学習が行える．</li><li>提案フレームワークを“シーン”の補完及び“物体モデル”の補完の2種類だいぶ異なったタスクに実験を行い，良い精度を達成したので，”提案した“無監督探索的な”フレームワークを遷移学習でほかのタスクに用いられる．</li><li>SUN360(Scene dataset)及び”ModelNet” (Object dataset)を用いて，従来のいくつかベースとなる手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>Interactive 環境でのVQAタスク(Embodied Question Answeringなど)は環境から“情報量が豊かな画像”を集めるのが重要の一環なので，提案フレームワークを用いられそう．</p></li><li><p><a href="https://arxiv.org/abs/1709.00507">論文</a></p></li></ul></div></div><div class="slide_index">[#25]</div><div class="timestamp">2018.5.24 17:50:58</div></div></section><section id="PU-Net_Point_Cloud_Upsampling_Network"><div class="paper-abstract"><div class="title">PU-Net: Point Cloud Upsampling Network</div><div class="info"><div class="authors">Lequan Yu, XIANZHI LI, Chi-Wing Fu, Daniel  Cohen-Or, Pheng-Ann Heng</div><div class="conference">CVPR 2018</div><div class="paper_id">355</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>data-drivenなポイントクラウドアップサンプリング手法の提案．スパースなポイントクラウドから，もっとデンスでユニフォームなポイントクラウドを取得できる．</li><li>従来の2D画像super-resolutionタスクと比べ，3D Upsamplingでは処理対象が空間オーダーとレギュラー構造がないポイントクラウドで，物体の本当のサーフェス(ポイントクラウドのリアル物体)に近づき，点の密度も均等であることがタスクの目標となる．こういったことから，提案手法はポイントクラウドからマルチレベルの特徴を抽出し，更にマルチブランチで特徴を拡張することにより，ポイントクラウドの局所及びグローバルな情報を取得できる．</li><li>提案ネットワークPU-Netは入力のポイントクラウド(N points)に対し①ポイントクラウドに対し異なるスケールのパッチを抽出し，②パッチからPointNet++を用いたマルチレベルの特徴抽出を行う．③feature expansion構造により特徴を拡張し，④全結合層を用いて出力のポイントクラウド(N＊ｒ points)を生成する．また，物体のサーフェスまでの距離及びポイントクラウドの過密程度を基準に，ジョイントロスを設計した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PU_Net.png" alt="PU_Net"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新たな評価指標：“物体のサーフェスまでの距離偏差”及び“ポイントクラウド分布のユニフォーム性”を評価できる指標を提案し，この2つの指標においてSHREC2015データセットに対し従来研究より優れた精度と指摘した．</li><li>Pointnet++を用いてローカル及びグローバル情報抽出を行うので，ポイントクラウドの幾何的無オーダーを対応できる</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>提案手法を更に発展し物体モデルの補完およびアップサンプリング同時にできることを期待される</p></li><li><p>Pointnet++を基本構造として使っていることがすごそう</p></li><li><p><a href="https://arxiv.org/abs/1801.06761">論文</a></p></li></ul></div></div><div class="slide_index">[#26]</div><div class="timestamp">2018.5.24 17:36:47</div></div></section><section id="Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective"><div class="paper-abstract"><div class="title">Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective</div><div class="info"><div class="authors">J.Zhang, T.Zhang, Y.Daiy, M.Harandi, and R.Hartley</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.10910</div></div><div class="slide_editor">KotaYoshdia</div><div class="item1"><div class="text"><h1>概要</h1><p>深層学習を用いた教師あり学習による顕著性の検出方法は教師データに依存する．そこで，“汎化能力を改善しつつ教師データなしで顕著性マップを学習することは可能か？”という問いに対して，弱いものやのノイズのある教師なし顕著性検出手法によって生成される多数のノイズラベルを学習することによって教師なしで顕著性の検出を行った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の教師なし顕著性検出に新たな顕著性を推定し，複数のノイズの多い顕著性検出方法から顕著性マップを学習する．</li><li>我々の深層学を用いた顕著性検出モデルは，人間のアノテーションなしでEnd to Endで学習できとても簡潔である．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>評価実験をしたところ従来の教師なしの顕著性検出方法を大きく上回り，深層学習を用いた顕著性の精度と同等のものとなった．</li><li><a href="https://arxiv.org/pdf/1803.10910">Paper</a></li></ul></div></div><div class="slide_index">[#27]</div><div class="timestamp">2018.5.23 20:28:11</div></div></section><section id="Cross-View_Image_Synthesis_using_Conditional_GANs"><div class="paper-abstract"><div class="title">Cross-View Image Synthesis using Conditional GANs</div><div class="info"><div class="authors">Krishna Regmi and Ali Borji</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>対応する航空写真とストリートビュー写真間の変換を行うcGANを提案．pix2pixによる変換に比べて，オブジェクトの正しいセマンティックスを捉え維持する変換が可能となっている．提案したcGANモデルは２つあり，X-Fork とX-Seq と呼んでいる．出力が変換画像とセグメンテーションマップであることが特徴．Inception Scoreの比較実験をすると，航空写真からストリートビュー方向の変換ではがX-Forkが優れ，逆方向の変換ではX-Seqの生成結果が優れていることがわかった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-View_Image_Synthesis_using_Conditional_GANs_fig.png" alt="Image"><br>256x256の解像度で生成可能．gがストリートビューで，aが航空写真に当たる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>X-Forkは１つのGeneratorと１つのDiscriminatorから成るシンプルな構成のcGAN．出力は変換後の画像とセグメンテーションマップの２つであることが特徴．</li><li>X-Seqは２つのGeneratorと２つのDiscriminatorから成るcGAN．１つ目のGeneratorで変換後の画像を生成．それを元に２つ目のGeneratorでセグメンテーションマップを生成する．<br>セグメンテーションマップのGround-Truthには，学習済みのRefineNetを用いた生成結果を使用している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>航空写真とストリートビューという劇的に見た目が変わる場合の変換において，どのようなことが問題点となるのか５つ挙げられていたので気になる場合は元論文を参照してください．</li><li>コードやデータは公開予定</li><li><a href="https://arxiv.org/abs/1803.03396">arXiv</a></li></ul></div></div><div class="slide_index">[#28]</div><div class="timestamp">2018.5.23 20:24:52</div></div></section><section id="Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence"><div class="paper-abstract"><div class="title">Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</div><div class="info"><div class="authors">D. H. Park et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">性能がよく，かつ説明可能なモデルの実現のための新規手法の提案．
これまでの説明可能なモデルは視覚的なAttentionのみやテキストの説明のみという単一のmodalだけだったのに対して，
この論文では両者を合わせたmulti-modalな説明を出力可能にした．
それを行う手法の提案と，学習と評価に使うデータセットを構築したのがこの論文のContribution．
データセットはVQAと静止画からのActivity Recognitionのタスクで，
従来あったデータセットに，理由のテキスト説明と視覚的な根拠となった領域のアノテーションを追加して作成．
手法は，まず答えを出力して，それを元に根拠となった理由を出力するという形式のネットワーク構造を採用．
</div></div><div class="item2"><img src="slides/figs/Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png" alt="Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>モデルの出力に加えて視覚的，テキストのmulti-modalな根拠説明をする手法を提案</li><li>VQAとActivity Recognitionでそれを評価可能なデータセット（追加アノテーション）を構築</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1802.08129">論文 (arXiv)</a></li><li>データセットはまだ公開されていない模様</li></ul></div></div><div class="slide_index">[#29]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation"><div class="paper-abstract"><div class="title">A Variational U-Net for Conditional Appearance and Shape Generation</div><div class="info"><div class="authors">Patrick Esser, Ekaterina Sutter, Björn Ommer</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>画像を構成する成分はshape(ジオメトリ、ポーズなど)とappearanceであるという考えのもと、VAEによってappearanceを推定し、
U-Netにshapeを学習させることで入力画像のappearanceとshapeの
片方を保ったままもう一方を変更することが可能なVariational U-Netを提案。
通常のVAEではshape、appearanceの分布を分離することが不可能なため、
VAEに画像とshapeを入力することでappearanceの特徴量を抽出し、U-Netによってshape情報を保つように学習を行う。
shapeとして体のポーズや線画が入力される。トレーニングデータには同一物体に対する様々なバリエーションの画像は必要としない。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>VAEでappearanceを、U-Netでshapeを学習させることで画像に内在する2つの事前分布を別々に学習することができるVarational U-Netを提案。</li><li>コンディションによって画像を編集するpix2pixとポーズをコンディションとして人物画像を編集するPG2と比較を行った。COCO、DeepFashion、Market-1501データセットにおいてSSIMやIS、
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VAEとU-Netのいいとこ取りをすることで、2つの変数を扱うことが可能になった。</li><li><a href="https://arxiv.org/abs/1804.04694">論文</a></li><li><a href="https://compvis.github.io/vunet/">Project Page</a></li><li><a href="https://github.com/CompVis/vunet">GitHub</a></li></ul></div></div><div class="slide_index">[#30]</div></div></section><section id="Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies"><div class="paper-abstract"><div class="title">Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</div><div class="info"><div class="authors">Hanbyul Joo, Tomas Simon, Yaser Sheikh</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>表情、体全体の動き、手のジェスチャといった様々なスケールの動きをマーカー無しでキャプチャするdeformation modelである”Frankenstein”と”Adam"を提案。
3Dキャプチャシステムに置いて、画像の解像度と3Dキャプチャシステムの視野はトレードオフであるため、
体の局所的な動きと全体的な動きを同時に捉えことは難しかった。提案手法では顔、両手、両足、
手の指における3Dキーポイントと3D Point Cloudを用いて表情などの
局所的モーションと体全体のモーションをキャプチャすることができるFrankensteinを構築。
また70人のトラッキングデータを用いてFrankensteinモデルを最適化することで、
髪と服を表現することが可能なAdamモデルを提案。結果は既存手法とのトラッキングの精度によって比較している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>表情や手のジェスチャといった局所的なモーションと、体全体の動きを同時にトラッキングすることが可能なdefromation modelを提案。620台のVGAカメラと31台のHDカメラが必要とする。</li><li>state-of-the-artである<a href="http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">SMPL</a>では顔の表情を表現することは不可能だが、提案手法では可能になっている。</li><li>SMPLとトラッキングにおけるGTとのオーバーラップを計算した結果、SMPLが84.79%であるのに対し提案手法は87.74%となり、提案手法の方が高い精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.01615">論文</a></li><li><a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture/">Project Page</a></li><li><a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">Video</a></li></ul></div></div><div class="slide_index">[#31]</div></div></section><section id="SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild"><div class="paper-abstract"><div class="title">SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild</div><div class="info"><div class="authors">Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, ; David Jacobs</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付き合成顔画像とin-the-wildなラベルなし実顔画像のどちらもトレーニングデータとして使用することで、実顔画像からシェイプ、リフレクタンス、イルミネーションを推定してリコンストラクションをend-to-endに行うSfSNetを提案。
実顔画像に十分なラベルがついているデータセットが存在しない、という問題を解決。Shape from Shading(SfS)のアイディアに基づき、
低周波成分を合成顔画像から、高周波成分を実顔画像から推定する。リコンストラクションされた画像のL1ロスを取ることで、
トレーニングにおける合成顔画像と実画像の橋渡しが行われる。リコンストラクションにはランバーシアンレンダリングモデルを使用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル付きの合成顔画像とラベルなしの実世界顔画像でトレーニングすることで、実世界顔画像の法線、アルベド、シェーディングを推定しインバースレンダリングを行うSfSNetを提案。</li><li>インバースレンダリングによってリコンストラクションされた画像のロスを取ることで、合成顔画像と実世界顔画像の橋渡しを実現。</li><li>インバースレンダリングの見た目がstate-of-the-artよりも良い結果となった。</li><li>法線・シェーディングの推定精度が、法線・シェーディング単体をそれぞれ推定するstate-of-the-artよりも良い結果となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>画像をリコンストラクションする際によく使われるU-NetではなくResNetを使った理由についても議論されている。</li><li><a href="https://arxiv.org/abs/1712.01261">論文</a></li><li><a href="https://senguptaumd.github.io/SfSNet/">Project Page</a></li><li><a href="https://github.com/senguptaumd/SfSNet">GitHub</a></li></ul></div></div><div class="slide_index">[#32]</div></div></section><section id="Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination"><div class="paper-abstract"><div class="title">Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination</div><div class="info"><div class="authors">Hazel Doughty, Dima Damen and Walterio Mayol-Cuevas</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2つの動画から、手術や絵を描くなどの技能がどちらが上かを予測する手法の提案。入力動画をTemporal Segment Networks(リンク参照)によりいくつかのセグメントに分割し，技能評価に用いるフレームを3枚選択する。
技能評価の学習は、2つの動画のどちらが技能が上か、2つの動画の技能が同じであるとき同じであると判定できるかの2つの尺度をロスとして行う。
技能を表すスコアは、Two Stream CNN(リンク参照)によって空間と時間それぞれについてスコアを取得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手術、ピザ生地をこねる、絵を描く、箸を使うの4つの技能を撮影したデータセットにより実験を行った。そのうち絵を描く、箸を使うは新たにデータセットを構築した。
全てのタスクで70%以上の精度を達成し、箸を使う以外のタスクではベースラインと比べ精度が向上した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1703.09913">論文</a></li><li><a href="https://arxiv.org/abs/1406.2199">Two Stream CNN</a></li><li><a href="https://arxiv.org/abs/1608.00859">Temporal Segment Networks</a></li></ul></div></div><div class="slide_index">[#33]</div><div class="timestamp">2018.5.22 17:48:35</div></div></section><section id="LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation"><div class="paper-abstract"><div class="title">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</div><div class="info"><div class="authors">T. Hui et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">FlowNet2よりも，性能が良く，モデルサイズが小さく，高速に動作するOptical Flow推定手法を提案．
FlowNet2（Feature Warping, Correlation）は性能が良いけどモデルサイズが大きい，
SPyNet（ピラミッド構造を採用）はモデルが小さいけど性能はあまり良くない，
ということで，提案手法は両者の良いところを合わせることをしている．
2フレームを入力として，各フレームをCNNに入れてピラミッド構造の特徴表現を得る．
一番解像度の低いところから順にFlow推定を繰り返していって洗練化していく．
各Flow推定では軽量な2つのモデルをカスケードさせたりして2フレーム間の大きな移動にも対応しながら，
軽量かつ高速な推定を実現．
</div></div><div class="item2"><img src="slides/figs/LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png" alt="LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>軽量な2つのネットワークをカスケードさせて使うCascaded flow inferenceの提案</li><li>CNNベースのFlow推定にFlow Regularizationを導入</li><li>高性能，省メモリ，高速な推定を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1805.07036">論文 (arXiv)</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/">プロジェクトページ</a></li><li><a href="https://github.com/twhui/LiteFlowNet">コード (GitHub)</a></li><li>カスケード構造が複雑でなぜこれが良いのか少し納得しにくい</li><li>実験は各コンポーネントのON/OFFで性能比較がわかりやすい</li></ul></div></div><div class="slide_index">[#34]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="Person_Transfer_GAN_to_Bridge_Domain_Gap_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</div><div class="info"><div class="authors">Longhui Wei, Shiliang Zhang, Wen Gao and Qi Tian</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-identification (ReID)のパフォーマンスは大きく向上したが，複雑なシーンや照明の変化、視点や姿勢の変化といった問題の調査は未だなされていない．本稿ではこれらの問題に関する調査を行った．このためにMulti-Scene MultiTime person ReID dataset (MSMT17)を構築した．またドメインギャップがデータ間に存在するため，このドメインギャップを埋めるためのPerson Transfer Generative Adversarial Network (PTGAN)を提案した．実験ではPTGANによってドメインギャップを実質的に狭められることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_PTGAN1.png" alt="1"><img src="slides/figs/20180522_PTGAN2.png" alt="2"><img src="slides/figs/20180522_PTGAN3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ReIDを行う際の現実的な問題について網羅的に調査</li><li>新たなReIDデータセットMSMT17を構築</li><li>データ間のドメインギャップを埋めるPTGANを提案</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.08565.pdf">論文</a></li></ul></div></div><div class="slide_index">[#35]</div><div class="timestamp">2018.5.22 17:09:22</div></div></section><section id="Zero-Shot_Sketch-Image_Hashing"><div class="paper-abstract"><div class="title">Zero-Shot Sketch-Image Hashing</div><div class="info"><div class="authors">Yuming Shen, Li Liu, Fumin Shen and Ling Shao</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模スケッチベース画像検索において，既存の手法では学習中にカテゴリの存在しないスケッチクエリがある場合失敗するという問題がある．本稿ではそのような問題を解決するZero-shot Sketch-image Hashing(ZSIH)モデルを提案した．2つのバイナリエンコーダとデータ間の関係を強化する計3つのネットワークで構成される．重要な点として，Zero-shot検索での意味的な表現を再構成する際に生成的ハッシングスキームを定式化する点である．Zero-shotハッシュ処理を行う初のモデルであり，関連する研究と比較しても著しく精度が向上した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_ZSIH1.png" alt="1"><img src="slides/figs/20180522_ZSIH2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>スケッチイメージハッシングの研究において初のZero-shot</li><li>意味的な表現を再構成する際に生成的ハッシングスキームを定式化</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02284.pdf">論文</a></li></ul></div></div><div class="slide_index">[#36]</div><div class="timestamp">2018.5.22 16:03:53</div></div></section><section id="Lions_and_Tigers_and_Bears_Capturing_Non-Rigid_3D_Articulated_Shape_from_Images"><div class="paper-abstract"><div class="title">Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</div><div class="info"><div class="authors">Silvia Zuffi, Angjoo Kanazawa and Michael J. Black</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンは人間をキャプチャするために設計されており，自然環境での使用や野生動物のスキャンおよびモデリングには不向きという問題がある．この問題を解決する方法として，画像から3Dの形状を取得する方法を提案した．SMALモデルを画像内の動物にフィット，形状が一致するようにモデルの形状を変形(SMALR)，さらに複数の画像においても整合性がとれるよう姿勢を変形させ、詳細な形状を復元する．本手法は，従来の手法に比べ大幅に3D形状を詳細に抽出することを可能にするだけでなく，正確なテクスチャマップを抽出し，絶滅した動物といった新しい種についてもモデル化できることを可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_lion.png" alt="Item3Image"><img src="slides/figs/20180501_lion2.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンが困難な動物のモデルを構築する方法を提案</li><li>SMALモデルを基として形状を変形させることで，より詳細な3D復元が可能</li><li>上記手法により，一貫したテクスチャマップの抽出が可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#37]</div><div class="timestamp">2018.5.22 15:06:58</div></div></section><section id="DOTA_A_Large-scale_Dataset_for_Object_Detection_in_Aerial_Images"><div class="paper-abstract"><div class="title">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</div><div class="info"><div class="authors">Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Tetsuya Narita</div><div class="item1"><div class="text"><h1>概要</h1><p>俯瞰画像から物体検出するためのデータセットを提案．従来のデータセットのものよりも小さい物体が多いデータセットである．各画像は4000×4000ピクセルであり，さまざまな大きさ，向き，形状を示す物体を含む．データセットは15カテゴリに分類されており，188282のインスタンスを含み，それぞれは任意の四角形でラベリングされている．人工衛星での物体検出の基礎構築のために，DOTA上の最先端の物体検出アルゴリズムを評価した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DOTA.png" alt="DOTA.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>俯瞰画像データセット内のインスタンスは小さいものの割合が高く，細かいものも検出可能人工衛星による物体検出に応用が利く可能性を示唆．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10398">論文</a></li></ul></div></div><div class="slide_index">[#38]</div><div class="timestamp">2018.5.21 18:34:11</div></div></section><section id="Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography"><div class="paper-abstract"><div class="title">Illuminant Spectra-based Source Separation Using Flash Photography</div><div class="info"><div class="authors">Zhuo Hui, Kalyan Sunkavalli, Sunil Hadap, and Aswin C. Sankaranarayanan</div><div class="conference">CVPR2018</div><div class="paper_id">752</div></div><div class="slide_editor">Kouyou OTSU</div><div class="item1"><div class="text"><h1>概要</h1><p>フラッシュを当てた状態の写真とそうでない写真の2種類を利用して，画像を光源の違いに基づく構成画像へと自動的に分離するアルゴリズムの提案．2つの写真の色情報の違いに基づき，光源に対応するスペクトルや陰影との関係を見出す．従来手法と比較して，光の色合いや陰影を忠実に反映した低ノイズでの分離が可能であることを示した(従来手法(Hsu et.al.)でのSNR:10.13dB 提案手法でのSNR 20.43dB)．また，提案手法が画像のライティングの編集，カラー測光ステレオに有用であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>光源分離にカメラのフラッシュを利用（手軽）</li><li>従来手法を上回る性能．</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1704.05564">論文</a></li><li><a href="https://www.youtube.com/watch?v=7iizopNYJT0">動画</a></li></ul></div></div><div class="slide_index">[#39]</div><div class="timestamp">2018.5.21 20:53:52</div></div></section><section id="Multi-Label_Zero-Shot_Learning_with_Structured_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Multi-Label Zero-Shot Learning with Structured Knowledge Graphs</div><div class="info"><div class="authors">Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>この論文は,各々の入力インスタンスに対して,複数の見えないクラスラベルを予測できるmulti-label learning及びmulti-label zero-shot learning(ML-ZSL)の新しい深層学習の提案した研究．
提案手法は複数のラベル間で人間が関心を持つsemantic knowledgeをグラフの中に組み込むことにより,
情報伝播メカニズムを学習し見えているクラスと見えないクラスの間の相互依存関係をモデル化することに適用できる．
本手法はstate-of-the-artと比較して,同等または改善されたパフォーマンスとして達成をすることができる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/171106526.png" alt="171106526"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・見た目だけでなく,経験を通して学んだ知識を使って物体を認識・WordNetから観察された知識グラフをend-to-endの学習フレームワークに組み込み,意味空間に電番されるラベル表現と情報を学習
・NUS-81およびMS-COCOの結果をWSABIE,WARP,Fast0Tag,Logisticsと比べたところ精度について一番高い結果を残した．
・ML-ZSLについてもFast0Tagと比べて高い精度を残している．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.06526">論文</a></li></ul></div></div><div class="slide_index">[#40]</div><div class="timestamp">2018.5.22 14:28:22</div></div></section><section id="Wasserstein_Introspective_Neural_Networks"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>generatorとdiscriminatorを一つのモデルで表現するIntrospective Neural Network(INN)に対してwasserstein distanceを導入することで、INNと同等の生成能力・識別能力を保ちつつclassifierにおけるCNNの数を20分の1にしたWasserstein INN(WINN)を提案。
生成された画像の比較はDCGAN、INN for generative(INNg)、INNgのclassifierにおけるCNNを一つにしたINNg-singleと行った。
またadversarial exampleに対して頑健な識別精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Wasserstein_Introspective_Neural_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>INNにwasserstein distanceを導入することで、生成・識別においてINNと同等以上の性能を持ちながら識別器におけるCNNの数が20分の1であるIWNNを提案。</li><li>テクスチャの生成やCelebA・SVHNを学習することで生成された画像はDCGANと比べてはっきりとしており質が高い。</li><li>CIFAR-10の学習によって生成された画像におけるInception scoreはDCGANの方が良い結果となった。</li><li>CNN、ReosNet、ICNと比較して、adversarial exampleに対する誤識別率が低く、 adversarial examples に惑わされずに識別を行うことが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08875">論文</a></li><li><a href="https://github.com/kjunelee/WINN">GitHub</a></li><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lazarow_Introspective_Neural_Networks_ICCV_2017_paper.pdf">Introspective Neural Networks for Generative Modeling</a></li></ul></div></div><div class="slide_index">[#41]</div></div></section><section id="Nonlinear_3D_Face_Morphable_Model"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンデータを使用せずにin-the-wildな顔画像のみを用いてencoder-decoderによって3D Morphable Model(3DMM)を生成する手法を提案。生成された3DMMを nolinear 3DMMと呼んでいる。
従来のlinear 3DMMは学習のために3Dスキャンデータが必要であり、かつPCAによって次元削減を行うため表現力に乏しいという問題点があった。
提案手法ではencoderによってプロジェクション、シェイプ、テクスチャのパラメタを取得し、decoderによってシェイプ、テクスチャを推定する。
また初期の学習では既存手法によって得られる3DMMのプロジェクションパラメタ、
シェイプパラメタとUV空間から得られるテクスチャを擬似的なGTとすることで弱教師学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Nonlinear_3D_Face_Morphable_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンデータを使用せずに、in-the-wildな顔画像のみを学習させることで、入力画像から3D Morphalbe Modelを生成する。</li><li>linear 3DMMと比較して、3次元形状、テクスチャの精度が高い。また見た目もGTにより近い。</li><li>顔のアラインメントにおいてstate-of-the-artよりも高い精度を達成。</li><li>3次元形状における精度はstate-of-the-artと同等であった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><li>弱教師学習がどれほど影響を持つかが気になった。</li><ul><li><a href="https://arxiv.org/abs/1804.03786">論文</a></li><li><a href="http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html">Project page</a></li></ul></div></div><div class="slide_index">[#42]</div></div></section><section id="UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition"><div class="paper-abstract"><div class="title">UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition</div><div class="info"><div class="authors">Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos Zafeiriou</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>in-the-wildな入力顔画像から得られるUVマップの補完をU-Netで行う手法を提案。入力画像に対して3D Morphalbe Modelを適用し不完全なUVマップを取得し、U-Netで補完を行うように学習を行う。
discriminatorにはUVマップ全体と顔領域の判定をさせる。
またUVマップの個人性が失われないように、アイデンティティーに関するロスを取る。
1892人のUVマップをもつWildUVデータセットの構築も行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>in-the-wildな顔画像に対してもリアルかつ精度の高いUVマップの補完を達成。入力されるUVマップが50%欠けていても補完可能。</li><li>入力画像からUVマップと3D shapeを取得するため、入力画像を任意の顔向きに編集可能。</li><li>横向き顔画像から生成されたUVマップはPSNR, SSIMにおいて既存手法を上回る精度を達成。</li><li>frontal-profile face verificationにおいてstate-of-the-artを上回る94.05%を達成。</li><li>1892のアイデンティティーのUVマップをもつ大規模UVマップデータセットであるWildUVデータセットを公開（予定）。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.04695">論文</a></li></ul></div></div><div class="slide_index">[#43]</div></div></section><section id="LIME_Live_Intrinsic_Material_Estimation"><div class="paper-abstract"><div class="title">LIME: Live Intrinsic Material Estimation</div><div class="info"><div class="authors">A. Meka, M. Maximov, M. Zollhöfer, A. Chatterjee, H.P. Seidel, C. Richardt and Ch. Theobalt</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>単RGB画像で，リアルタイムに材質反射特性を推定する手法を提案し，デモシステムを作った．</p><p>構造は，主に複数のU-Netからなり，それぞれ前景セグメンテーション，スペキュラー推定，鏡面反射推定を行う．ロス関数も定義．</p><p>さらに，形状情報も使えるのなら，低・高周波光源情報の推定も可能．連続撮影時の光源情報の連続性を考慮した時系列統合の枠組みも提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LIME_Live_Intrinsic_Material_Estimation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用的なシチュエーション（リアルタイム，複雑な光源下，連続撮影）で利用可能であることを示している．</li><li>定性，定量評価を行い，性能の良さを示している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>デモビデオを作り慣れているように見えるあたり，CG勢と思われる．デモも結構評価されているだろうか．
アプリケーション枠で評価されるように書いているかもしれない．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://www.youtube.com/watch?v=5ntLiAYsMm4">Youtube</a></li><li><a href="http://gvv.mpi-inf.mpg.de/projects/LIME/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#44]</div><div class="timestamp">2018.5.21 21:08:44</div></div></section><section id="Fast_End-to-End_Trainable_Guided_Filter"><div class="paper-abstract"><div class="title">Fast End-to-End Trainable Guided Filter</div><div class="info"><div class="authors">H. Wu, S. Zheng, J. Zhang, K. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>低解像度＋高解像ガイダンスマップを与えると，高解像度画像を効率的（省計算時間，省メモリ）に出力できるGuided Filtering Layerなるものを提案．</p><p><a href="http://kaiminghe.com/eccv10/">GuidedFilter</a>は，
空間的に変化する線形変換行列のグループとして表現でき，
CNNに統合可能．つまり，end-to-endで最適化可能な
深層ガイデッドフィルタネットワークを構成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_End-to-End_Trainable_Guided_Filter_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Fast_Image_Processing_ICCV_2017_paper.pdf">Context Aggregation Network</a>にGuided Filtering Layerを載せたものを、5つの先進的な画像処理タスクで試したところ，<strong>10～100倍高速</strong>であり，<strong>SoTA性能</strong>も出た．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>かなり省コストになっている．DNN導入可能にするように（エレガントに）定式化し，コストダウンしつつ深層学習できるようにする手法がいくつか見られている．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://github.com/wuhuikai/DeepGuidedFilter">GitHub</a></li></ul></div></div><div class="slide_index">[#45]</div><div class="timestamp">2018.5.21 20:01:20</div></div></section><section id="Guide_Me_Interacting_with_Deep_Networks"><div class="paper-abstract"><div class="title">Guide Me: Interacting with Deep Networks</div><div class="info"><div class="authors">Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager and Federico Tombari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>CNNにより学習したタスクの出力結果に対して、人間がヒント(例：画像中に空は見えない)を与えていくことで精度向上を図る研究。CNNモデルをheadとtailの2つのパートに分割し、headから得られた特徴マップをヒントによって修正していくことで精度の向上を実現する。
その際、ネットワークの重みを更新するのではなく修正に用いるパラメータを言語情報から推測することで行う。
ネットワークの予測結果とground truthの差分を取り、正しく予測できていない物体の種類や位置を推定することで学習に用いる文章は自動で生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Guide_Me_Interacting_with_Deep_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>セマンティックセグメンテーションにより実験を実施したところ、クラス間違い、物体の一部が欠けている、物体の一部のみが見えるといったケースにおいて精度が向上することを確認した。ヒントを繰り返し与えていくことはノイズとなってしまうためあまり精度が向上しなかった。
従来のディープラーニングは一度学習をしてしまうと得られる出力が固定されてしまうのに対して、人間が介入することで結果を変えるという新しい応用方法を提案している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11544">論文</a></li></ul></div></div><div class="slide_index">[#46]</div><div class="timestamp">2018.5.21 16:15:43</div></div></section><section id="Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting"><div class="paper-abstract"><div class="title">Face Detector Adaptation without Negative Transfer or Catastrophic Forgetting</div><div class="info"><div class="authors">Muhammad Abdullah Jamal, Haoxiang Li, Boqing Gong</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔検出におけるターゲットドメインからソースドメインへのadaptationを、negative transferとcatastrophic forgettingの両方を引き起こさずに行う手法を提案。
negative transferとはadaptation後のソースドメインにおける検出精度がadaptation前のソースドメインにおける検出精度に劣ることを指し、
catastorophic forgettingとはadaption後におけるソースドメインの検出精度が著しく下がることを指す。
提案手法では、ソースドメインとターゲットドメインの違いを、ロス関数とDNNの重みの差分で表現し、
この差分がなくなるように学習を行う手法を提案。
またターゲットドメインにface or notのラベルがないという状況も考えて教師あり学習だけでなく教師なし学習、
半教師あり学習の結果についても議論を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソースドメインとターゲットドメインの違いを、DNNのロス関数・重みの差分で表現することでadaptationを行った。</li><li>実験は、CascadeCNN+AFLW(25000 faces), Faster-R CNN+WIDER FACE dataset(393,703 faces, highly labeled)の2つのモデルでソースドメインの学習を行い、ターゲットドメインははFDDB(5171 labeled faces)、COFWで行った。</li><li>検出結果はターゲットドメインのみを学習した検出器、ソースドメインからターゲットドメインへfine tuningされた検出器、domain adaptaionを行うstate-of-the-artと比較を行った。提案手法はターゲットドメインにおける検出においてもっとも高い精度を達成。
またソースドメインにおける検出においてもターゲットドメインのみを学習した識別器と同等の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>adaptationというより、もはやトレーニングデータセットの事後拡張となっており、後でトレーニングデータを追加したくなった時に有用なのではないだろうか。</li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face.pdf">論文</a></li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face-supp.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#47]</div></div></section><section id="Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions"><div class="paper-abstract"><div class="title">Extreme 3D Face Reconstruction: Looking Past Occlusions</div><div class="info"><div class="authors">Anh Tuấn Trần, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, Gérard Medioni</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要s</h1><p>入力顔画像からバンプマップや視点を推定することで、入力画像からは見えていない側面や、強いオクルージョンがある顔画像からも精度の高い三次元形状を取得する手法を提案。
入力画像から帯域的な情報として三次元の大まかな形と、
局所的な情報としてしわなどのディティールを表現するバンプマップを別々のDNNモデルを使って取得する。
続いてオクルージョンがある場合には、バンプマップが不自然な起伏を持つため深層学習による修正を行う。
最後に顔の対称性を利用して、入力画像からは見えていない側面などをルールベースで復元する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像から3Dモデル全体を一気に復元するのではなく、帯域的な特徴と局所的な情報を分けて取り扱うことで精度の高い三次元復元を可能にした。</li><li>結果の評価は復元された三次元形状による個人認証の精度で行っている。画像にオクルージョンがない場合にはstate-of-the-artよりも高い精度を達成。オクルージョンがある場合でも、オクルージョンがない場合よりと比べて2%ほどしか劣らなかった。(state-of-the-artはそもそもオクルージョンを考慮できない。)</li><li>復元された三次元形状は、既存手法がオクルージョンを考慮することができなかったりシワなどの復元ができていないのに対して、提案手法ではオクルージョンがある場合でもシワなどの詳細な情報を復元できている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>帯域的な顔形状の復元やバンプマップの修正などを既存手法に頼っているものの、復元された三次元形状は既存手法に比べて圧倒的なクオリティを持つ。
しかし形状自体のGTとの比較がなかったのが残念。</li><li><a href="https://arxiv.org/abs/1712.05083">論文</a></li><li> <a href="https://github.com/anhttran/extreme_3d_faces">GitHub</a></li></ul></div></div><div class="slide_index">[#48]</div></div></section><section id="InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering"><div class="paper-abstract"><div class="title">InverseFaceNet: Deep Monocular Inverse Face Rendering</div><div class="info"><div class="authors">Hyeongwoo Kim, Michael Zollhöfer, Ayush Tewari, Justus Thies, Christian Richardt, Christian Theobalt</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>実世界の3D顔モデルを使用せず合成された3DモデルのみでCNNをトレーニングすることで、実世界の顔画像から顔向き、形、表情、リフレクタンス、イルミネーションの3D復元を行う手法を提案。
CNNをトレーニング際の問題点として、実世界の3D顔モデルに対するアノテーションが足りないという問題があった。
これに対して、実世界の顔画像から推定されるパラメタと合成顔から推定されるパラメタに対してself-supervised bootstrappingを行うことで、
トレーニングに使用する合成顔3Dモデルのパラメタの分布を実世界のパラメタの分布に近づくようにトレーニングデータを逐次的に更新を行うことで、
CNNの学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>self-supervised bootstrappingを使用することで、実世界のパラメータを再現するように合成顔のデータセットを再構築することで、データセットがないという問題に取り組んだ。</li><li>既存の学習ベースの手法に比べて、ジオメトリーにおいて最も高い精度を達成。</li><li>最適化ベースの手法に比べると、パーツのディティールやシワの再現の精度が悪い。</li><li>リミテーションとして、データセットにない顔向きや髪によるオクルージョンを考量することができない。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>異なるドメインを使ったトレーニングの方法として、GANを使ってcross domainの分布を近づける方法が提案されているなど、トレーニングデータ不足を解決する方法が提案されてきている。</li><li><a href="https://arxiv.org/abs/1703.10956">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/arXiv17_Inverse/supple.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#49]</div></div></section><section id="Towards_Pose_Invariant_Face_Recognition_in_the_Wild"><div class="paper-abstract"><div class="title">Towards Pose Invariant Face Recognition in the Wild</div><div class="info"><div class="authors">Jian Zhao, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang Zhao</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>様々な照明環境、表情をした横向き顔画像を入力として、正面顔画像を生成することで高い個人認証率を達成するGANベースのPose Invariant Model(PIM)というネットワークを提案。
学習で使用できるトレーニングデータが少ないため、効率的かつ過学習を防ぐために以下のようにPIMを構築。<li>顔全体を生成するgeneratorと両目・鼻・口の4つのパーツを生成するgeneratorを用意。</li><li>4つのパーツが検出された画像と取得できない画像(横顔画像など)を異なるドメインの画像とみなして、cross-domain adversarial trainingを行うことで、両目・鼻・口を復元。</li><li>上記のGANを２セット用意し、discriminator同士でlearning to learnを行うことで効率的な学習を行った。</li></p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Towards_Pose_Invariant_Face_Recognition_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>2つのGANをもつTP-GANやDR-GANは最適化が困難で合ったが、これに対してlearning-to-learnを導入することでこの問題を解決。</li><li>MultiPIE、CFPデータセットにおいて様々な角度の顔画像に対する個人識別においてほぼ全てのケースにおいてstate-of-the-artよりも優れた精度を達成。(唯一Multi-PIEで顔向きが±30°の場合にTP-GANに劣った。)</li><li>横向き顔画像から生成される正面顔画像において、既存手法ではテクスチャが崩れていたり完全に正面を向いていない場合があったが、提案手法では見た目が良い正面顔画像を生成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>データセットが少ないという根本的な問題に対して、cross-domain adversarial training、learing to learnを行うことで解決しているが、これがデータベースが欠乏している他の問題設定でも解決できるのかを試してみたい。</li><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#50]</div></div></section><section id="Ring_loss_Convex_Feature_Normalization_for_Face_Recognition"><div class="paper-abstract"><div class="title">Ring loss: Convex Feature Normalization for Face Recognition</div><div class="info"><div class="authors">Yutong Zheng, Dipan K. Pal and Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>DNNによって得られた特徴量を超球面上に配置するように正規化を行うロス関数であるRing lossを提案。特に教師あり識別問題においてはDNNによる特徴量を正規化することでより精度の高いモデルを構築することができる、
というアイディアもとにRing lossを提案。
SoftMaxといった基本的なロス関数と組み合わせることでより高い精度を達成。
実験には様々な識別タスクを行うことができる顔データセットを用いることで、精度の向上を確認した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Ring_loss_Convex_Feature_Normalization_for_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SoftMaxとSphereFaceにRing lossを組み合わせることでLFW, IJB-A Janus, Janus CS3, CFP, MegaFaceデータセットにおけるface verification, identificationにおいて他のロス関数と同等あるいはそれ以上の精度を達成。</li><li>極端に低解像度の画像におけるface matchingにおいてベースラインの手法を凌駕した。</li><li>実験ではResNet64を使用。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#51]</div></div></section><section id="Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images"><div class="paper-abstract"><div class="title">Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images</div><div class="info"><div class="authors">Hao Zhou, Jin Sun, Yaser Yacoob, David W. Jacobs</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dモデルから実画像へのドメイン変換をGANによって行うことで、単一顔画像から照明パラメタを推定するLabel Denoising Adversarial Network(LDAN)を提案。
人の顔画像に対して照明パラメタ(論文で使用されているのは37次元の球面調和関数)がアノテーションされたデータセットがないため、
3Dモデルを使用してFeature Netと呼ばれるネットワークで中間特徴量を取得し、
中間特徴量からLightning Netを用いて照明パラメタの推定を学習。
続いて人の顔画像に対して、既存手法を用いてノイズが乗った照明パラメタを取得し、
人の顔画像に対してもFeature Netを新しく学習し、
3D モデルから得られた中間特徴量と共にGANに入力することでドメインの変換を行うことでノイズが除去された照明パラメタを取得。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>単一画像からの照明パラメタの推定という問題に対して、初めて学習ベースの手法を提案。</li><li>結果の比較は19の照明環境が用意されているMultiPieデータセットで行い、推定されたパラメータに対する識別を行うことで精度を評価。state-of-the-artに比べて識別精度およびユークリッド距離・Q値におけるAUCで最も高い精度を達成。</li><li>同問題を扱う既存手法が最適化ベースということもあり、既存手法と比べて10万倍のスピードで実行可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GANを使って異なるドメインの特徴量を同じ空間にマップする考え方は既に<a href="https://arxiv.org/abs/1702.05464">Adversarial Discriminative Domain Adaptation</a>によって提案されているが、異なる点としては[Eric et al.]はGANのロスしか使っていないが、この方法では写像がうまく行かず、
A→A', B→Bと学習して欲しいところをやA→B', B→A'といった写像を学習してしまう。
これを解消するために、lightning netで得られたパラメータに対するL2ロスを取ることでこれを解消。</li><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#52]</div></div></section><section id="Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment"><div class="paper-abstract"><div class="title">Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</div><div class="info"><div class="authors">Amit Kumar, Rama Chellappa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔向きをコンディションとして与え木構造で表された顔のランドマークを学習させることで、顔のランドマーク推定を行うPose Conditioned Dendritic CNN(PCD-CNN)を提案。
顔のコンディションはPoseNetにより出力された値を使用する。
顔のランドマークを木構造として与えることで、ランドマークの位置関係を利用してCNNを学習させた。
また提案ネットワークはPCD-CNNと通常のCNNの二段階になっており、
後段のCNNをファインチューニングすることでランドマークのポイント数が違うデータセットや顔向き推定などの他のタスクにも適用可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークをPCD-CNNとCNNの二段階で構成することで、異なるランドマークのポイント数や顔向き推定といった他のタスクにも適用可能。</li><li>顔向きをコンディションとして与えることで推定精度が向上。また、20FPSで実行が可能。　</li><li>AFLW, AFWデータセットにおいてランドマークの推定精度がstate-of-the-artよりも高い推定精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#53]</div></div></section><section id="Multi-Image_Semantic_Matching_by_Mining_Consistent_Features"><div class="paper-abstract"><div class="title">Multi-Image Semantic Matching by Mining Consistent Features</div><div class="info"><div class="authors">Qianqian Wang, Xiaowei Zhou and Kostas Daniilidis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>ノイズを考慮しつつ、数千もの画像セット全てにおいて一致する(信頼できる)特徴を見出すことで、画像間の対応を図るマッチング手法。マッチングはセマンティック性を考慮することができる（目と目、耳先と耳先など）これにより、一貫性がある画像セット内で信頼できる特徴の関係を確立。何千もの画像を処理する場合にスケーラブルな手法。つまりは数に頑健。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG" alt="Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法では、全てのペアで対応する関係を最適化していたが、本手法では、特徴の選択とラベリングに着目し、信頼度の高い特徴のみを用いた疎なセットのみで識別、マッチングする。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>図は中の左が出力結果であり、目は青、耳は黄色、鼻は赤など各特長の意味を理解し、マッチングを成功させている。</p><ul><li><a href="https://arxiv.org/pdf/1711.07641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#54]</div><div class="timestamp">2018.5.21 11:27:27</div></div></section><section id="Learning_Intrinsic_Image_Decomposition_from_Watching_the_World"><div class="paper-abstract"><div class="title">Learning Intrinsic Image Decomposition from Watching the World</div><div class="info"><div class="authors">A. Uthors, B. Uthors and C. Uthors</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Intrinsic Image Decompositionのために，時間経過とともに照明が変化するビデオを使ったCNNの学習方法を提案．正解の Intrinsic Imageが不要な点が強みである．学習が完了したモデルは単一画像に対して適用できるよう汎化しており，いくつかのベンチマークに対して良い結果となった．<br>Contribution：<br>・データセット（BigTime）の公開．室内，室外両方での照明変化のあるビデオと画像シーケンスのデータセット．<br>・このGround Truthを含まないデータを使った手法の提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Intrinsic_Image_Decomposition_from_Watching_the_World_fig.png" alt="Image"><br>学習時：ラベル無しで，視点が固定され照明が変化するビデオを学習に利用する．<br>テスト時：単一画像からintrinsic image decompositionを行う．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>最適化ベースのIntrinsic Decomposition手法と，機械学習手法の間に位置する手法と言える．<br>・U-netに似た構造のCNN．<br>・Lossの工夫：画像ペア全てを考慮するall-pairs weighted least squares lossとシーケンス全体のピクセル全てを考慮するdense, spatio-temporal smoothness loss．最適化ベースのlossをフィードフォワードネットワークのlossとして利用する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Intrinsic image decompositionとは，入力された1枚の画像をreflectance画像とshading画像の積に分解する問題のこと．<br>intrinsic imagesのGround Truthを大規模に揃えることは困難．</p><ul><li><a href="https://arxiv.org/abs/1804.00582">arXiv</a></li></ul></div></div><div class="slide_index">[#55]</div><div class="timestamp">2018.5.21 11:26:41</div></div></section><section id="Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network"><div class="paper-abstract"><div class="title">Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network</div><div class="info"><div class="authors">Zizhao Zhang, Yuanpu Xie, Lin Yang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>階層的入れ子構造の識別器を使用し，テキストから高解像画像を生成するGANを提案．end-to-endの学習で高解像画像の統計量を直接モデルリングすることが可能な手法．これは，step-by-stepで高解像画像を生成するStackGANとは異なる点である．複数のスケールの中間層に対して階層的入れ子構造の識別器を使用することで中間サイズレベルでの表現に制約を加え，生成器が真の学習データの分布を獲得しやすくする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>新しい構造と，lossの工夫でtext-to-imageのタスクで高解像画像の生成を可能とした．<br>・hierarchical-nested Discriminatorを使用．<br>・lossには，pair lossとlocal adversarial lossを使用する．pair lossでは入力テキストと生成画像が一致しているかを評価．local adversarial lossでは生成画像の細部の質を評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09178">arXiv</a></li></ul></div></div><div class="slide_index">[#56]</div><div class="timestamp">2018.5.21 11:22:05</div></div></section><section id="Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images"><div class="paper-abstract"><div class="title">Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images</div><div class="info"><div class="authors">Tribhuvanesh Orekondy, Mario Fritz, Bernt Schiele</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>プライバシー保護のために画像に含まれる個人的な情報を自動的に改変する手法の提案．プライバシーを守りつつ画像の有用性を保つためのトレードオフが問題となる．有用性を保つためには改変する領域サイズが最小限である必要があり，これをセグメンテーションの問題として取り組む．</p><p>Contribution:</p><ul><li>データセットの公開．様々な種類のプライバシーのラベルが，ピクセルレベルとインスタンスレベルで与えられている自然画像の初のデータセット．</li><li>モデルの提案．多様な個人情報を自動的に改変するモデルを提案する．正解のアノテーションに対して83％の正解率を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images_fig.png" alt="Image"><br>指紋，日時，人，顔，ナンバープレートを黒く塗りつぶせている．<br>他にも，住所やメールアドレスのようなテキスト情報や顔や車椅子などの視覚情報，あるいはテキストと視覚情報を合わせたものなど，多様な個人情報に対応するデータセットとモデルを提案．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>どのような対象(Textual, Visual, Multimodal)を扱うかで使用するモデルは異なる．<br>Textualな対象では，Sequence Labelingを使用する．<br>VisualとMultimodalな対象では，Fully convolutional instance-aware semantic segmentationを使用する．<br>Nearest Neighborなどのベースライン手法と比較を行なっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像全体を黒く塗ればプライバシーは保護されるが，画像の価値がなくなるので，トレードオフが存在する．<br>データセットを作った貢献がメイン．プライバシー保護のためのアノテーションを行ったことで，それなりの正解率で個人情報の改変を行えるようになった．</p><ul><li><a href="https://arxiv.org/abs/1712.01066">arXiv</a></li></ul></div></div><div class="slide_index">[#57]</div><div class="timestamp">2018.5.21 11:17:12</div></div></section><section id="Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion"><div class="paper-abstract"><div class="title">Disentangling Structure and Aesthetics for Style-aware Image Completion</div><div class="info"><div class="authors">Andrew Gilbert, John Collomosse, Hailin Jin, and Brian Price</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ノンパラメトリックのInapinting手法を提案．<br>視覚的な構造とスタイルをdeep embeddingすることで，パッチの検索と選択の際に視覚的なスタイルを考慮することが可能で，さらに，パッチのコンテンツを補完画像のスタイルに合わせるためのneural stylizationが可能となる．この手法は，patch-basedの手法とgenerativeベースの手法の架け橋的な補完手法である．<br>技術的貢献：<br>・style-aware optimization<br>・adaptive stylization</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>以下の手順で画像補完を行う．<br>１．スタイルを考慮して穴に埋める候補を検索する<br>２．補完画像と構造とスタイルが合うパッチをMRFで複数集め，選択する<br>３．選択されたパッチを補完画像のスタイルに変換する</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://personal.ee.surrey.ac.uk/Personal/J.Collomosse/pubs/Gilbert-CVPR-2018.pdf">論文pdf</a></li></ul></div></div><div class="slide_index">[#58]</div><div class="timestamp">2018.5.21 11:09:50</div></div></section><section id="DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks"><div class="paper-abstract"><div class="title">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</div><div class="info"><div class="authors">Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiˇri Matas</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>motion deblurringのためのGAN(DeblurGAN)を提案．structural similarity measureとアピアランスでSoTA．ブラーを除去した画像で物体検出の精度を出すことで，ブラー除去モデルの質を評価するという方法を提案．提案手法は，質だけでなく実行速度も優れており，従来手法の５倍の速さがある．モーションブラーのかかった画像を合成するための方法を紹介し，そのデータセットもコード，モデルとともに公開．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks_fig.png" alt="Image"><br>ブレを除去してからYOLOで検出すると精度が良くなることを示している．これをDeblurモデルの指標にすることができると主張．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>loss：WGANによるAdversarial lossとPerceptual loss</li><li>構造：畳み込み，instance normalization層，ReLU関数から成るResBlockの繰り返しがメインで，出力するときに入力画像を加算するglobal skip connectionを持つ．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>最近のGAN手法やテクニックを詰め込んで，新しく作ったデータセットを利用したらSoTAがでたという感じ．テクニカルな貢献はあまりなさそう．</p><ul><li><a href="https://github.com/KupynOrest/DeblurGAN">GitHub</a></li><li><a href="https://arxiv.org/abs/1711.07064">arXiv</a></li></ul></div></div><div class="slide_index">[#59]</div><div class="timestamp">2018.5.21 11:05:29</div></div></section><section id="Learning_to_Understand_Image_Blur"><div class="paper-abstract"><div class="title">Learning to Understand Image Blur</div><div class="info"><div class="authors">Shanghang Zhang, Xiaohui Shen, Zhe Lin, Radom ́ır Meˇch, Joa ̃o P. Costeira, Jose ́ M. F. Moura</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ボケ(blur)が望ましいのか否かと，そのボケが写真のクオリティーにどのような影響を与えているのかを，自動的に理解するアルゴリズムは少ない．この論文では，blur mapの推定とこのボケの望ましさの分類を同時に行うフレームワークを提案する．</p><p>貢献：</p><ul><li>ボケを検出することと，画像の質という点でボケを理解することを同時に行うのは，おそらく初めての研究．ABC-FuseNetというネットワークを提案．</li><li>１万枚のデータセット（SmartBlur）の公開．ピクセルごとにボケがかかっているか３段階でラベルづけ．さらに，画像ごとにボケの望ましさ(desirability)をラベルづけ．</li><li>SmartBlurと他の公開データセットで実験を行い．blur mapの推定とボケの望ましさの分類がSoTAを超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Understand_Image_Blur_fig.png" alt="Item3Image"><br>ボケ具合をピクセルごとに３段階で示し，ボケの望ましさも出力する．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>ABC-FuseNetでは，低レベルのボケの推定と高レベルの画像内で重要コンテンツの理解の二つを行う．<br>A: attention map，FCNである．<br>B: blur map，Dilated Convolutionとpyramid pooling, Boundary Refinement用の層を使ってblurの推定を行う．<br>C: content feature map，ResNet-50を使ってコンテンツの特徴を抽出．<br>ボケの推定はBによって行い，ボケの望ましさの分類はA, B, Cから得られた特徴を用いて行う．ネットワーク全体をEnd-to-endで学習することができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ボケを軽減するための研究は多いが，ボケが全て邪魔とは言えない．ボケを効果的に利用することで，写真の印象が良くなることもある．いいボケなのか，悪いボケなのかの判断も必要だというモチベーションがある．</p><p>コード，データセットは以下に公開予定</p><ul><li><a href="https://github.com/Lotuslisa/Understand_Image_Blur">GitHub</a></li><li><a href="http://users.eecs.northwestern.edu/~xsh835/assets/cvpr2018_smartblur.pdf">論文</a></li></ul></div></div><div class="slide_index">[#60]</div><div class="timestamp">2018.5.21 10:50:21</div></div></section><section id="Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags"><div class="paper-abstract"><div class="title">Tags2Parts: Discovering Semantic Regions from Shape Tags</div><div class="info"><div class="authors">Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>指定された形状のタグに強く関係する領域を検出する手法の提案．明示的に領域ごとのラベリングはなく，さらにあらかじめセグメンテーションされていない状況で，形状のタグを与えた時に領域を発見するという問題設定．難しい点は，オブジェクトのタグという弱い教師情報からポイントごとのラベルを細かく出力する必要があること．このために分類とセグメンテーションを同時に行うネットワークを使う．形状ごとのタグからポイントごとの予測を得るためのネットワーク構造（WU-net）を提案したことがメインの貢献．</p><p>学習が完了すれば，タグが不明な形状に対しても手法を適用することができる．また，元々Weakly-supervised用に提案しているが，strongly-supervised用としても利用できる手法となった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net風のWU-netを提案．U-netから修正した点は，<br>・浅いU型の構造を3回くりかし，skip-connectionで密に繋がっている．深いU型1回の場合との結果の違いを図示している.<br>・セグメンテーションの用の隠れ層にタグ分類用の層を追加．(元々のは，strongly-supervised セグメンテーション用に設計されているので．)</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>３D形状としてはボクセル表現を使用．64×64×64 cubical gridを入力する．</p><ul><li><a href="https://arxiv.org/abs/1708.06673">arXiv</a></li></ul></div></div><div class="slide_index">[#61]</div><div class="timestamp">2018.5.21 10:40:57</div></div></section><section id="Neural_3D_Mesh_Renderer"><div class="paper-abstract"><div class="title">Neural 3D Mesh Renderer</div><div class="info"><div class="authors">Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ニューラルネットワークに組み込むことができる3Dメッシュのレンダラーである Neural Renderer を提案。レンダリングの『逆伝播』と呼ばれる処理をニューラルネットワークに適した形に定義し直した．そしてこのレンダラーを<br>・一枚の画像からの3Dメッシュの再構成（ボクセルベースの再構成との比較あり）<br>・画像から3Dへのスタイル転移と3D版ディープドリーム<br>に応用できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_3D_Mesh_Renderer_fig.png" alt="Image"><br>2D-to-3Dスタイルトランスファーの例</p></div></div><div class="item3"><div class="text"><h1>方法</h1><p>従来のままでレンダリングの操作が処理の途中にあると逆伝播が行えない状態であるので，レンダリングのための勾配を定義することでニューラルネットワークの中にレンダリング操作を加えても学習を行えるようにした．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://hiroharu-kato.com/projects/neural_renderer.html">プロジェクトサイト</a></li><li><a href="https://github.com/hiroharu-kato/neural_renderer">GitHub</a></li><li>３Dの形式には様々ある（ポイントクラウド，ボクセル，メッシュなど）が，3Dメッシュは効率的で表現能力が高く直感的な形式だそう．</li></ul></div></div><div class="slide_index">[#62]</div><div class="timestamp">2018.5.21 10:28:19</div></div></section><section id="Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos"><div class="paper-abstract"><div class="title">Demo2Vec: Reasoning Object Affordances from Online Videos</div><div class="info"><div class="authors">Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese and Joseph J. Lim</div><div class="conference">CVPR2018</div><div class="paper_id">1387</div></div><div class="slide_editor">KazuhoKito</div><div class="item1"><div class="text"><h1>概要</h1><p>商品などのデモンストレーションの映像の特徴を通してその商品などのアフォーダンスを推論する研究．映像から埋め込みベクトルを抜き出すことで，ヒートマップと行動のラベルとして特定のもののアフォーダンスを予測するDemo2Vecモデルを提案．また，YouTubeの製品レビュー動画を集め，ラベリングすることでOnline Product Review detaset for Affordande(OPRA)を構築．</p></div></div><div class="item2"><div class="text"><p> </p><img src="slides/figs/Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG" alt="Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG"></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アフォーダンスのヒートマップと行動のラベルの予測に関し，RNNの基準よりよいパフォーマンスを達成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>YouTubeで公開されている動画では，Demo2Vecを用いてある物体のデモ動画からSawyer robotのEnd Effectorを予測したヒートマップの地点に移動するように制御させている様子を見ることができる．</p><ul><li><a href="http://ai.stanford.edu/~kuanfang/pdf/demo2vec2018cvpr">論文</a></li><li><a href="https://sites.google.com/view/demo2vec/">ProjectPage</a></li><li><a href="https://www.youtube.com/watch?v=UT1QohPIioU">YouTube</a></li></ul></div></div><div class="slide_index">[#63]</div><div class="timestamp">2018.5.20 22:42:02</div></div></section><section id="Probabilistic_Plant_Modeling_via_Multi-View_Image-to-Image_Translation"><div class="paper-abstract"><div class="title">Probabilistic Plant Modeling via Multi-View Image-to-Image Translation</div><div class="info"><div class="authors">Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi Yagi</div><div class="conference">CVPR 2018</div><div class="paper_id">368</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>葉に隠れていても３次元の枝構造を多視点画像から推測できるようにした。多視点からの植物画像を入力として枝構造の２次元確率マップをdropoutを取り入れたPix2Pixで推測して、それらから３次元の確率構造を作成した。最後にpartical floｗシュミレーションによって明確な３次元の枝構造を生成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Probabilistic_Plant_Modeling.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>葉や他の枝によって隠れてしまっていても枝構造を生成できるようにした。ベイジアンPix2Pixを利用することで植物の３次元構造をより正確に表せるようにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.09404.pdf">論文</a></li></ul></div></div><div class="slide_index">[#64]</div><div class="timestamp">2018.5.20 20:53:44</div></div></section><section id="ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes"><div class="paper-abstract"><div class="title">ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>synthetic-to-realな変換を行う際に、1)モデルがsyntheticにoverfitするstyleの側面と、2)syntheticとrealの分布の違いの側面から発生する2つの問題があることに著者らは着目している。解決するために、前者はtarget guided distillation、後者はspatial-aware adaptationという手法を提案し、それを組み合わせた Reality Oriented ADaptation Network(ROAD-Net)を考案。GTAV/SYNTHIA - Cityscapesの適合タスクで評価し、sotaのsemantic segmentationモデルの汎化性能を向上したことを確認。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG" alt="ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Semantic SegmentationへのDomain Adaptationの適用が新しい。</li><li>結果もまたNonAdaptなPSPNetからmIoUが約11.6%向上している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://cvpaperchallenge.github.io/CVPR2018_Survey/#/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation">Learning to Adapt Structured Output Space for Semantic Segmentation</a>と目的と対象が似通っている。どちらもクラス分類で得られる特徴(ImageNetで学習されたpretrain model)がsegmentationでは有効ではないという主張であり、これをもとにそれぞれmulti-scaleな手法と、distillationによる手法と異なるアプローチをとっているのが興味深い。</li><li>spatial-aware adaptationはPatchGANと似通っており同様の性質を持つ？</li></ul><ul><li><a href="https://arxiv.org/abs/1711.11556">arxiv</a></li></ul></div></div><div class="slide_index">[#65]</div><div class="timestamp">2018.5.20 19:20:29</div></div></section><section id="Gated_Fusion_Network_for_Single_Image_Dehazing"><div class="paper-abstract"><div class="title">Gated Fusion Network for Single Image Dehazing</div><div class="info"><div class="authors">Wenqi Ren Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang</div><div class="conference">CVPR2018</div><div class="paper_id">404</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>霧がかかった画像(hazy input)から更に３つの入力，White balanced input，Contrast enhanced input，Gamma corrected inputを計算して導出し，これらの異なる入力間の外観差に基づきピクセル単位のConfidence Mapを計算する．これらを学習することで鮮明な画像を生成するMulti-scale Gated Fusion Network(GFN)を開発した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180404GFN_result.png" alt="Item3Image"><img src="slides/figs/180404GFN_network.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法と比較し，実装や再現が容易であり，また出力結果もPSNR，SSIMともに従来手法より高い評価となっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00213">arXiv</a></li><li><a href="https://sites.google.com/site/renwenqi888/research/dehazing/gfn">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#66]</div><div class="timestamp">2018.5.14 12:31:27</div></div></section><section id="AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation"><div class="paper-abstract"><div class="title">AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation</div><div class="info"><div class="authors">J.Nath, K.Phani, K.Uppala, A.Pahuja and R.V.Babu</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.01599</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>教師あり深層学習による手法は単眼カメラ画像における深さ推定に対して良い結果を出している．しかし．grand truthを得るためにはノイズに影響され，コストもかかる．合成データセットを用いた場合の深度推定では固有のドメインにしか対応していなく，自然なシーンに対して対応するのが難しいと言われる．この問題に対応するため，Adversalな学習と対応したターゲットの明確な一貫性をかすこと事によりAdaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>高次元の構造化エンコーダ表現に作用する，教師なしの敵対的適応設定AdaDepthを提案．</li><li>新規の特徴を再構成する正則化フレームワークを使用して適応表現にコンテンツ一貫性を課すことでモード崩壊の問題に取り組んだ．</li><li>最小限の教師データでの自然シーンの深度推定タスクにおいてSoTAを達成．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.01599">Paper</a></li></ul></div></div><div class="slide_index">[#67]</div><div class="timestamp">2018.5.20 15:52:52</div></div></section><section id="End-to-end_learning_of_keypoint_detector_and_descriptor_for_pose_invariant_3D_matching"><div class="paper-abstract"><div class="title">End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching</div><div class="info"><div class="authors">Georgios Georgakis, Srikrishna Karanam,Ziyan Wu,Jan Ernst,Jana Kosecka</div><div class="conference">CVPR 2018</div><div class="paper_id">227</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>End-to-Endで3次元空間における特徴点の抽出とマッチングを行う手法を提案した。2つの距離画像を入力とし、VGG-16 を利用したFaster R-CNNを基本構造としている。
２つの距離画像からそれぞれVGG−16を利用して特徴マップを作成し、RPNにより領域候補を推定して、ROIプーリング層、全結合層を経て特徴量ベクトルを作り出す。最終的にcontrastive lossを利用して得られた特徴量間の対応関係を求めた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/keypoint_detector.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>初めてEnd-to-Endで3次元マッチングを行えるようにした。ノイズ環境下においてキーポイントマッチングで従来手法のHarris3D +FPFHなどよりも10％以上高い精度を出した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1802.07869.pdf">論文</a></li></ul></div></div><div class="slide_index">[#68]</div></div></section><section id="AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</div><div class="info"><div class="authors">Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アテンションドリブン，複数ステージでのRefineによって，テキストから詳細な画像を生成するGANを提案．CUBデータセットとCOCOデータセットでinception scoreがstate of the artを超えた．生成画像の特定の位置をワードレベルで条件付けしていることを示した．</p><p>貢献：<br>・Attentional Generative Adversarial NetworkとDeep Attentional Multimodal Similarity Model(DAMSM)の提案．<br>・実験でstate-of-the-art GAN modelsを超えたことを示す．<br>・ワードレベルで自動的に生成画像の一部をアテンションするのは初である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>・Attentional Generative Networkはセンテンスの特徴から始めて段階的に画像を高精細にしていくネットワークで，途中にアテンションレイヤーからのワード特徴を入力して条件付けする．<br>・各解像度に対してそれぞれDiscriminatorがある．<br>・最終的な解像度になったあと，Image Encoderにて局所的な画像特徴量とし，ワード特徴量とDAMSMにて比較することで，生成画像の細部がどれくらい単語に忠実であるか評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・StackGANの著者も共著にいる．<br>・アテンションにより生成箇所を局所に向けることで，COCOのような複雑なシーンでも対応できるようになっている．</p><ul><li><a href="https://arxiv.org/abs/1711.10485">arXiv</a></li></ul></div></div><div class="slide_index">[#69]</div><div class="timestamp">2018.5.19 13:50:16</div></div></section><section id="From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN"><div class="paper-abstract"><div class="title">From source to target and back: Symmetric Bi-Directional Adaptive GAN</div><div class="info"><div class="authors">Paolo Russo, Fabio M. Carlucci, Tatiana Tommasi and Barbara Caputo</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SBADA-GANの提案．（Symmetric Bi-Directional ADAptive Generative Adversarial Network）<br>unsupervised cross domain classificationにフォーカス.<br>ラベルが与えられるSourceのサンプルを利用して，最終的にはTargetの分類問題を解く．SourceのサンプルをTargetのドメインに(Image-to-Imageの)マッピングをし，同時に逆方向も行う．分類器の学習に利用するのは，Sourceサンプル，TargetをSource風にしたもの，SourceをTarget風にしてさらにSource風に戻した３種類を使う．それぞれにラベルもしくは擬似ラベルを付与して学習する．テスト時はTargetサンプルのクラスを予測したいので，Target用の分類器と，TargetサンプルをSource風にしてから入力するSource用の分類器の２つを使用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>セルフラベリングの使用．Source用の分類器に制約を課す</li><li>class consistency lossの導入．Generatorとともに利用することで両方向のドメイン変換がお互いに影響し合うようになる．安定性と質向上の効果．最終的な目標である分類問題を解くことに有効．</li><li>例えばSource側のDiscriminatorは，RealサンプルとしてSource画像を使い，FakeサンプルとしてTarget画像をSource画像風にGeneratorでドメイン変換した画像を使う．</li><li>（問題設定的に）Source側の分類器にはクラスラベルによる学習ができる．</li><li>SourceとTargetの双方向のサンプル生成のための二つadversarial lossと，二つのclassification lossを同時に最小化する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.08824">arXiv</a></li></ul></div></div><div class="slide_index">[#70]</div><div class="timestamp">2018.5.19 14:15:18</div></div></section><section id="Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs"><div class="paper-abstract"><div class="title">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</div><div class="info"><div class="authors">Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, Yung-Yu Chuang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習ベースで画像のエンハンスメントを行う手法の提案．入力として「良い」写真のセットを使う．このセットに含まれる特色を持つように変換することが「エンハンスメント」に繋がると定義する．エンハンスメント問題をimage-to-imageの問題として扱い，提案手法は「良い」写真のセットの中で共通の特色を発見することを狙っている．普通の写真のドメインを「良い」写真のドメインに変換すれば良いとし，（CycleGANのような）２方向GANを以下の３つの工夫とともに利用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>Contribution</h1><ul><li>global featureを使ったU-netの利用．これがシーンの状況，照明条件，対象のタイプの情報を捉える．</li><li>WGANのためのadaptive weighting schemeを提案．収束を早める．</li><li>individual batch normalization layersの利用．Generatorは入力データの分布により適応するようになる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Flickerのレタッチされた写真を利用するなどしている．</li><li>Adobeがプロ写真家一人一人のレタッチ方法を再現するという機能を実装するのも近いかもしれない．</li><li>ハイダイナミックレンジの写真にしたらエンハンスされていると思っている節がある．</li><li><a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Chen2018DPE.pdf">論文</a></li></ul></div></div><div class="slide_index">[#71]</div><div class="timestamp">2018.5.19 13:33:54</div></div></section><section id="Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts"><div class="paper-abstract"><div class="title">Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</div><div class="info"><div class="authors">Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, and Ahmed Elgammal</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Wikipediaのようにノイズの多いテキストからzero-shot learningを行うためのGAN用いる方法を提案．GANを使ってテキストが表現するオブジェクトのビジュアル的な特徴を生成する．オブジェクトのクラスごとに特徴を近い位置にembeddingできれば良い．これができれば後は教師あり手法で分類を行えることになる.<br>コントリビューション：</p><ul><li>zero-shot learningにおいてUnseenであるクラスのテキスト記述からvisual featureを生成することで，zero-shot learningを従来の分類問題にしてしまう．generative adversarial approach for ZSL (GAZSL) ．</li><li>ノイズを抑制するためのFC層と埋め込み後のクラス識別性を高めるvisual pivot regularizationの提案．</li><li>zero-shot recognition, generalized zero-shot learning, and zero-shot retrievalという複数のタスクでstate-of-the-art手法を超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts_fig.png" alt="Image"></p><p>左上段がFakeデータを作るストリーム．左下段がRealデータを作るストリーム．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Unseenクラスについてのノイズを含むテキスト記述を入力とし，このクラスのvisual featureを生成するGANを提案．テキストから生成されるvisual featureをFakeデータとし，真の画像から得られるvisual featureをRealデータとしてGANを学習．</p><ul><li>テキストのembedding後，FC層で次元圧縮をし，ノイズの影響を軽減．</li><li>生成された特徴のクラス間の識別性を保存するために, visual pivot regularizationを利用．Generatorの更新に利用．</li><li>Realデータとして真の画像からvisual feature得る際にはVGGを利用．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.01381">arXiv</a></li></ul></div></div><div class="slide_index">[#72]</div><div class="timestamp">2018.5.19 13:28:25</div></div></section><section id="MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation"><div class="paper-abstract"><div class="title">MoCoGAN: Decomposing Motion and Content for Video Generation</div><div class="info"><div class="authors">Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>教師不要でコンテンツとモーションという要素に分解し，ビデオを生成するGANを提案．コンテンツを固定しモーションのみ変化させることや，逆も可能．広範囲の実験を行い，量と質ともにSoTAであることを確認．人の服装とモーションの分離や，顔のアイデンティティーと表情の分離が可能であることを示している．</p><p>Contribution:・ノイズからビデオを生成する，条件なしでのビデオ生成GANの提案．
・従来手法では不可能である，コンテンツとモーションのコントロールが可能なこと
・従来のSoTA手法との比較</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>GAN．</li><li>ランダムベクトルのシーケンスをビデオフレームのシーケンスにマッピングするGenerator．ランダムベクトルの一部はコンテンツ，もう一部はモーションを指定するもの．</li><li>コンテンツの部分空間はガウス分布でモデル化．モーションの部分空間はRNNでモデル化．</li><li>Generatorは一つのフレーム分をベクトルからフレームにマップする働きだけなので，モーションを決めるのは連続するベクトルを生成するRNN部分となる．</li><li>1枚のフレームを入力とするDiscriminatorと連続した数フレームを入力とするDiscriminatorを使うGAN構造を新たに提案．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>ビデオはコンテンツとモーションに分けられるという前提（prior）からスタート</li><li><a href="https://arxiv.org/abs/1707.04993">arXiv</a></li></ul></div></div><div class="slide_index">[#73]</div><div class="timestamp">2018.5.19 13:08:06</div></div></section><section id="Finding_It_Weakly-Supervised_Reference-Aware_Visual_Grounding_in_Instructional_Videos"><div class="paper-abstract"><div class="title">Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</div><div class="info"><div class="authors">De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, Juan Carlos Niebles</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>言語的な文脈の中で指示語からそれが何であるかを特定する問題（Visual Grounding; 「それを取ってください」の「それ」を動画中から探索するなど）を扱う論文である。この問題に対してMIL（Multiple Instance Learning）を参考にした弱教師付き学習であるReference-aware MIL（RA-MIL）を用いて解決する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180518VisualGrounding.png" alt="180518VisualGrounding"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像に対するVisual Groundingが空間的な関係性を捉えるのに対して、Visual Groundingは時間的な関係性を捉える課題である。YouCookII/RoboWatch datasetにて処理を行った結果、弱教師付き学習であるRA-MILを適用するとVisual Groundingに対して精度向上することを明らかにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Language and Visionの課題はすでに動画にまで及んでいる。Visual Groundingのみならず、新規問題設定を試みた論文として精読してもよいかも？それと視覚と言語のサーベイ論文は読んでみたい</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-ramil.pdf">論文</a></li><li><a href="http://ai.stanford.edu/~dahuang/">著者</a></li><li><a href="http://aclweb.org/anthology/D15-1021">視覚と言語のサーベイ論文</a></li></ul></div></div><div class="slide_index">[#74]</div><div class="timestamp">2018.5.18 16:30:52</div></div></section><section id="Practical_Block-wise_Neural_Network_Architecture_Generation"><div class="paper-abstract"><div class="title">Practical Block-wise Neural Network Architecture Generation</div><div class="info"><div class="authors">Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ブロック単位でのアーキテクチャ生成手法であるBlockQNNを提案。Q学習（Q-Learning）を参考にして高精度なニューラルネットを探索的（ここではEpsilon-Greedy Exploration Strategyと呼称）に生成する。基本的には生成したブロックを積み上げることによりアーキテクチャを生成するが、早期棄却の枠組みも設けることで探索を効率化している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517BlockQNN.png" alt="180517BlockQNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ブロック単位でニューラルネットのアーキテクチャを探索するBlockQNNを提案した。同枠組みはHand-craftedなアーキテクチャに近い精度を出しており（CIFAR-10のtop-1エラー率で3.54）、探索空間を削減（32GPUを3日間使用するのみ！）、さらに生成した構造はCIFARのみならずImageNetでも同様に高精度を出すことを明らかにした。ネットワーク構造の探索問題においてブロックに着目し、性能を向上させると同時に同様の枠組みを複数のデータセットにて成功させる枠組みを提案したことが、CVPRに採択された基準である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ここから数年で、practicalなGPU数（8GPUや4GPUなど）、1日以内の探索で解決するようになると予想される（し、してくれないと一般の研究者/企業が参入できない）。</p><ul><li><a href="https://arxiv.org/abs/1708.05552">論文</a></li></ul></div></div><div class="slide_index">[#75]</div><div class="timestamp">2018.5.17 13:12:12</div></div></section><section id="Residual_Dense_Network_for_Image_Super-Resolution"><div class="paper-abstract"><div class="title">Residual Dense Network for Image Super-Resolution</div><div class="info"><div class="authors">Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>低解像画像から高解像画像（SR; super-resolution image）を復元するための研究で、DenseNet（論文中の参考文献7）を参考にしたResidual Dense Networks (RDN)を提案して同課題にとりくんだ。異なる劣化特徴をとらえたモデルであること、連続的メモリ構造（Contiguous Memory Mechanism）やコネクションを効果的にするResidual Dense Blockを提案したこと、Global Feature Fusionにより各階層から総合的な特徴表現、を行い高解像画像を復元した。DenseNetで提案されているDense Blockと比較すると、提案のResidual Dense Blockは入力チャネルからもスキップコネクションが導入されているため、よりSRの問題設定に沿ったモデルになったと言える。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ResidualDenseNetwork.png" alt="180517ResidualDenseNetwork"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>高解像画像を復元するための改善として、DenseNetを改良したRDNを提案した。Dense Blockを置き換え、より問題に特化したResidual Dense Blockを適用。実験で使用した全てのデータセット（Set5, Set14, B100, Urban100, Manga109）の全てのスケール（x2, x3, x4）にて従来手法よりも良好なAverage PSNR/SSIMを記録した。結果画像はGitHubのページなどを参照されたい。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>課題の肝をつかんで、従来提案されている効果的な手法を改善できるセンスを磨きたい。</p><ul><li><a href="https://arxiv.org/abs/1802.08797">論文</a></li><li><a href="https://github.com/yulunzhang/RDN">GitHub</a></li></ul></div></div><div class="slide_index">[#76]</div><div class="timestamp">2018.5.17 12:47:07</div></div></section><section id="Three_Dimension_Human_Pose_Estimation_in_the_Wild_by_Adversarial_Learning"><div class="paper-abstract"><div class="title">Three Dimension Human Pose Estimation in the Wild by Adversarial Learning</div><div class="info"><div class="authors">Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li, Xiaogang Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>現在でもチャレンジングな課題として位置付けられる人物に対する3次元姿勢推定に関する研究で、Adversarial Learning (AL)を用いて学習を実施。問題設定としては「多量の」2次元姿勢アノテーション+「少量の」3次元姿勢アノテーションを使用することで、新規環境にて3次元姿勢推定を実行することである。本論文で提案するALではG（生成器）として、2D/3Dのデータセットからそれぞれ2D/3Dの姿勢を推定、実際のデータセットからアノテーションを参照（リアル）して、生成されたものか、データセットのアノテーションなのかを判断（D; 識別器）させることで学習する。G側の姿勢推定ではHourglassによるConv-Deconvモデルを採用、D側には3つの対象ドメイン（オリジナルDB、関節間の相対的位置、2D姿勢位置と距離情報）を入れ込んだMulti-Source Discriminatorを適用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517PoseGAN.png" alt="180517PoseGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANに端を発する敵対的学習を用いて、3次元姿勢に関するアノテーションが少ない場合でもドメイン依存をすることなく3次元姿勢推定を可能にする技術を提案した。また、もう一つの新規性としてドメインに関する事前知識を識別器に入れ込んでおくmulti-source discriminatorについても提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>少量のラベル付きデータが用意できていれば、ドメイン関係なく推定ができるという好例である。データとアノテーションに関連するのはCG/敵対的学習/教師なし/ドメイン適応などで、これらは現在のCVにおいても重要技術。少なくともお金がないとクラウドソーシングでデータが集められないという構図を変えたいと思っている。</p><ul><li><a href="https://arxiv.org/abs/1803.09722">論文</a></li><li><a href="http://www.cs.cmu.edu/~xiaolonw/publication.html">著者</a></li></ul></div></div><div class="slide_index">[#77]</div><div class="timestamp">2018.5.17 12:03:18</div></div></section><section id="Gesture_Recognition_Focus_on_the_Hands"><div class="paper-abstract"><div class="title">Gesture Recognition: Focus on the Hands</div><div class="info"><div class="authors">Pradyumna Narayana, J. Ross Beveridge, Bruce A. Draper</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手部領域に着目してチャネルを追加することにより、ジェスチャ認識自体の精度を高めていくという取り組み。従来型のマルチチャネル（rgb, depth, flow）のネットワークでは限定的な領域を評価して特徴評価を行なっていたが、提案のFOANetでは注目領域（global, right hand, left hand）に対して分割されたチャネルの特徴を用いて特徴評価を行い識別を実施する。図に示すアーキテクチャがFOANetである。FOANetでは12のチャネルを別々に処理・統合し、統合を行うネットワークを通り抜けて識別を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517FOANet.png" alt="180517FOANet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手部領域に着目し、よりよい特徴量として追加できないか検討した、とういアイディア自体が面白い。また、ChaLearn IsoGD datasetの精度を従来の67.71%から82.07まで引き上げたのと、同じようにNVIDIA datasetに対しても83.8%から91.28%に引き上げた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>あまりメジャーに使用されているDBではないが、重要課題を見つけてアプローチする研究は今後さらに必要になってくる？一番最初に問題を解いた人ではないが、二番目に研究をして実利用まで一気に近づけられる人も重宝される。</p><ul><li><a href="http://www.cs.colostate.edu/~draper/papers/narayana_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#78]</div><div class="timestamp">2018.5.17 11:20:46</div></div></section><section id="Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment"><div class="paper-abstract"><div class="title">Direct Shape Regression Networks for End-to-End Face Alignment</div><div class="info"><div class="authors">X. Miao, X. Zhen, V. Athitsos, X. Liu, C. Deng and H. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアライメントにおいて，Direct shape regression networkを提案．いくつかの新しい構造を組み合わせている．(1)二重Conv，
(2)フーリエ特徴プーリング，
(3)線形低ランク学習．
顔画像-顔形状間の高い非線形関係性（初期化への強い依存性，ランドマーク相関導出の失敗）の問題を解決する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>複数の新しい構造の定義</li><li>いくつかのケースでSoTAを超える性能．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://xinxinmiao.github.io/pdfs/1607.pdf">論文</a></li></ul></div></div><div class="slide_index">[#79]</div><div class="timestamp">2018.5.18 11:57:02</div></div></section><section id="Scale-recurrent_Network_for_Deep_Image_Deblurring"><div class="paper-abstract"><div class="title">Scale-recurrent Network for Deep Image Deblurring</div><div class="info"><div class="authors">X. Tao, H. Gao, Y. Wang, X. Shen, J. Wang, J. Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>coarse-to-filneに単画像デブラーリングする，Scale-recurrent Network (SRN-DeblurNet)を提案．</p><p>構造的には，(1)入出力がピラミッド画像，
(2)中間はUnet，
(3)最終層の出力を第1層に注入（Recurrent）し，ピラミッド画像の枚数分実行．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Scale-recurrent_Network_for_Deep_Image_Deblurring_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>シンプルでパラメータ数が少ない．</li><li>SoTAを超える性能．例もすごいきれいになっているように見える．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>見た目明らかにきれいになっていると，やはり評価したくなる．</p><ul><li><a href="https://arxiv.org/abs/1802.01770">arXiv</a></li></ul></div></div><div class="slide_index">[#80]</div><div class="timestamp">2018.5.18 11:02:11</div></div></section><section id="Convolutional_Neural_Networks_with_Alternately_Updated_Clique"><div class="paper-abstract"><div class="title">Convolutional Neural Networks with Alternately Updated Clique </div><div class="info"><div class="authors">Yibo Yang et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>従来のCNNの構造では基本的に決められた方向へのみのforwardを行うのに対して、すべてのレイヤー間で結合を持つClique blockで構成されるClique Netの提案。CIFAR-10でSoTA、その他ImangeNetやSVHNでも少ないパラメータでSoTAに匹敵する精度を記録。</p></div></div><div class="item2"><img src="slides/figs/Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png" alt="Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Clique blockでは以下のような処理が行われる。</p><ul><li>畳み込み層によってすべての層を共通の特徴マップで初期化。</li><li>ある層に対して、他のすべての層から畳み込み結合した値で更新。これを各層に対して順次行い、すべての層で更新したら1つのStageが終了。</li><li>上記を決められたStage数行う。畳み込み結合の重みはStage間で共有する。</li></ul><p>DenseNetの拡張に近い構造のため妥当性があり、実際に精度が出ている点が強い。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10419">論文</a></li></ul></div></div><div class="slide_index">[#81]</div></div></section><section id="Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning"><div class="paper-abstract"><div class="title">Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning </div><div class="info"><div class="authors">Chuang Gan et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成画像のペア間のフローと教師ラベルのない実画像のペア間のデプスを推定することによってシーン認識、行動認識のための表現学習を行う研究。フロー推定を行ったのち、デプス推定にfine-tuningし、さらに目的となるタスクにfine-tuningする。
直感的には、低レベルな特徴が獲得されそうだが、行動認識などの高次な問題設定でも効果を発揮した。</p></div></div><div class="item2"><img src="slides/figs/Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png" alt="Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>多段にfine-tuningするため、初期の問題設定によって獲得した特徴が失われてしまう可能性があるので、２段目のfine-tuning時にはfine-tuning前の出力結果への蒸留を同時に行う。ImageNetのpretrainingとも行動認識において補間的な関係がある。表現学習自体での使用データが少ないのに関わらず高い精度向上が実験的に示されたことが大きなcontributionだと考えられる。        </p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>特徴のforgetを防ぐ手法は、複数のタスクで学習済みモデルを作成する際に、その順番が重要となるような状況で有用だと思われる。既存手法との比較においては今回は+αのデータを利用している点はフェアではないと感じた。
また、目的のタスクへのfine-tuningの際のフレームペアの選び方などの詳細な設定が記されていなかった。主に精度評価のみで、高次なタスクでうまくいく考察が少なく、疑問もあった。</p><ul><li><a href="http://cseweb.ucsd.edu/~haosu/papers/cvpr18_geometry_predictive_learning.pdf">論文</a></li></ul></div></div><div class="slide_index">[#82]</div></div></section><section id="Learning_to_Compare_Relation_Network_for_Few-Shot_Learning"><div class="paper-abstract"><div class="title">Learning to Compare: Relation Network for Few-Shot Learning</div><div class="info"><div class="authors">F. Sung, Y. Yang, L. Zhang, T. Xiang, P.H.S. Torr, T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>メタ学習を用いたFew-shot learningの新しい枠組み，Relation Networkの提案．一度学習されれば，ネットワークのアップデートの必要なしに新しいクラスの画像分類ができるようになる．</p><p>1エピソードにおける少数の画像の比較によって距離メトリックを学習するメタラーニングを行う．少数の新クラスの代表画像群とクエリ画像の関連性スコアの比較により，追加学習なしに新クラス画像分類が行える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Compare_Relation_Network_for_Few-Shot_Learning_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>再学習しなくても，データさえ用意しておけば未知のクラスも分類可能な画像分類器ができる．</li><li>Zero-shot learningにも拡張可能．</li><li>シンプルで，高速に動作し，拡張性も高い．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>テスト時も少数のデータを用意しておけば，という考え方はイマドキ感がある．</p><ul><li><a href="https://arxiv.org/abs/1711.06025">arXiv</a></li><li><a href="https://github.com/lzrobots/LearningToCompare_ZSL">GitHub</a></li></ul></div></div><div class="slide_index">[#83]</div><div class="timestamp">2018.5.18 10:30:48</div></div></section><section id="MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos"><div class="paper-abstract"><div class="title">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</div><div class="info"><div class="authors">Z.Li and N.Snavely</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00607</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>画像における深度予測はCV分野において基本的なタスクである．既存の手法は学習データによる制約が伴う．今回提案する手法では，インターネットの画像をデータセットとするMVSの手法を改良し，既存の3D reconstructionとsemantic ラベルを組みわせて大規模な深度予測モデルであるMegaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションを用いた順序による深度関係を自動で拡張</li><li>MegaDepthが強力なモデルであることを示すために膨大なインターネット画像を使い検証</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>深度予測にsemantic ラベルを取り入れることで精度が向上．</li><li>semanticラベルを用いており，複雑背景における物体検出にも応用可能かも！！</li><li><a href="https://arxiv.org/pdf/1803.01599.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#84]</div><div class="timestamp">2018.5.18 02:33:22</div></div></section><section id="Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks"><div class="paper-abstract"><div class="title">Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks</div><div class="info"><div class="authors">FXuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>リアルタイムで顔の回転に頑健な顔検出を行うProgressive Calibration Network(PCN)を提案。PCNは3つのステージで構成されており、それぞれのステージでは検出された領域を0° or 180°回転させる、
0° or 90° or -90°回転させる、頭が上にくるように顔を回転させる、という処理をそれぞれ行う。
また各ステージ共通で検出された領域が顔であるか顔でないかという識別を行う。第1,2ステージで粗く回転を行うことで第3ステージにおける回転量と、
各ステージにおける顔識別の学習が容易になったことで、高精度かつリアルタイムに顔検出を行うことが可能となった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来手法であるデータオーギュメンテーション、角度の値域を分割してそれぞれの検出器を学習させる方法、角度の回転角を推定する流手法では、どれもネットワークが大きくなりすぎるためにリアルタイムでの実行が難しかった。</li><li>解像度が40x40以上の顔を検出。</li><li>state-of-the-artの手法と比べて同等の精度を達成し、かつGPUを使用した際の実行スピードは4.2倍となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li> <a href="https://github.com/Jack-CV/PCN">GitHub with Demos</a></li><li><a href="https://arxiv.org/abs/1804.06039">論文</a></li></ul></div></div><div class="slide_index">[#85]</div></div></section><section id="Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning"><div class="paper-abstract"><div class="title">Partially Shared Multi-Task Convolutional Neural Network with Local Constraint for Face Attribute Learning</div><div class="info"><div class="authors">Jiajiong Cao, Yingming Li, Zhongfei Zhang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアトリビュート推定に有効なネットワークであるPS-MCNN/-LCを提案。従来手法のMCNNでは、類似度の高いアトリビュートの識別率を高めるために、
類似度の高いアトリビュートのごとにグループを形成し、MCNNの高い層では各グループごとにCNNを形成して学習を行なっていた。
そのため低い層で得られていた特徴量が消失するという問題が起きていた。
これを解決するために、MCNNに対して各レベルで得られた特徴量を教諭するShared Netを導入したPS-MCNNを提案。
また同一人物において推定されたアトリビュート同士のロスをとるPS-MCNN-LCも提案した。
ネットワークの構築に関する議論も行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>同一人物において推定されたアトリビュート同士のロスをとることで、アトリビュートの空間を限定することが可能となるという考えのもとPS-MCNN-LCを提案している。</li><li>state-of-the-artに比べて、CelebAデータセットではPS-MCNN-LCが40種全てのアトリビュートにおいて最も高い精度を達成、LFWAデータセットではPS-MCNN/-LCを合わせて37種において最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>精度が上がったことはもちろんだが、既存研究であるMCNNのリミテーションを正確に見抜いてネット枠を改善している点が採択につながったと考えられる。</li><li><a href="http://person.zju.edu.cn/attachments/2018-04/01-1524825782-717898.pdf">論文</a></li></ul></div></div><div class="slide_index">[#86]</div></div></section><section id="Deep_Semantic_Face_Deblurring"><div class="paper-abstract"><div class="title">Deep Semantic Face Deblurring</div><div class="info"><div class="authors">Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に対してセマンティックセグメンテーション(face sparsing)を利用することで、モーションブラーが加えられた正面顔画像に対するCNNベースのデブラーリング手法を提案。
face sparsingによって顔のパーツの位置関係や形といった情報を利用することができると主張。
また学習の際には様々なカーネルサイズによるブラー画像を同時に与えるのではなく、
小さなカーネルサイズのブラー画像から順々に学習させるincremental trainingことでデブラーリング精度を向上させた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Semantic_Face_Deblurring.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ブラー画像はランダムな3D cameraの軌道によって与えられ、カーネルサイズは13x13~27x27までを学習させた。</li><li>ロスとしてデブラーリング画像のL1 loss, face parsing画像のL1 loss, adversarial loss, CNNの特徴量マップのL2 ロスを使用。</li><li>tate-of-the-artに比べてデブラーリング画像とソース画像のPSNR、SSIM、顔の検出率、個人認証の精度においてもっとも良い精度を達成し、それぞれ約5%, 5%, 28%, 4%向上した。</li><li>state-of-the-artに比べて実行スピードが約44%向上した。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>学習データを少しずつ変化させて、順々に最適化を行うincremental trainingは、学習データをパラメトリックに変化可能な他の問題に対しても有用なトレーニング方法だと思われる。</li><li><a href="https://arxiv.org/abs/1803.03345">論文</a></li></ul></div></div><div class="slide_index">[#87]</div></div></section><section id="Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning to Adapt Structured Output Space for Semantic Segmentation</div><div class="info"><div class="authors">Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Semantic Segmentationに関するDomain Adaptationの研究。Semantic Segmentationをsource domainとtarget domain間の空間的な類似性を持つ構造的な出力として考え、出力空間(prediction map)でのDomain Adaptationを行う敵対的学習手法を提案。低次特徴は利用せず、高次特徴のみを複数のDiscriminatorにより異なる空間解像度ごとに適応させる(Multi-level Adversarial Learning)。実験ではsynthetic-to-realとcross-cityでの比較を行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png" alt="Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像分類タスクを中心に発展していたDomain Adaptationを画素単位の構造予測が必要なSemantic Segmentationに適用した点。</p><p>Semantic Segmentationに限らず構造予測をするタスクへも容易に拡張ができる。</p><p>画像分類と比較して、アノテーションの労力がかかるため実用性・将来性がある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10349">arxiv</a></li><li><a href="https://github.com/wasidennis/AdaptSegNet">github</a></li></ul></div></div><div class="slide_index">[#88]</div><div class="timestamp">2018.5.17 20:28:45</div></div></section><section id="Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics"><div class="paper-abstract"><div class="title">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</div><div class="info"><div class="authors">Alex Kendall et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>学習時のタスクごとの重みによって精度がかなり変化する。そこでNNのマルチタスクモデルにおいて各出力を分布表現にし、その同時確率を最尤推定するように学習することで結果的にタスクごとの不確実性を考慮した重み付けを損失関数に課す。実験ではSemantic Segmentation, Instance Segmentation, Depth estimationのマルチタスク学習を行い、等しい重みや手動での重み設計時よりも良い結果となった。</p></div></div><div class="item2"><img src="slides/figs/Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png" alt="Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>モデルから各タスクに対して不確実性を表す値を同時に出力させる。回帰タスクの場合はこれが分散を表し、最終的には回帰出力値を平均とするガウス分布として表現する。識別タスクについては不確実性が分布の温度パラメータとして扱われる。これらの同時確率を最尤推定すると、通常の損失に対してタスクごとに適応的に重み付けされた損失を最適化していることになる。理論的にも妥当であり、精度向上は大きくチューニングの手間が省けるという点でかなり便利である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>簡単な実装でハイパーパラメータが減るという点でかなり有用に感じた。様々なマルチタスクで行った訳ではないのでこの手法の汎用性がきになる。結局、識別の場合は通常でも不確実性は考慮しているので、本質的に新しいのは回帰の場合である。</p><ul><li><a href="https://arxiv.org/abs/1705.07115">論文</a></li></ul></div></div><div class="slide_index">[#89]</div></div></section><section id="Compare_and_Contrast_Learning_Prominent_Visual_Differences"><div class="paper-abstract"><div class="title">Compare and Contrast: Learning Prominent Visual Differences</div><div class="info"><div class="authors">S.Chen and K.Grauman</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00112</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの画像間で最も顕著な違いは表せられるがその他の細かい違いは示されないことが多い．それに対して，より多くの違いによって画像を比較できるようなモデルの構築をした．また，そのモデルを使って，UT-Zap50K shoesとthe LFW10のデータセットを用いて評価したところSoTAであった．構築したモデルを画像記述と画像検索に導入し，拡張を図った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Compare_and_Contrast_Learning_Prominent_Visual_Differences.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像中から目立つ部分をアノーテーションで収集し，ランク付けすることでモデルの構築．</li><li>UT-Zap50K shoes（靴）とthe LFW10（顔）のデータセットを用いて評価．</li><li>画像記述と画像検索のタスクに応用し，拡張を図る</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像説明文に応用できればキャプショニングの幅を広げられそう．</p><ul><li><a href="https://arxiv.org/pdf/1804.00112.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#90]</div><div class="timestamp">2018.5.17 16:18:51</div></div></section><section id="Learning_Rich_Features_for_Image_Manipulation_Detection"><div class="paper-abstract"><div class="title">Learning Rich Features for Image Manipulation Detection</div><div class="info"><div class="authors">P. Zhou, X. Han, V.I. Morariu and L.S. Davis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像修正検出．修正箇所をちゃんと注目すべきで，リッチな特徴の学習が必要．修正後画像から修正領域を検出するtwo-stream Faster R-CNNを提案．
RGB stream：コントラスト差，不自然境界とかを捉える．Noise stream：ノイズの非一貫性を捉える．<a href="https://ieeexplore.ieee.org/document/6197267/">Steganalysis Rich Model</a>でとれたノイズ特徴に基づく．
そして，両者のバイリニアプーリングで共起性を捉える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Rich_Features_for_Image_Manipulation_Detection.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>修正箇所のノイズ感の差を見るアイデアは昔にあったが，それを導入したという温故知新．</li><li>実験によりリサイズや圧縮に対するロバスト性におけるSOTAを確認．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.04953">arXiv</a></li><li><a href="http://www.cosy.sbg.ac.at/~uhl/mmsec/LukFriSPIE06_v9.pdf">過去のノイズ感の差を使った画像加工領域検出の例</a></li></ul></div></div><div class="slide_index">[#91]</div><div class="timestamp">2018.5.17 15:16:28</div></div></section><section id="Real-Time_Seamless_Single_Shot_6D_Object_Pose_Prediction"><div class="paper-abstract"><div class="title">Real-Time Seamless Single Shot 6D Object Pose Prediction</div><div class="info"><div class="authors">Bugra Tekin et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>1枚のRGB画像から物体の6次元姿勢を推定する研究. CNN を用いた単一のネットワーク (YOLO v2 ベース) で RGB 画像から物体の 3D bounding box を直接推定する. post-process 無しで高精度な姿勢推定が可能なため, 実時間（従来手法の約５倍速）で従来手法と同程度の推定精度を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png" alt="fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークはRGB画像1枚の入力に対して, 各物体の制御点（3D bounding box 8点 と centroid 1点）の位置, カテゴリー, 推定の確信度を出力する.</li><li>推定された物体の9つの制御点の位置に対して PnP 問題を解くことで6次元姿勢を推定する.</li><li>物体の bounding box の情報から学習を行うので物体の詳細な3次元モデルが必要無い. また, テクスチャーが殆ど無い物体に対しても適用が可能.</li><li>物体が複数あった場合でも PnP 以外の部分の計算量は増えないので, 物体数に関わらず計算時間はほぼ一定.(従来手法の SSD-6D は線型に増加.)</li><li>LINEMOD や OCCLUSION データセットを用いた評価実験では従来手法 (BB8 や SSD-6D) と同等かそれ以上の精度を 50fps (SSD-6Dの約５倍) で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08848" target="blank">[論文] Real-Time Seamless Single Shot 6D Object Pose Prediction</a></li><li><a href="https://btekin.github.io/" target="blank">[著者HP] Bugra Tekin</a></li></ul></div></div><div class="slide_index">[#92]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="Video_Captioning_via_Hierarchical_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Video Captioning via Hierarchical Reinforcement Learning</div><div class="info"><div class="authors">Xin Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video captioning のための階層型強化学習フレームワークを提案. Caption を複数のセグメントに分割し, High-level の Manager Module が各セグメントのコンテキストをデザインし, Low-level の Worker Modeule が単語を生成することで順次セグメントを作成する. 提案手法は MSR-VTT データセット を用いた評価実験で既存手法よりも複数の評価尺度で良い結果となった. また, video captioning のための新しい大規模データセットを公開. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png" alt="fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video captioning の問題を強化学習の問題として定式化し直し, 効率的に学習をすることができる階層型強化学習手法を提案した.</li><li>High-level の Manager Module が目標を達成するために必要なゴールを設定し, Low-level の Worker Modeule がゴールを達成するための基本行動を行う. また, Internal Critic がゴールが達成されたかどうかの評価を行う.</li><li>Action recognition や segmentation で主に用いられている Charades データセットをもとにvideo captioning のための新しい大規模データセットを作成. 既存の MSR-VTT データセットよりも詳細で長い caption が与えられている.</li><li>MSR-VTT データセットを用いた評価実験では, 既存手法（Mean-Pooling, Soft-Attention, S2VT等）と比較して複数の評価尺度で最も良い結果を得た.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.11135" target="blank">[論文] Video Captioning via Hierarchical Reinforcement Learning</a></li><li><a href="http://www.cs.ucsb.edu/~xwang/#" target="blank">[著者HP] Xin Wang</a></li></ul></div></div><div class="slide_index">[#93]</div><div class="timestamp">2018.5.17 12:11:55</div></div></section><section id="Multi-view_Consistency_as_Supervisory_Signal_for_Learning_Shape_and_Pose_Prediction"><div class="paper-abstract"><div class="title">Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</div><div class="info"><div class="authors">Shubham Tulsiani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>１枚のRGB画像から物体の形状とカメラ姿勢の両方を推定する研究. 異なる視点から見たときの一貫性(具体的には物体の輪郭または深度情報の一貫性)を教師情報として用いるため, 従来手法と異なり学習時に物体の３次元形状と姿勢のいずれについても直接の教師データも必要としない.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png" alt="fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体の形状とカメラ姿勢の両方を推定するタスクに置いて, 直接の教師データを用いずに学習する方法を提案した.</li><li>学習時の入力は同一の物体を異なる位置から撮影したRGB画像２枚と２枚目の画像の物体の Mask または Depth 画像.</li><li>１枚目の画像から３次元形状, ２枚目の画像からカメラ姿勢をそれぞれ推定し, 推定された形状を推定された姿勢から見た時に, 与えられたマスク画像と同じ結果が得られるように学習を行う.</li><li>ShapeNet データセットを用いた評価実験では, 直接の教師あり学習を行った手法とほぼ同等の結果であった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.03910" target="blank">[論文] Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</a></li><li><a href="https://shubhtuls.github.io/mvcSnP/" target="blank">[Project Page]</a></li><li><a href="https://github.com/shubhtuls/mvcSnP" target="blank">[Code]</a></li></ul></div></div><div class="slide_index">[#94]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing"><div class="paper-abstract"><div class="title">PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing</div><div class="info"><div class="authors">Dan Xu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"></div><h1>概要</h1><p>CNNに対して中間的に法線方向推定と輪郭推定も加えることで最終的にdepth推定とscene parsingの精度を向上させる。法線方向と輪郭についてはdepthとscene parsingのラベルから計算可能であるので追加にアノテーションする必要はない。
NYUD-v2とCityscapesにおいてSoTA。  </p></div><div class="item2"><img src="slides/figs/PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png" alt="PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>中間的に推定した結果を元に最終的な目的タスクを出力するが、その中間出力として3つのパターンを考えた(タスクをに分けずconcat, タスクごとにconcat, attention機構を取り入れたconcat)。 attention機構を取り入れたconcatが最も良い結果となった。シンプルな手法だが、実験結果が良いので評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>「distillation」という言葉を用いているが、生徒モデルと教師モデルがあるようなdistillation手法は使われておらず、単に複数の中間タスクからのMulti-modalな情報の統合に対してその言葉が使用されている。
単に通常のマルチタスク推定に中間タスクを導入したのみでかなりシンプルな印象。</p><ul><li><a href="https://scirate.com/arxiv/1805.04409">論文</a></li></ul></div></div><div class="slide_index">[#95]</div></div></section><section id="Convolutional_Sequence_to_Sequence_Model_for_Human_Dynamics"><div class="paper-abstract"><div class="title">Convolutional Sequence to Sequence Model for Human Dynamics</div><div class="info"><div class="authors">Chen Li, Zhen Zhang, Wee Sun Lee, Gim Hee Lee</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時空間的な特徴を捉えて、長期のモーション予測を行う研究である（ここではいかに最初の限られた情報量のみでシーケンスを推定できるかどうかについて検証を行なっている）。この課題に対し、Convolutional Long-term Encoderを用いてより長期的な隠れ変数をデコーダにより推定する。このエンコーダ-デコーダ構造にて短期〜より長期的な変数の予測を可能にする。本手法では主にRNNベースのSequence-to-SequenceなモデルにConvolutionalな要素を加えたことが技術的発展であると主張。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ConvSeq2Seq.png" alt="180517ConvSeq2Seq"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>より長期の（といっても数秒間のシーケンス？）人物モーション予測（ここでは人物姿勢位置を予測）を実現したことが課題設定として大きい。手法としてはConvolutional Long-term Encoderやその抽象化された特徴をデコーダにより長期隠れ変数を推定。Human3.6MやCMU Motion Capture datasetにて高い精度を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Short-termからLong-term（Short-term: 〜3秒、Long-term: 5秒〜; 明確な定義はなされていないが。。）の行動/姿勢の予測はまだまだ未解決だし、何を予測するかに関しての定義づけ自体の整備も曖昧なままである。まだまだ参入の余地が残されているように見える。</p><ul><li><a href="https://arxiv.org/abs/1805.00655">論文</a></li><li><a href="https://github.com/chaneyddtt/Convolutional-Sequence-to-Sequence-Model-for-Human-Dynamics">GitHub</a></li></ul></div></div><div class="slide_index">[#96]</div><div class="timestamp">2018.5.17 01:05:51</div></div></section><section id="LSTM_Pose_Machines"><div class="paper-abstract"><div class="title">LSTM Pose Machines</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu, Jiahao Pang, Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Convolutional Pose Machine (CPM)のCNN部分を再帰的ネットであるLSTM (Long-short term memory)により置き換えた人物姿勢推定手法。時系列的に連続するフレーム（e.g. t, t+1, t+2）の入力に対して処理を実行し姿勢を推定する。CPMとは基本となるアーキテクチャの考え方（multi-stage algorithm）は同様であるが、それぞれのステージ間でパラメータを共有している点で異なる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516LSTMPoseMachine.png" alt="180516LSTMPoseMachine"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CPMと同じmulti-stageの姿勢推定学習を、LSTMの構造にて実現したことが技術的なポイントである。さらに、CPMとは異なりステージ間でパラメータを共有することで精度向上が見られたと説明。Penn Action datasetやJHMDB datasetにて最高精度を叩き出した。JHMDBにて93.6@PCK(=0.2)、Penn Actionにて97.7@PCK(=0.2)を記録。さらに、各フレーム時のメモリチャンネルの挙動も可視化し、どのような際に成功するか/失敗するかを明らかにした。複雑姿勢（複雑背景？）の際にはエッジに着目していて、姿勢推定が成功する際にはピンポイントで関節位置を回帰する傾向にある。処理速度の面においても本論文の技術では25.6msで動作した（CPMは48.4ms）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アーキテクチャ自体上手くいったモデルを異なるアプローチ（この場合はConvolutionalなモデルをRecurrentに変更）で実行して、どういった改善ができるかを試しているところが面白い。LSTMの場合にはステージ間でパラメータを共有するところがポイントであった。その上で精度を向上している点が実行力に優れていると言える。</p><ul><li><a href="https://arxiv.org/abs/1712.06316">論文</a></li><li><a href="http://www.jimmyren.com/">著者</a></li><li><a href="https://github.com/lawy623/LSTM_Pose_Machines">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=-sP3LWl6Ul0">YouTube</a></li></ul></div></div><div class="slide_index">[#97]</div><div class="timestamp">2018.5.16 17:25:49</div></div></section><section id="DecideNet_Counting_Varying_Density_Crowds_Through_Attention_Guided_Detection_and_Density_Estimation"><div class="paper-abstract"><div class="title">DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation</div><div class="info"><div class="authors">Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>混雑時の人数カウントにおける問題点を解決するため、End-to-Endで学習可能なDecideNet（DEteCtIon and Density Estimation Network）を提案する。混雑時の人数カウントでは、従来（１）人物検出では認識ミスによる過不足によりカウントを誤ってしまう、（２）回帰ベースの手法では人物が存在しない領域が蓄積されると実際のカウントよりも多く集計されてしまう、という問題が存在した。DecideNetでは検出ベース/回帰ベースを別々に行い、それらの結果を総合してカウントを行うという点で従来法を解決していると言える。実験では本論文で提案のDecideNetが混雑時の人数カウントにおいてもっとも優れた精度を達成したと主張。検出/回帰の手法としてはFaster R-CNN/RegNetを適用している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516DecideNet.png" alt="180516DecideNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3つのベンチマーク（Mall, ShanghaiTech PartB, WorldExpo10 dataset）においてState-of-the-artな精度を達成すると同時に、混雑時の人数カウントの問題と異なるアプローチを同時実行して相補的なアプローチDecideNetを提案したことが採択された大きな理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異なる複数のアプローチを統合して最高精度を達成するためには、その分野における積み重ねと実装力が必要である。論文の書き方と合わせて鍛えていくことで毎回難関国際会議に突破できる力がつくと思われる。</p><ul><li><a href="https://arxiv.org/pdf/1712.06679.pdf">論文</a></li><li><a href="www">Project</a></li><li><a href="www">GitHub</a></li></ul></div></div><div class="slide_index">[#98]</div><div class="timestamp">2018.5.16 11:59:31</div></div></section><section id="Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation"><div class="paper-abstract"><div class="title">Cascaded Pyramid Network for Multi-Person Pose Estimation</div><div class="info"><div class="authors">Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu and J. Sun</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>複数人ポーズ推定には，キーポイントの半／全遮蔽や，複雑な背景といった要素(hard keypoints)が問題になる．Cascaded Pyramid Networkを提案．
hard keypointに対応するためのもの．2つの構造からなる．</p><ul><li>GlobalNet<br>ピラミッド構造をしていて，遮蔽などの無いシンプルなキーポイントの検出として作用する．この時点ではhard性にはあまり対応していない．</li><li>RefineNet<br>hard keypointを考慮した層．
GlobalNetのピラミッドな特徴を拾って，ResNetのBottleneckにかける．
ここで，何もしないとシンプルキーポイントだけ見てしまうので，損失関数の計算時，online hard keypoints miningする．
テスト時のロスを参考にオンラインでhard keypointを選択，選んだキーポイントのものだけバックプロパゲーションにまわすという作業．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規ネットワーク構造の提案</li><li>MS COCO keypoint benchmarkにてSOTA</li><li>実験を結構頑張っている様子．online hard keypoint miningの有無に関する議論などある．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>online hard keypoint miningについて実装可能なレベルでは詳しく書いてなかった．コード読めということか．</p><ul><li><a href="https://arxiv.org/abs/1711.07319">arXiv</a></li><li><a href="https://github.com/chenyilun95/tf-cpn">GitHub</a></li></ul></div></div><div class="slide_index">[#99]</div><div class="timestamp">2018.5.16 18:24:47</div></div></section><section id="One-shot_Action_Localization_by_Learning_Sequence_Matching_Network"><div class="paper-abstract"><div class="title">One-shot Action Localization by Learning Sequence Matching Network</div><div class="info"><div class="authors">H. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ある長い動画中から指定した対象動画と同じActionを探してくるOne-shot Action Localizationの研究．
Matching Networkという手法がベースになっていて，それを動画のAction Localizationに応用．
基本的には動画をEncoding (Video Encoder) して，
類似度を計算 (Similarity Network) して，ラベリング (Labeling Network)．
長い方の動画はSliding Windowで分割 (Proposals) して，Proposalsと指定動画の間で類似度を計算．
Encoderは動画でよくやられるTwo-stream CNNとLSTMを利用．
学習はMeta Learningの形式で定式化され，End-to-Endで学習可能．
</div></div><div class="item2"><img src="slides/figs/One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png" alt="One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Deep時代になってからほとんどやられていなかったOne-shot Action Localization (Action search)</li><li>ProposalsのEncoding，類似度計算，ラベリングと3つすべてが微分可能でEnd-to-Endで学習可能</li><li>普通のTemporal Action LocalizationのSOTA手法よりもOne-shotの設定では高い性能を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20One-shot%20action%20localization%20by%20learning%20sequence%20matching%20network.pdf">論文（著者ページ）</a></li><li>やっている事自体は至って普通のアプローチに感じる</li><li>End-to-End, Meta Learningと今風の形で実現できているのが評価されているのかな</li></ul></div></div><div class="slide_index">[#100]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="Exploit_the_Unknown_Gradually_One-Shot_Video-Based_Person_Re-Identification_by_Stepwise_Learning"><div class="paper-abstract"><div class="title">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</div><div class="info"><div class="authors">Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ワンショット学習（One-shot Learning）により動画像における人物再同定（person re-identification）を実行する論文。ラベルなしのtracklets（人物から抽出した動線）が容易かつ事前に手に入ることから、このtrackletsを徐々に改善しつつ人物同定率を高めるようにCNNを学習していく手法を提案する。本論文での学習では、最初にひとつのラベルを用いて初期化したあと、（１）信頼度の高い少量のサンプル（簡単なサンプル）に対して擬似ラベルを付与、（２）擬似ラベルを含めたラベルを元にカテゴリを更新してより難しいサンプルも取り込む、を繰り返して学習を行う。実験的に擬似ラベルを選択する方法についても議論している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516OneShotREID.png" alt="180516OneShotREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>正解ラベルが付与されたある画像一枚を準備するだけで擬似ラベルを推定して徐々に学習を進めていくワンショット学習を提案した。人物再同定の問題においては有効な解決策であることを示したことがCVPRに採択された基準である。ワンショット学習によりrank-1の精度が21.46@MARS dataset、16.53@DukeMTMC-VideoReID datasetであり、コードも公開されている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ワンショットのラベルと信頼できる擬似ラベルから徐々に概念を獲得するのはうまいやり方。あらゆる枠組みで用いることができそう。</p><ul><li><a href="https://yu-wu.net/pdf/CVPR2018_Exploit-Unknown-Gradually.pdf">論文</a></li><li><a href="http://xuanyidong.com/publication/cvpr-2018-eug/">Project</a></li><li><a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually">GitHub</a></li><li><a href="https://yu-wu.net/">著者</a></li></ul></div></div><div class="slide_index">[#101]</div><div class="timestamp">2018.5.16 11:18:38</div></div></section><section id="PoseTrack_A_Benchmark_for_Human_Pose_Estimation_and_Tracking"><div class="paper-abstract"><div class="title">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</div><div class="info"><div class="authors">Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, Bernt Schiele</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画シーケンスにおいて2D姿勢推定のベンチマークを提供する。本論文で提案するベンチマークでは特に、人物の重なりを含む混雑シーン、密なアノテーションを提供する。さらに右の画像で示すようにドメイン依存していない多様な（diverse）シーンを捉えつつ姿勢アノテーション数でも有数、1画像に対する複数人物/ビデオに対するラベルづけにも対応している。トータルでは23,000画像に対して153,615人の姿勢アノテーションを行なった。チャレンジとしては単一フレームに対する姿勢推定（single-frame pose estimation）、ビデオに対する姿勢推定（pose estimation in videos）、姿勢トラッキング（pose tracking）を提供し、評価用サーバも提供する。同DBに対するベンチマーキングではOpenPoseにも導入されているPAFを改良したML-LAB（引用52）がトップ（70.3@mAP）、Mask R-CNNをベースにしたProTracker（引用11）は64.1@mAPであった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515PoseTrackBenchmark.png" alt="180515PoseTrackBenchmark"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>大規模かつ静止画ではなく動画に対する人物姿勢データセットを構築し、さらには評価サーバを提供、さらに最先端手法に関するベンチマーキングを行なっていることが新規性およびCVPRに通った理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの比較図に多様なドメインから収集（diverse）と書かれているが、これらをすべて統合すると相当な量のデータになるのでは？（だれかやってそう）もしくはドメインを合わせれば学習の効果がありそう。</p><ul><li><a href="https://arxiv.org/abs/1710.10000">論文</a></li><li><a href="www.posetrack.net">Project</a></li><li><a href="https://scholar.google.com/citations?hl=ja&amp;user=6rl-XhwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">著者</a></li></ul></div></div><div class="slide_index">[#102]</div><div class="timestamp">2018.5.15 12:16:20</div></div></section><section id="Camera_Style_Adaptation_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Camera Style Adaptation for Person Re-identification</div><div class="info"><div class="authors">Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-ID（人物再同定）は異なるカメラ間で同一人物を対応づける問題設定であり、画像の質や形式が異なるため非常に困難である。本論文ではカメラ間のスタイル変換を行うことでカメラに依存せず安定して認識できる特徴抽出（camera-invariant descriptor subspace）を行い、人物再同定の問題を高度に解決することを目的とする。この問題に対してCycleGANを適用することでカメラ間の特徴変換を捉えた上で、データ拡張を行う。存在するノイズへの対策として有効と思われる正則化:Label Smooth Regularization (LSR)を適用する。LSRを使用する場合では学習データに対するオーバーフィッティングが見られず、有効な手法であることが判明した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515CamStyleTransferREID.png" alt="180515CamStyleTransferREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CycleGANによるカメラ間のスタイル変換を実現してデータ拡張、LSRによりノイズへの対応を行いオーバーフィッティングを回避していることが新規性である。また、人物再同定においてその高い精度（Market-1501のrank-1にて89.49%、DukeMTMC-reIDのrank-1にて78.32%）を実現している。さらに、LSRを用いることでベースラインからの精度向上が見られる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CycleGANが学習データを増やすという意味でも取り上げられている。例えば東大井上氏の論文もCycleGANを用いてデータ拡張を行い、スタイルの異なる画像に変換することでデータアノテーションの労力を削減している。</p><ul><li><a href="https://arxiv.org/abs/1711.10295">論文</a></li><li><a href="https://github.com/zhunzhong07/CamStyle">Project</a></li><li><a href="http://zhunzhong.site/">著者</a></li><li><a href="https://github.com/junyanz/CycleGAN">CycleGAN</a></li><li><a href="https://naoto0804.github.io/cross_domain_detection/">東大井上氏論文</a></li></ul></div></div><div class="slide_index">[#103]</div><div class="timestamp">2018.5.15 11:50:25</div></div></section><section id="Dense_3D_Regression_for_Hand_Pose_Estimation"><div class="paper-abstract"><div class="title">Dense 3D Regression for Hand Pose Estimation</div><div class="info"><div class="authors">Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>単眼距離画像から簡易的かつ効果的に3次元手部姿勢推定を実施する技術について提案する。従来の3D手部姿勢回帰の手法と比較して、本論文ではピクセルごとの（pixel-wise）解析を可能とする。手法としては2D/3Dの関節点を返却するカスケード型の多タスクネットワーク（multi-task network cascades）を提案し、End-to-Endでの学習を行う。その後MeanShiftによりピクセルごとの姿勢位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515DenseHandReg.png" alt="180515DenseHandReg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のほとんどの手法では関節レベルの手部姿勢推定であったのに対して、本論文で提供する技術はピクセルベースの3D手部姿勢推定であることが新規性である。ピクセルごとの回帰はノンパラメトリックな手法を構築した。MSRA/NYU hand datasetにてすべての従来手法よりも高い精度で手部姿勢推定を実行した。また、ICVL hand datasetでは（頭打ちになっていると思われる）論文5には及ばなかったが、接近した精度を叩き出すことに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>HandTrackingも激戦であるが、本論文ではピクセルベースの回帰とState-of-the-art（最高精度）という強みを活かして論文を通している。</p><ul><li><a href="https://arxiv.org/abs/1711.08996">論文</a></li><li><a href="https://github.com/melonwan/denseReg">GitHub</a></li></ul></div></div><div class="slide_index">[#104]</div><div class="timestamp">2018.5.15 11:20:15</div></div></section><section id="Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition"><div class="paper-abstract"><div class="title">Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition</div><div class="info"><div class="authors">Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からshapeの三次元復元を行う際に、画像から個人性(顔の形など)を反映した3Dモデルと、個人性以外(表情など)を反映した3Dモデルをencoderで別々に生成しdecoderで三次元復元を行う手法を提案。
生成された顔のshapeは三次元復元におけるstate-of-the-artよりも高い精度を達成し、
また生成されたshapeによる顔認証においても多くの既存手法より高い精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の三次元復元の手法では顔のディティールは再現するものの、アラインメントなどの個人性の再現が完全ではなかった。提案手法では個人性を反映したモデルとそうでないモデルを分離して学習させることで、この問題を解決した。</li><li>様々なデータセットにおいて、生成された顔の3D shapeはstate-of-the-artに比べて最も低いaccuracyを達成。</li><li>生成された3D shapeにおけるランドマークなどのaccuracyにおいてももっとも低い値を獲得。</li><li>生成された3D shapeによる個人認証においても、多くの既存手法よリも高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>disentangleのファクターとして個人性を選んだのはあくまで人間であって、今後の発展ではもっと優秀なファクターを深層学習が導き出してくれるかもしれない。</li><li><a href="https://arxiv.org/abs/1803.11366">論文</a></li></ul></div></div><div class="slide_index">[#105]</div></div></section><section id="Seeing_Small_Faces_from_Robus_Anchors_Perspective"><div class="paper-abstract"><div class="title">Seeing Small Faces from Robust Anchor’s Perspective</div><div class="info"><div class="authors">Chenchen Zhu, Ran Tao, Khoa Luu, Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>アンカーベースで画像中の小さな顔に対する検出精度を向上させる手法を提案。アンカーベースの手法では画像中に等間隔で並べられた点(アンカー)を中心とした矩形によって物体を検出する。
アンカーによる検出精度を評価する数値としてExpected Max Overlapping(EMO) scoreを提案し、
EMOを深層学習に学習させることで、小さな顔(16X16)に対する検出精度を向上した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Small_Faces_from_Robus_Anchors_Perspective.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のアンカーベースの手法ではIoUを学習させていたため、解像度が16x16などの小物体に対する学習が困難であったが、EOM scoreを学習させることで小物体の検出精度が大きく向上。</li><li>従来のアンカーベースの手法よりも検出精度が向上、特に小さな顔に対する検出精度が大きく向上したが、実行時におけるスピードは従来手法と同程度。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09058">論文</a></li></ul></div></div><div class="slide_index">[#106]</div></div></section><section id="Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification"><div class="paper-abstract"><div class="title">Exploring Disentangled Feature Representation Beyond Face Identification</div><div class="info"><div class="authors">Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に関するタスクに汎用的な特徴量を得ることができるDistilling and Dispelling Autoencoder(D2AE)を提案。Encoderによって顔から個人性を表現する特徴量(性別など)と個人性を排除した特徴量(表情など)を抽出する。
取得された特徴量により、個人識別、アトリビュートの識別、顔のアトリビュート編集、顔の生成を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Encoderによって顔から個人性を表現する特徴量と個人性を排除した特徴量を抽出することで、これらの特徴量により様々な顔のタスクを行うことが可能となった。</li><li>LFWデータセットにおける個人識別でaccuracyが約99.0%、TPRが約98.0%であり、既存手法と同等の精度を達成。</li><li>LFWA、CelebAデータセットにおける顔のアトリビュート認識は83.16%となり、アトリビュートを学習していないにも関わらず、アトリビュートを学習した既存手法と同等の精度を達成した。</li><li>顔のアトリビュートの編集、アトリビュートを保ったアイデンティティーの転写といった編集が可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>このネットワークを用いて他の物質の個人性を抽出して何が出てくるのか興味がある。例えば顔の代わりに魚を学習させて、鯛ごとの個人性、マグロごとの個人性を抜き出してみるなど。</li><li><a href="https://arxiv.org/abs/1804.03487">論文</a></li></ul></div></div><div class="slide_index">[#107]</div></div></section><section id="Robust_Facial_Landmark_Detection_via_a_Fully-Convolutional_Local-Global_Context_Network"><div class="paper-abstract"><div class="title">Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network</div><div class="info"><div class="authors">D. Merget, M. Rock and R. Gerhard</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>FCNの中にKernel convolutionを暗黙的に入れ込み，大域的特徴情報を残すというアイデアを提案．Conv層で局所特徴を取り，KernelConvでそれをブラーにかけ，DilatedConv層で大局的特徴をリファインするという構造．</p><p>特に解像度に独立・きっちりROIがとれない・要複数検出対応・要遮蔽対応な顔ランドマーク検出タスクに有効．KernelConvによって勾配平滑化と過学習抑制が働き収束しやすくなる．
アウトライア弾きのために，事前処理ステップにおいて，ネットワーク出力をシンプルなPCAベース2D形状モデルにフィットしておく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Robust_Facial_Landmark_Detection_via_a_Fulaly-Convolutional_Local-Global_Context_Network_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来は階層構造やプーリング，統計モデルへのフィッティングで対応していたところを，FCNに直に大域的特徴を入れ込むようにした．</li><li>構造単純化により，学習パラメータが少なくなる．</li><li>顔ランドマーク検出に適用してみて，いくつかのSOTAな手法より良い性能を出した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.mmk.ei.tum.de/fileadmin/w00bqn/www/Verschiedenes/cvpr2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#108]</div><div class="timestamp">2018.5.15 13:31:33</div></div></section><section id="Direction-aware_Spatial_Context_Features_for_Shadow_Detection"><div class="paper-abstract"><div class="title">Direction-aware Spatial Context Features for Shadow Detection</div><div class="info"><div class="authors">X.Hu, L.Zhu, C.W.Fu, J.Qin, and P.A.Heng</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.04142</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>影の周りには様々な背景があり，セマンティクスを理解しなければならないため，影の検出は基本的のようで困難である．それに対して，方向認識の方法で画像のコンテキストを解析することで影検出手法を提案する．空間のRNN内のコンテキスト特徴が密集している箇所にアテンションを導入することで方向認識の手法を定式化する．97％の検出精度と38％のバランスエラー率の低減を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direction-aware_Spatial_Context_Features_for_Shadow_Detection.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>空間的なRNNに対してアテンション機構を設計しdirection-aware spatial context (DSC)モジュールを構築することで方向認識の方法で空間的なコンテキストを学習．</li><li>重み付き交差エントロピー損失が影と影でない領域における検出精度のバランスが取れるように設計．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>影の検出だけでなく，顕著性検出およびセマンティックセグメンテーションなどの他のアプリケーションで使用する事もできそう．</p><ul><li><a href="https://arxiv.org/pdf/1712.04142.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#109]</div><div class="timestamp">2018.5.15 02:31:23</div></div></section><section id="Learning_to_Act_Properly_Predicting_and_Explaining_Affordances_from_Images"><div class="paper-abstract"><div class="title">Learning to Act Properly: Predicting and Explaining Affordances from Images</div><div class="info"><div class="authors">Ching-Yao Chuang, Jiaman Li, Antonio Torralba and Sanja Fidler</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>現実の多様な場面での環境の物体に対するアフォーダンスの推定する研究。ADE20kを基にしたADE-Affordanceというデータセットの提案。このデータセットはリビングなどの屋内から、道路や動物園などの屋外まで幅広いタイプの画像とそのannotationで構成。また、画像中の物体に対してアフォーダンスの推理を行うための，画像からcontextual informationを伝えるGraph Neural Networksの提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Act_Properly.PNG" alt="Learning_to_Act_Properly.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ある場面の状況下での適切でない行動の理由について身体的や社会的な観点から説明・画像上のある物体に対してだけでなくその場面を全体としてとらえてアフォーダンスの推論を行っている．
・物体間の依存関係をモデル化することでアフォーダンスとその説明を生成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07576">論文</a></li><li><a href="http://www.cs.utoronto.ca/~cychuang/learning2act/">Project Page</a></li></ul></div></div><div class="slide_index">[#110]</div><div class="timestamp">2018.5.14 19:28:40</div></div></section><section id="Discriminability_objective_for_training_descriptive_captions"><div class="paper-abstract"><div class="title">Discriminability objective for training descriptive captions</div><div class="info"><div class="authors">R.Luo, B.Price, Scott Cohen and G.Shakhnarovich</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.04376</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>現在のキャプショニング方法は，2つの異なる画像であるにも関わらず，同じキャプションを生成してしまうなどの弁別性にかけている．それに対して，学習の際に画像とキャプションの一致度を直接関連付けるLossを組み込むことによって他のキャプションよりも弁別性のあるキャプションを生成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Discriminability_objective_for_training_descriptive_captions.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>機械翻訳の評価指標であるBLEU，METEOR，ROUGE，CIDErやSPICEにおいても既存のキャプショニング手法よりも高いスコアを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これにより，同じような画像に対するバリエーションが増え，ユニークなイメージキャプショニングの幅が広がった!!</p><ul><li><a href="https://arxiv.org/pdf/1803.04376.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#111]</div><div class="timestamp">2018.5.14 19:39:23</div></div></section><section id="A_Face-to-Face_Neural_Conversation_Model"><div class="paper-abstract"><div class="title">A Face-to-Face Neural Conversation Model</div><div class="info"><div class="authors">Hang Chu, Daiqing Li, Sanja Fidler</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された会話文に対して、その返答と適切な顔のジェスチャーを生成する手法。映画データセットを元にトレーニングデータセットを構築。
RNNに対してディスクリミネータの出力を報酬とした強化学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Face-to-Face_Neural_Conversation_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は会話文のみ、あるいは動画。動画が入力の場合には同じテキストでも発話者の表情によって出力される返答文が変化する。</li><li>出力が会話文だけの場合よりも、同時に顔のジェスチャを生成した方が生成された会話文がよりGTの会話文に近くなったことを主張。</li><li>データセットは250種類の映画データセットMovieQAにおいて単一人物が写っているシーンにおいて顔向、ジェスチャカテゴリ、タイムスタンプを取得することで構築した。</li><li>生成された返答文の妥当性を評価するためにamazon mechanical turkを実施。GANを導入したことで返答文の多様性、妥当性がstate-of-the-artの手法に勝った。</li><li>このモデルで学習したボットとリアルタイムで会話することも可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>デモを見るとまだ返答文自体には違和感があるが、顔のジェスチャがつくことで会話している気分になる。ボットのモデルが謎のおじさん。</li><li><a href="http://chuhang.github.io/files/publications/CVPR_18_2.pdf">論文</a></li><li><a href="http://www.cs.toronto.edu/face2face">Project page</a></li></ul></div></div><div class="slide_index">[#112]</div></div></section><section id="CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion"><div class="paper-abstract"><div class="title">CosFace: Large Margin Cosine Loss for Deep Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔認識のための新たなロス関数としてソフトマックス関数をベースとしたLarge Margin Cosine Loss(LMCL)を提案した研究。LMCLはソフトマックス関数の指数部分を重みベクトルWと特徴量ベクトルxの内積においてWとxのノルムを1とし、定数mを引いた関数。
認識タスクでは異なるクラスタ間の距離を遠く、同じクラスタ間の距離を近くする、という基本的な考えがある。
LMCLはこの考えを元に上記のようにL2正則化を施すことで、Wとxのノルムに左右されることなくWとxの角度空間においてクラスタの分離を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソフトマックス関数において重みベクトルの大きさ、入力特徴量のノルムを除外することで、cosの影響を最大限に大きくしWとxの角度空間におけるマージンの最大化を提案。</li><li>face identification(この人はAさんであるか？)、face verification(この人は女性であるか？)の多くのタスクにおいて,ソフトマックス関数由来のロス関数、state-of-the-artの手法よりも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>汎用的な認識タスクに使用できそうだが、顔認識に限定したのはデータセットや既存研究との比較のため？</li><li><a href="https://arxiv.org/abs/1801.09414">論文</a></li></ul></div></div><div class="slide_index">[#113]</div></div></section><section id="Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models"><div class="paper-abstract"><div class="title">Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</div><div class="info"><div class="authors">Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Cen Wang, jingyi Yu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>異なる位置の点光源１個によって照らされた５枚の正面顔画像から高品質な３次元顔形状を最適化によって復元する研究。被写体の正面に5つのLED点光源が配置されいている照明環境で撮影を行う。
入力画像に対して3D morphable modelを適用することで簡易的な3次元顔形状を生成し、法線マップ組み合わせることで点光源の位置をピクセル単位で推定する。
またセマンティックセグメンテーションを行うことで体毛が生えいてる領域とそうでない領域に分割し、体毛が生えている領域にはフィルタ処理を行うことでノイズを除去する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>顔画像からいきなり光源位置を推定するのではなく、一度morphalbe モデルに生成することで推定精度が大きく向上。</li><li>3Dスキャンなどの大掛かりな装置を必要としない。</li><li>顔の小じわ、毛穴、まつ毛なども再現するほど高品質な3次元顔形状を復元。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>推定された光源位置自体の精度結果を見てみたかった。</li><li>配置する点光源の位置については特に言及がなかったが、配置による影響の比較結果がみてみたかった。</li><li><a href="https://arxiv.org/abs/1711.10870">論文</a></li></ul></div></div><div class="slide_index">[#114]</div></div></section><section id="FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors"><div class="paper-abstract"><div class="title">FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</div><div class="info"><div class="authors">Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔の超解像度化を学習させる際にランドマーク、パーツの位置推定を同時に行うネットワーク(FSR Net)を提案した研究。同ネットワークをベースにFSR GANも提案。
また生成された高解像度画像に対する評価尺度として生成画像とGTにおけるランドマークのNRMSE、顔パーツに対するセマンティックセグメンテーション画像(parsing)に対するPSNR、SSIM、MSEを提案。
GANベースの手法では高精細な画像が生成されるがPSNR、SSIMが低くなり、MSEをロスとしたネットワークではPSNR、SSIMは高いがボケた画像になってしまう、というジレンマから上記の評価尺度を導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像は16x16の様々な顔むきの画像、出力は128x128に超解像度化された画像。</li><li>state-of-the-artの手法よりもSSIM、PSNRが高く、また新たな評価尺度として提案したランドマーク、face parsingの位置推定も既存手法よりも高い精度となった。</li><li>新たに提案した評価指標自体の妥当性は、FSR GANとFSR Netを比べた際に、FSR Netの方がボケた画像を生成したにも関わらずSSIM、PSNRが高く、一方でFSR GANの方がランドマーク、face parsingの推定精度が高かったことを根拠に主張している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>比較画像において既存手法の画像があまりにもボケているため、既存手法のコントリビューションを確かめるという意味でも調査が必要と感じた。</li><li><a href="https://arxiv.org/abs/1711.10703">論文</a></li><li><a href="https://github.com/tyshiwo/FSRNet">GitHub</a></li></ul></div></div><div class="slide_index">[#115]</div></div></section><section id="_2D3D_Pose_Estimation_and_Action_Recognition_using_Multitask_Deep_Learning"><div class="paper-abstract"><div class="title">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</div><div class="info"><div class="authors">Diogo C. Luvizon, David Picard, Hedi Tabia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>相互に関連性がある2D/3D姿勢推定+人物行動認識を多タスク学習（Multi-task Learning）により最適化した論文である。それぞれで学習を行ったときよりも高い精度を実現することを明らかにし、複数のデータセットにてState-of-the-artな性能を叩き出した。2Dと3Dの姿勢推定、人物行動の特徴量が相補的に補完し合い特徴学習をより高度にしている？</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180514PoseActionMultiTask.png" alt="180514PoseActionMultiTask"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>姿勢推定（しかも3D姿勢推定も含めて）や人物行動認識を単一の枠組みで解決、さらには多タスク学習により別々に学習したときよりも高い精度でふたつの問題を解決した。さらに複数のベンチマーク（姿勢推定：Human3.6M, MPII/行動認識：PennAction, NTU）にて最高精度も叩き出したことが採択の理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>動画シーケンスから姿勢と行動を同時出力する、ありそうでなかった研究である。先にやったもの勝ちだが、高度な最適化を実施し特に最高精度を出すのは難しい。CVPRではState-of-the-artとなるかどうかがひとつの採点基準でもある（が、全てではない）ため、実装力をつけておくに越したことはない。</p><ul><li><a href="https://arxiv.org/abs/1802.09232">論文</a></li><li><a href="https://www.youtube.com/watch?v=MNEZACbFA4Y">Youtube</a></li></ul></div></div><div class="slide_index">[#116]</div><div class="timestamp">2018.5.14 13:04:47</div></div></section><section id="Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Kuniaki Saito et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>目的のタスクに特化した２つの分離境界を利用したドメイン適応手法。従来の埋め込み空間においてドメイン間の分布を単に近づける方法に対して、あるタスクと解くための分離境界を考慮して適応を行う。この枠組みでの適応はtargetでの損失の上界を下げる埋め込み空間への写像を求める作業と類似している。さまざまなドメイン適応のベンチマークにおいてSoTA。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Source(S)で学習を行った二つの識別境界を作成する。その識別器がTarget(T)で異なる判断を行ったサンプル(discrepancy)はSの分布とは乖離している領域であると考えられる。以下のような敵対的な適応を行う。(1) TにおけるDiscrepancyが増加するよう識別境界を学習。(2) Discrepancyが減少するように埋め込み空間を学習。(3)Sでの識別は常にうまくいくよう学習。
識別境界を考慮した適応という新規性、理論的な背景、論文の明快さ、精度としての結果が揃っている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アイデアの面白さと同時に論文が非常にわかりやすかった。識別境界はあくまで埋め込み関数を適化するために得たものなので、この枠組みで得られる最終的なもの以外(得られた埋め込み空間上で新たに学習したもの)でもうまくいくのではないかと感じた。</p><ul><li><a href="https://arxiv.org/abs/1712.02560">論文</a></li></ul></div></div><div class="slide_index">[#117]</div></div></section><section id="Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders"><div class="paper-abstract"><div class="title">Generative Non-Rigid Shape Completion with Graph Convolutional Autoencoders</div><div class="info"><div class="authors">Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>非剛体的な変形を伴う３Dオブジェクトの形状補完．部分的な形状補完のための学習ベースの手法としてgraph-convolutionを含むVAEを提案した．推論時には，既知の部分的な入力データに合う形状を生成できる変数を潜在空間で探すように最適化する．結果として人体と顔の合成データ，リアルなスキャンデータに対する補完が可能であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>従来手法よりも優れている点</h1><ul><li>訓練中に部分的な形状を見る必要なしに，任意スタイルで一部として切り出されたデータを扱えること</li><li>人間以外にも，任意の種類の３Dデータに適用できる手法であること</li><li>形状補完はデータに適合する解が複数ある問題であり，複数のもっともらしい解を生成し，この問題に対応できること</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.00268">arXiv</a></li></ul></div></div><div class="slide_index">[#118]</div><div class="timestamp">2018.5.13 16:22:39</div></div></section><section id="Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Eye In-Painting with Exemplar Generative Adversarial Networks</div><div class="info"><div class="authors">Brian Dolhansky, Cristian Canton Ferrer</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要，新規性</h1><p>eye-Inpaintingを行う手法．顔のようなそれぞれ固有の特徴を持つ画像においてのInpaintingで，従来のDNNによる手法は新しい顔を生成するなどidentityを保たなかった．exemplar informationを利用するconditional GAN（ExGANs）を提案．参照画像やperceptual codeというidentifying information（exemplar information）をGANの複数の箇所で利用することで，perceptualに優れ，identityを反映した結果を生成することができた．identifying informationをGANの複数の箇所で利用することが新しい．さらに，将来の比較のためにEye-Inpaintingのタスクの新しいベンチマークとデータセットを用意した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法概要</h1><p>cGANの一種．参照画像のIdentityを符号化するネットワークと，Generator，Discriminatorから成る．identifying informationを生成に利用するだけでなく，DiscriminatorやPerceptual lossの算出にも利用している．参照画像をベースにした場合と符号をベースにした場合にアプローチを分けている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.03999">arXiv</a></li></ul></div></div><div class="slide_index">[#119]</div><div class="timestamp">2018.5.13 16:12:11</div></div></section><section id="Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks</div><div class="info"><div class="authors">Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>特徴ベクトルのクラスタリングでGANの入力ベクトルを作成する学習方法で，ロゴの生成と操作が可能とした．ロゴのデータは高マルチモーダルのデータであり，従来のSoTAではmode collapseを起こしてしまうが，提案する学習方法では多様なロゴを生成する．iWGANをCIFER-10で学習するとき，提案する学習方法によって，Inception scoreでSoTA達成．Contribution:</p><ul><li>600k以上のロゴを収集してデータセットを構築</li><li>マルチモーダルなロゴデータでのGANの学習方法</li><li>潜在空間の探索によって，インタラクティブなロゴ生成</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks_fig.png" alt="Image"></p><p>上段はデータセットから．下段が生成結果．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Clustered GAN Trainingと読んでいる．GANのネットワークは，DCGANとimproved Wasserstein GAN with gradi- ent penalty (iWGAN)を利用．オートエンコーダーの中間特徴ベクトルもしくは，Resnetの特徴ベクトルをクラスタリングして，Generatorの入力ベクトルとする．このクラスタリングでセマンティックに意味のあるクラスタを形成し，GANの学習を向上させることが可能．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://data.vision.ee.ethz.ch/sagea/lld/">データセット</a></li><li>ロゴ・ジェネレーター・インターフェースも用意されている．スライダーを動かして，生成結果を操作できる</li><li><a href="https://arxiv.org/abs/1712.04407">arXiv</a></li></ul></div></div><div class="slide_index">[#120]</div><div class="timestamp">2018.5.13 16:03:23</div></div></section><section id="Multi-Agent_Diverse_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Multi-Agent Diverse Generative Adversarial Networks</div><div class="info"><div class="authors">Arnab Ghosh, Viveka Kulharia, et al.</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>多様で意味のあるサンプルを生成可能な，複数のGeneratorと１つのDiscriminatorから成るGAN(MAD-GAN)を提案．一つのGeneratorが一つの構成要素を担当する混合モデルとしてはたらく．いくつかの従来のGAN手法と比較実験を行い，MAD-GANは多様なモードを獲得できることを確認．さらに，理論的な分析も行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Agent_Diverse_Generative_Adversarial_Networks_fig.png" alt="Image">それぞれの行が異なるGeneratorによって生成した結果．行はそのGeneratorにランダムなノイズzを入力して生成した結果．マルチビューなデータセットから異なるモードを異なるGeneratorが学習していることを確認できる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>Multi-agent GAN．複数のGeneratorと１つのDiscriminatorで構成．</li><li>Generator同士は，最終層以外は重みを共有している．</li><li>複数のGeneratorの生成サンプルと真のサンプルをDに入力し，Discriminatorは，FakeとRealの判別だけではなくて，そのFakeの生成サンプルを与えるGeneratorがどれであるかも予測する．これによって，複数のモードがある時，個別のモードに対してそれぞれのGeneratorを振り分けるようにDiscriminatorが学習する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>image-to-image変換,multi-view生成， face generationなど多数の実験を行っている．</li><li>展望は，MAD-GANでは複数のGeneratorを使うことになるが，いくつのGeneratorが必要なのかを推定できるようにすること．</li><li><a href="https://arxiv.org/abs/1704.02906">arXiv</a></li></ul></div></div><div class="slide_index">[#121]</div><div class="timestamp">2018.5.13 15:50:21</div></div></section><section id="SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis"><div class="paper-abstract"><div class="title">SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis</div><div class="info"><div class="authors">Wengling Chen, James Hays</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチから写真を生成する手法の提案．50のカテゴリの写真を生成することができる．スケッチに対して，自動でデータ拡張をする方法を示し，その拡張方法がタスクに有効であることを示す．さらに追加の目的関数と新しいネットワーク構造も提案．マルチスケールの入力画像を入れることで情報の流れを向上させている．結果はまだphotorealisticとは言えないが，従来手法よりリアルでinception scoreの高い結果を得た．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>データ拡張の方法として，エッジ検出などのいくつかの処理を組み合わせている．</li><li>ネットワーク構造はU-net構造だが，各ブロックで入力画像で条件付けを行うのが特徴．以前の層で抽出された特徴マップと比べ新しい特徴量を入力画像から選択的に抽出するための内部マスクを学習するため，Masked Residual Unitというブロックモジュールを導入した．（DCGAN, CRN, ResNetとの比較がある）</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GeneratorにもDiscriminatorにも途中で画像やラベルの情報をinjectionする方法が増えている印象．</li><li>sketchから似ている写真を検索してくるという方法がこれまでよく研究されていた．今回は，スケッチから新しく写真を生成する（質はまだ低い）</li><li><a href="https://arxiv.org/abs/1801.02753">arXiv</a></li></ul></div></div><div class="slide_index">[#122]</div><div class="timestamp">2018.5.13 15:37:35</div></div></section><section id="ScanComplete_Large-Scale_Scene_Completion_and_Semantic_Segmentation_for_3D_Scans"><div class="paper-abstract"><div class="title">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</div><div class="info"><div class="authors">Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Juergen Sturm, Matthias Nießner</div><div class="conference">CVPR 2018</div><div class="paper_id">584</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>部分的なシーンの3Dデータからシーンの幾何及びボクセルごとのセマンティック情報をコンプリートする手法ScanCompleteを提案した．</li><li>従来，シーンの3次元情報を完全に収集するのが非常に困難，シーンの３次元のデータの膨大さや形状情報のバリエーションの多さは従来のシーン補完に対して困難な問題設定である．そういったため，シーンのコンプリートでは出力の質が低いという問題点がある(contentsとして応用するレベルではない)．こういった困難を解決するため，提案手法は①trainとtestデータの入力解像度を異なる値に設定し， testの場合シーンのサイズの変化を対応できるようにする．②coarse-to-fineなfully convolution 3DCNNを用いて，グローバルなシーンの構造特徴および精密な局所的補間をできるようにする．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/scancomplete.png" alt="scancomplete"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>異なる入力シーンのサイズを自由に対応できる（最大70×60×3m くらいまでできる）</li><li>従来の手法：3D-EPN,SSCNetなどの従来手法と比べ，scene completion, semantic labeling両方精度がSOTA</li><li>出力結果が3D Contentsとして応用できるレベル</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.10215">論文</a></li></ul></div></div><div class="slide_index">[#123]</div><div class="timestamp">2018.5.14 14:43:15</div></div></section><section id="Learning_from_Millions_of_3D_Scans_for_Large-scale_3D_Face_Recognition"><div class="paper-abstract"><div class="title">Learning from Millions of 3D Scans for Large-scale 3D Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模3D顔データセットを構築し、そのデータによってトレーニングされたCNNが高い3D顔認識精度を持つことを示した論文。従来の3D顔データセットはデータ数が少なく、最も多いND-2006でも888アイデンティティー・13540種類のみであったが、本論文で構築されたトレーニング用データセットはおよそ10万アイデンティティー・310万種類。
このトレーニングデータを用いてCNNを学習させることで、認識精度は98.74%となりstate-of-the-artよりも優っていることを確認した。
また既存の3D顔データセットをマージすることで、1853アイデンティティー・31K種類のテスト用3D顔データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180513Learning_from_Millions_of_3D_Scans-scale_3D_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>トレーニング用の3D顔データは1000人の3Dスキャンデータに対して、変形に要するエネルギーがもっとまた商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。も高くなる顔のペアを合成して生成。また商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。
前者は別の顔を識別するため、後者は似た顔を識別する目的で用意されたデータである。
生成された顔に対して水平方向、垂直方向から15度ずつ撮影することで、計100,005アイデンティティー・3,169,275種類の3D顔データを生成。</li><li>既存の3D顔認識・2D顔認識手法に対してオープン・クローズドテスト両方における精度を比較したところ、提案モデルがもっとも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.05942">論文</a></li></ul></div></div><div class="slide_index">[#124]</div></div></section><section id="Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</div><div class="info"><div class="authors">Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像(128x128)のリアルタイムなタイムラプス動画の生成をするGANを提案．最初のフレームを与えると，近未来のフレームを生成する．新規性としては，</p><ul><li>タイムラプスデータセットを作成</li><li>タイムラプス向きの近未来予測ネットワークを提案（Multi-stage Dynamic Generative Adversarial Network (MD-GAN) ）</li><li>モーションのモデリングにGram matrixを導入し，実世界ビデオのモーションを模倣するためのadversarial ranking lossを提案</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks_fig.png" alt="fig"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>corse-to-fineの２ステージアプローチのGAN．ステージを分けた狙いとしては，１ステージ目でコンテンツの生成を行い，２ステージ目でモーションのモデリングを行うこと．１ステージ目のU-net風のネットワークでは3D convolutions と deconvolutions を含んでいる．</p><p>２ステージ目のDiscriminatorとして，モーションパターンをモデル化するためにGram matrix使って，adversarial ranking lossを算出する．1ステージの出力ビデオ，2ステージ目の出力ビデオ，真のビデオからランキングをとる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1709.07592">arXiv</a></li></ul><p>タイムラプス用のGANが初めて提案されたことが評価されたのかなという印象．定量的な評価はメインがPreference Opinion Scoreで, 他はMSE, PSNR and SSIM．</p></div></div><div class="slide_index">[#125]</div><div class="timestamp">2018.5.13 12:45:36</div></div></section><section id="Hyperparameter_Optimization_for_Tracking_with_Continuous_Deep_Q-Learning"><div class="paper-abstract"><div class="title">Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</div><div class="info"><div class="authors">Xingping Dong et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Object Tracking 手法において用いられる複数の Hyperparameter を強化学習によって各シークエンス毎に最適化する手法を提案. Hyperparameter の選択を Action, Tracking の精度の良さを Reward として, Normalized Advantage Functions (NAF) を用いた強化学習を行なっている. また, Heuristic を導入することで, 学習の遅さの問題を緩和した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png" alt="fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Object Tracking における Hyperparameter の最適化問題を強化学習の問題として定式化した.</li><li>上記の問題を既存の強化学習手法である NAF　(連続な行動が取れるように拡張された Q 学習の手法) を用いて解いた.</li><li>強化学習を適用した際に, 状態空間の次元の多さなどに由来する学習速度の遅さを huristic を導入することで緩和した.</li><li>OTB-2013 や VOT-2015 などのデータセットを用いて既存研究(Siam-py等)と比較. 同程度の速度で, 正確性とロバスト性の両方に置いて既存手法を上回った. </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20Hyperparameter%20optimization%20for%20tracking%20with%20continuous%20deep%20Q-learning.pdf" target="blank">[論文] Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</a></li><li><a href="https://arxiv.org/abs/1603.00748" target="blank">[関連論文: NAF] Continuous Deep Q-Learning with Model-based Acceleration</a></li></ul></div></div><div class="slide_index">[#126]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="Tangent_Convolutions_for_Dense_Prediction_in_3D"><div class="paper-abstract"><div class="title">Tangent Convolutions for Dense Prediction in 3D</div><div class="info"><div class="authors">Maxim Tatarchenko et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>３次元データを扱う新しい convolutional の方法 "Tangent Convolution" を提案. 全ての点の近傍点を仮想的な接平面上に射影し, 接平面上で畳み込みを行う. 接平面は法線ベクトルが計算できれば構成する事ができるため, 複数のデータ形式に対して同様に適用が可能. また, 事前計算を行う事によって大規模なデータベースに対しても効率的に計算を行う事が可能となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png" alt="fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力データの形式は法線ベクトルを近似的に求められるもの (point clouds, meshes, dpolygon soup) であればなんでも良い.</li><li>事前計算を行う事によって大規模なデータ（数百万オーダーの点群）も効率的に扱う事ができる.</li><li>提案手法の有効性を示すために Tangent Convolution を用いたネットワークを Semantic 3D Scene Segmentation のタスクに置いて既存手法 (PointNet, ScanNet, OctNet) と比較し, 複数の評価尺度に置いて最も良い精度となった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vladlen.info/papers/tangent-convolutions.pdf">[論文]</a></li></ul></div></div><div class="slide_index">[#127]</div><div class="timestamp">2018.5.12 11:33:55</div></div></section><section id="Im2Pano3D_Extrapolating_360_Structure_and_Semantics_Beyond_the_Field_of_View"><div class="paper-abstract"><div class="title">Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View</div><div class="info"><div class="authors">Shuran Song, Andy Zeng, Angel Chang, Manolis Savva, Silvio Savarese, Thomas Funkhouser</div><div class="conference">CVPR 2018</div><div class="paper_id">466</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・部分的に観測されたシーン(RGB-D)から，full sceneの構造及びセマンティックラベルを推定する新規な問題設定”semantic-structure view extrapolation”及びフレームワークを提案した．</p><p>・従来のview extrapolationは画像のboundryの色情報しか行わず，シーンのセマンティック構造に対してextrapolationを行う研究がない．そこで，この論文で，著者達がsemantic-structure view extrapolationを提案し，50%以下のシーンの観測データから構造及びセマンティックをextrapolation予測する．</p><p>・提案フレームワークは：①一枚のマルチチャンネルpanorama画像でシーンの情報(RGB，構造，セマンティック)を表示する；②3次元構造をデプスのような詳細な三次元情報を用いずに，3次元平面方程式で表示する．③マルチロス関数(ピクセルレベル，グローバルコンテキスト)を用いる．</p><p>・提案フレームワークの考え方は入力と出力を一枚のマルチチャンネルpanorama画像として表示し，encoder-decoderにより，欠損した入力からfullなpanorama画像を出力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Pano3D.png" alt="Im2Pano3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・CG データセットSUNCG及びリアルシーンデータセットMatterport3Dを用いて従来手法よりシーンの構造及びセマンティックの予測が優位．</p><p>・一枚のマルチチャンネルpanorama画像でシーンの情報を表示し，シーンの情報を固定なサイズにできるので，2次元畳み込みを用いられる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・マルチチャンネルpanorama画像でシーンの情報を保存するところが賢い</p><p>・提案フレームワークは構造的に理解しやすい，実装してみたい</p><ul><li><a href="https://arxiv.org/abs/1712.04569">論文</a></li><li><a href="http://im2pano3d.cs.princeton.edu/">ポロジェクト</a></li></ul></div></div><div class="slide_index">[#128]</div><div class="timestamp">2018.5.11 17:40:13</div></div></section><section id="Deep_image_prior"><div class="paper-abstract"><div class="title">Deep Image Prior </div><div class="info"><div class="authors">Dmitry Ulyanov et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_image_prior.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEで近づけるように学習するだけである。注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元されたような画像が得られる。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。着眼点や面白い実験方法に加え結果も伴っている研究</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。畳み込み処理の派生(Deformable convなど)でのpriorの検証も気になる。</p><ul><li><a href="https://arxiv.org/abs/1711.10925">論文</a></li></ul></div></div><div class="slide_index">[#129]</div></div></section><section id="Edit_Probability_for_Scene_Text_Recognition"><div class="paper-abstract"><div class="title">Edit Probability for Scene Text Recognition</div><div class="info"><div class="authors">F. Bai, Z. Cheng, Y. Niu, S. Pu and S. Zhou</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>OCRのstate-of-the-artな手法として，encoder-decoderで文字カテゴリごとのAttentionを取ってからテキスト認識をするvisual attentionベーステキスト認識があるが，
ある文字がよく見えなかったり1文字でも複数ピークが出てしまったりする問題はある．
GTとの差を取るとして，エンコード後の文字列で比較する<a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%BC%E3%83%99%E3%83%B3%E3%82%B7%E3%83%A5%E3%82%BF%E3%82%A4%E3%83%B3%E8%B7%9D%E9%9B%A2">編集距離</a>を取ることが考えらえるが，
本稿ではVAで出る尤度分布で比較する，編集確率（Edit Probablity）を提案する．
これにより，字抜けや余分な字を拾ってしまうような誤認識に強い文字認識を実現可能．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Edit_Probability_for_Scene_Text_Recognition_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Attentionベーステキスト認識においてstate-of-the-artな性能．</li><li>まさに正統進化といえる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>正統進化を，他のラボが，1年未満に行ってしまっているあたり，CV分野の流れの早さがうかがえる．</p><ul><li><a href="https://arxiv.org/abs/1805.03384">arXiv</a></li><li><a href="https://arxiv.org/abs/1706.01487">Visual attention models for scene text recognition</a>（<strong>ICDAR2017</strong>）</li></ul></div></div><div class="slide_index">[#130]</div><div class="timestamp">2018.5.10 18:29:27</div></div></section><section id="iVQA_Inverse_Visual_Question_Answering"><div class="paper-abstract"><div class="title">iVQA: Inverse Visual Question Answering</div><div class="info"><div class="authors">Feng Liu, Tao Xiang, Timothy Hospedales, Wankou Yang, Changyin Sun</div><div class="conference">CVPR 2018</div><div class="paper_id">1199</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQA問題の逆問題iVQA設定及びモデルを提案し (画像及び回答文から，質問文を生成する)，更に iVQAもVQAと同じく“視覚-言語”の理解のベンチマック問題設定になれると指摘した．</p><p>・iVQAタスクに用いられるmulti-modal dynamic inferenceなフレームワークを提案した．提案フレームワークは回答文を生成する段階で，“回答文”，“生成した部分的な質問文”によって導かれ動的に画像attentionを調整できる．</p><p>・更に，回答文の従来の自然言語的評価に， ランキングベースなiVQAタスクの回答文を評価できる指標を提案した．その指標により，などの面を評価できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/iVQA.png" alt="iVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・近年，従来のVQAの成功がデータセットバイアス及び質問文からの情報理解，画像の内容に対する理解がまだVQAにおいて深く利用されていないことが指摘された．そこで，画像と回答文から質問文を予測する問題設定iVQAを提案した， iVQAタスクにおいてはVQAと比べ，①画像内容の理解の要求が高い，②また回答文が常に短いので，質問文と比べよりスパースな情報抽出しかできないため，回答文に頼りすぎることにならない．③モデルの推定及びreasoning能力が更に必要である．</p><p>・提案フレームワークの各パーツ(dynamic attention, multi-modal inferenceなど)の有効性に関してAblation　studyを詳しく行った. 説得力がある．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・実験を通して，iVQAをVQAとヒュージョンしたら， VQAの精度を挙げられることを証明した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・VQAの問題点を深く理解した上での新規問題設定．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・新奇な考え方・詳しい分析実験・論文の理解しやすさなどが非常に良い</p><ul><li><a href="https://arxiv.org/abs/1710.03370">論文</a></li></ul></div></div><div class="slide_index">[#131]</div><div class="timestamp">2018.5.10 15:08:46</div></div></section><section id="Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation"><div class="paper-abstract"><div class="title">Sketch-a-Classifier: Sketch-based Photo Classifier Generation</div><div class="info"><div class="authors">C. Hu, D. Li, Y. Song, T. and T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>手書き画像から，書いたものの判別をする画像分類器を出力するメタ学習の提案．学習していない手書きカテゴリでも，そのカテゴリの画像分類器が出力される．3つの枠組みが作れる．
(1)スケッチ画像カテゴリ分類モデルを入力
(2)スケッチ画像を入力
(3)コースなリアル画像分類モデル＋スケッチ画像を入力</p><p>枠組みとしては，<a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/eccv2016_learntolearn.pdf">Model Regression Network</a>による．論文では，SVMパラメータの学習を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>多様性がある．作ったモデルの性質がよく把握されている</li><li>知識転用の新しい形が見える</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#132]</div><div class="timestamp">2018.5.10 13:49:15</div></div></section><section id="ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing"><div class="paper-abstract"><div class="title">ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing</div><div class="info"><div class="authors">C. Lin, E. Yumer, O. Wang, E. Shechtman and S. Lucey</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像合成の際に，背景に対して位置やサイズ感などが正しくなるように幾何的変換を求め，修正を加えてくれるGANを提案．たとえば，家具が適切な場所に置かれたり，メガネが適切に掛けられたりする．</p><p>構造的には複数のSpatial Transformer Networkをジェネレータとして組み込んでいることが特徴．複数のSTNにおける，反復<a href="https://en.wikipedia.org/wiki/Image_warping">画像ワーピング</a>（画像変形方法の一つ）と逐次学習を導入している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像変換が得られるので，間接的に高解像度画像に適用可能</li><li>ナイーブな単ジェネレータよりも高性能．</li><li>大きな差には弱い．奇抜なデザインのものや，大きな移動</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#133]</div><div class="timestamp">2018.5.10 12:27:25</div></div></section><section id="Two_can_play_this_Game_Visual_Dialog_with_Discriminative_Visual_Question_Generation_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Two can play this Game: Visual Dialog with Discriminative Visual Question Generation and Visual Question Answering</div><div class="info"><div class="authors">Unnat Jain, Lana Lazebnik, Alex Schwing</div><div class="conference">CVPR 2018</div><div class="paper_id">705</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・Visual Dialogタスクに用いられる質問の回答文と質問文を両方予測できるネットワークを提案した．</p><p>・提案フレームワークは100個の回答文(質問文)から正解を予測する(discriminative).  提案フレームワークは質問文，画像，キャプション，QA履歴，選択などの情報をsimilarity+Fusionネットにより100次元のベクトルを生成し，正解ラベルとのcross-entropy誤差を求める．</p><p>・また，従来Visual Dialogの質問文を評価する指標がない，著者達が質問文を評価できる“VisDial-Q evaluation protocol”を提案した．提案protocolは質問文を100個に固定し，予測した質問文がどれくらい通常の人により提出される可能性が高いかにより評価を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualDialog_DVQG_DVQA.png" alt="VisualDialog_DVQG_DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・同じネットワークで質問文と回答文を両方予測できる．</p><p>・質問文を評価できる指標の提案．</p><p>・Discriminative VQAタスクにおいて， VisDial評価指標は従来手法(HRE, MN, HCIAE-D-NP-ATT)より良い性能を達成した．</p><p>・VQGタスクにおいて，提案した評価指標“VisDial-Q evaluation protocol”により55.17% recall@5 と 9.32 mean rankを達成した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11186">論文</a></li></ul></div></div><div class="slide_index">[#134]</div><div class="timestamp">2018.5.10 04:08:59</div></div></section><section id="Social_GAN_Socially_Acceptable_Trajectories_with_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</div><div class="info"><div class="authors">Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese and Alexandre Alahi</div><div class="conference">CVPR2018</div><div class="paper_id">234</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>人や自律移動プラットフォームが，移動している人を避けるにはいくつかの経路が考えられる．本手法は，人間の経路予測にシーケンス予測とGANを組み合わせたツールを用いて，複数の経路予測を行う．Recurrent sequence-to-sequence modelは，複数の人の間で情報を集約するための新しいプーリング手法を用いて，観測者の行動を予測する．そして，GANを用いてもっともらしい行動をいくつか予測する．予測された経路はDiscriminatorへ入力され，Fake/Real判別をしGANを訓練していく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180509_SocialGAN1.jpg" alt="20180509_SocialGAN1.jpg"><img src="slides/figs/20180509_SocialGAN2.jpg" alt="20180509_SocialGAN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Generatorでは，複数の人が同時にどう動くか予測するために，Encoderの各LSTMの出力をまとめるプーリングモジュールを導入した．Discriminatorは，経路そのものがFake（人として社会的にあり得ない行動）またはReal（あり得る行動）を判断する．ETHやHOTELなどのデータセットを用いて評価実験を行った．12ステップ後のAverage Displacement Error（全ての時間での真値と予測値の誤差）は0.58（Social LSTM: 0.72），Final Displacement Error（最終目的とでの真値と予測値の誤差）1.18（Social LSTM: 1.54）となった．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>GANを使う手法は多く出てきているが，これは面白い応用方法だと思った．Discriminatorをどうやって学習していくかが肝になりそう．</p><ul><li><a href="https://arxiv.org/abs/1803.10892">arXiv</a></li></ul></div></div><div class="slide_index">[#135]</div><div class="timestamp">2018.5.9 01:45:09</div></div></section><section id="Neural_Baby_Talk"><div class="paper-abstract"><div class="title">Neural Baby Talk</div><div class="info"><div class="authors">Jiasen Lu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><p>画像内で検出した物体から文章を生成するイメージキャプショニングタスクを行うための新たなフレームワークの構築を行った．単語が格納されるスロットを文章内に生成し，生成したスロットを満たすように検出した物体を当てはめていくことでキャプションを行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508NBT.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>検出された物体の名称が入るスロットを最初に生成し，生成したスロットを満たしていくことでキャプションを行う手法が新しい．</p><p>イメージキャプショニングタスクにおいてFlickr30KとCOCOデータセットでSOTAを達成した.</p></div></div><div class="item4"><div class="text"><p></p><h1>コメント・リンク集<li><a href="https://arxiv.org/pdf/1803.09845.pdf">論文</a></li><li><a href="https://github.com/jiasenlu/NeuralBabyTalk">github</a></li></h1></div></div><div class="slide_index">[#136]</div></div></section><section id="Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image"><div class="paper-abstract"><div class="title">Attentive Generative Adversarial Network for Raindrop Removal from a Single Image</div><div class="info"><div class="authors">Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>写真から雨粒を除去する手法の提案</li><li>このタスクが難しいのは，<ol><li>どの領域が，雨粒によって隠されているか不明なこと</li><li>雨粒に隠された背景側の情報がないこと</li></ol></li><li>GAN，LSTMを利用</li><li>Generatorは，Attentive-Reccurent networkとContextual Autoencoderから構成</li><li>はじめにAttentive-Reccurent networkでattention mapを生成　次にContextual Autoencoderで，mapと入力画像から雨粒除去後の画像を生成
　attention mapは，Discriminatorの中間出力とMSE lossを取る際にも利用</li><li>visual attentionという情報によって，<ol><li>Generatorでは雨粒の領域と，周辺の構造にアテンションをより向けることができる</li><li>Discriminatorは復元した領域をより局所的に評価を行える</li></ol></li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image_fig.png" alt="Item2Image"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><ul><li>GeneratorとDiscriminatorの両方でvisual attentionを利用するようにしたこと</li><li>自作の1119枚の雨粒ありと無しのペア画像を用意し学習に利用</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10098">arxiv</a></li></ul></div></div><div class="slide_index">[#137]</div><div class="timestamp">2018.5.8 21:05:34</div></div></section><section id="Deformable_GANs_for_Pose-based_Human_Image_Generation"><div class="paper-abstract"><div class="title">Deformable GANs for Pose-based Human Image Generation</div><div class="info"><div class="authors">Aliaksandr Siarohin, Enver Sangineto, Ste ́phane Lathuilie`re, and Nicu Sebe</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>与えられたポーズ情報を条件として人物画像を生成するタスクを扱う．任意ポーズへの変形タスクで発生する，（服などの）変換前のピクセルと変換後のピクセルの対応が不整列である問題に対応するために，deformable skip connectionを対案する．
従来手法と比べ，条件画像の服の色・テクスチャを保存して別ポーズの画像を生成できている．
人物画像の生成に限らず，キーポイントを与えることのできる不整列のオブジェクトであれば，この手法が適用できると著者らは考えている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig2.png" alt="fig2"><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig3.png" alt="fig3"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net likeのEncoder-Decoder, GANdeformable skip connectionについて．
変換前後の両方のポーズ情報が既知なので，キーポイント周辺のピクセルが変換前から変換後にどこへ移動するか知ることができる．したがって，キーポイントの座標からアフィン変換を求め，畳み込みから得た特徴マップをアフィン変換することで，服の色やテクスチャを変換前から変換後の画像に移して生成できる．
Encoderの特徴量をアフィン変換し，Decoderの特徴量にskipするのがdeformable skip connectionである．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.00055">arXiv</a></li><li><a href="https://github.com/AliaksandrSiarohin/pose-gan">プログラム</a></li></ul></div></div><div class="slide_index">[#138]</div><div class="timestamp">2018.5.8 15:39:41</div></div></section><section id="VizWiz_Grand_Challenge_Answering_Visual_Questions_from_Blind_People"><div class="paper-abstract"><div class="title">VizWiz Grand Challenge: Answering Visual Questions from Blind People</div><div class="info"><div class="authors">Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo</div><div class="conference">CVPR 2018</div><div class="paper_id">491</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・盲人に集められたVQAタスクのデータセットVizWiz（画像と音声質問文）を提案した．VizWizが31,000枚の盲人が携帯により撮影し，画像ごとに画像を撮影した盲人が提出した音声質問文一つ付き．質問文ごとに，10個の回答文がアノテーションされている．</p><p>・従来のVQAデータセットほぼ人工設定により作成された方が多く，また現実環境の盲人ユーザを対象に“goal oriented”なVQAデータセット未だにない．そこで，盲人がカメラにより周囲環境を撮影し，環境を理解することを目的にして，盲人ユーザにより集められた画像及び質問文のデータセットを構築した．</p><p>・ 盲人ユーザにより撮影されたのでVizWizは画像の質が良くなく，又質問文が音声情報なので，はっきり発音が取れない場合などの問題点がある．提案データセットで現状のVQAモデルで検証した結果，性能が従来のデータセットで検証した性能より劣るので， VizWizが将来的の盲人のためのVQA応用に新たな挑戦を提出した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VizWiz.png" alt="VizWiz"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めての盲人により撮影及び質問したVQAデータセット．</p><p>・従来のVQAデータセットと比べ，もっと画像の周りの環境に関する質問文が多い．</p><p>・従来のVQAデータセットとの質問文の詳細的な特徴比べも行っている．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・盲人のためのVQAシステム構築に有力なデータセット．</p><ul><li><a href="https://arxiv.org/pdf/1802.08218.pdf">論文</a></li></ul></div></div><div class="slide_index">[#139]</div><div class="timestamp">2018.5.8 14:33:52</div></div></section><section id="Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points"><div class="paper-abstract"><div class="title">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</div><div class="info"><div class="authors">F. Baradel et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">RNNベースの行動認識を提案．
学習はRGB-Dを使うが，テスト時にはRGBのみを使うという設定．
テスト時にRGB-Dが使えてPose情報が使えればそれを使えばいいが，
それが使えないときもあるからそれに変わる手法を提案するという主張．
Poseでの間接位置に代わって，
Attentionベースでフレーム中から重要な局所要素 (Glimpse) を抽出＆トラッキング．
Glimpseの集合に基いて行動を認識するというフレームワーク．
Glimpseの抽出やトラッキングはそれぞれRNNベースで行う手法になっている．
</div></div><div class="item2"><img src="slides/figs/Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png" alt="Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>姿勢の代わりに別の局所要素を使うフレームワークを提案</li><li>Attention, External Memoryといった流行り?の要素が詰め込んである</li><li>RGB-D行動認識データセットにおいてRGBのみの利用でSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://perso.liris.cnrs.fr/christian.wolf/papers/cvpr2018.pdf">論文（著者版）</a></li><li><a href="https://arxiv.org/abs/1802.07898">論文 (Long-ver., arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=7yPDYYhaYI4">動画 (YouTube)</a></li><li>姿勢ベースの行動認識を姿勢を使わずにやるような話に近い印象</li></ul></div></div><div class="slide_index">[#140]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="High-Resolution_Image_Synthesis_and_Semantic_Manipulation_with_Conditional_GANs"><div class="paper-abstract"><div class="title">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div><div class="info"><div class="authors">Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>GANの枠組みにてセマンティックラベルからの高精細画像（HD-Image）生成に関する研究。意味ラベルからリアルな画像を生成するのみならず、インタラクティブな操作で画像生成をコントロールすることも可能。Residual blocksにより構成されるエンコーダ/デコーダ構造を（入力をスケールが異なる画像として）入れ子構造にしデコーダ直前の中間層で統合して画像生成を実行する。さらに、ラベルのみならずインスタンスレベルの特徴量を用いることで写実性が向上したと主張（論文中図4では物体境界面あたりに出ているボケが綺麗になっている）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508HDImageGAN.png" alt="180508HDImageGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法より、見た目の画像生成が明らかに良くなり、高画質の画像を対象にしても画像生成ができるようになった。従来手法（pix2pix（論文中文献21）, CRN（論文中文献5））さらに、インタラクティブな操作により生成画像を所望の結果に近づけることができる。動画像を見れば従来手法よりも鮮明になっていることは明らかであり、アーキテクチャや生成に関する知見も得ている。CVPRでoralになるための準備やプレゼンが論文中にも書かれていると感じた。やはりNVIDIAはずるいと言われるくらいの計算機環境が揃っているのではないか。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これはもう、学習画像として使えるのでは？（すでにだれか使って精度検証しているのでは？）</p><ul><li><a href="https://arxiv.org/abs/1711.11585">論文</a></li><li><a href="https://tcwang0509.github.io/pix2pixHD/">Project</a></li><li><a href="https://github.com/NVIDIA/pix2pixHD">GitHub</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/544">arXivTimes</a></li><li><a href="https://www.youtube.com/watch?v=3AIpPlzM_qs">YouTube</a></li></ul></div></div><div class="slide_index">[#141]</div><div class="timestamp">2018.5.8 12:46:17</div></div></section><section id="Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras"><div class="paper-abstract"><div class="title">Five-point Fundamental Matrix Estimation for Uncalibrated Cameras</div><div class="info"><div class="authors">D. Barath</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの未キャリブレーションカメラにおいて，<strong>5点のみ</strong>で基礎行列を推定する手法を提案．</p><p>回転不変な特徴点（SIFT等）を使う．3点は平面にあれば，他2点はどこでも可能．グラフカットRANSACのようなロバスト対応点推定と組み合わせれば，state-of-the-artな性能が出る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>通常，7点や8点取るアルゴリズムが用いられるが，リーズナブルな制約で，少ない情報のみでキャリブレーションできるのはうれしい．例えば図のようにキャリブレーションボードを小さくできたりする．
大変有用な研究成果．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00260">arXiv</a></li></ul></div></div><div class="slide_index">[#142]</div><div class="timestamp">2018.5.8 12:37:50</div></div></section><section id="Defense_against_Adversarial_Attacks_Using_High-Level_Representation_Guided_Denoiser"><div class="paper-abstract"><div class="title">Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser</div><div class="info"><div class="authors">Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu and Xiaolin Hu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>画像分類におけるadrversarial attackの防御手法として, high-level representation guided denoiser (HGD) を提案.target model (メインの処理を担うネットワーク) への前処理段階で用いる.
HGDは, マルチスケールインフォメーションを得るためU-netの構造を使い,
トレーニングするための損失関数として, 元画像とノイズの乗った画像をそれぞれ入力したときの出力差を用いる.
右図に提案手法の詳細を示す.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png" alt="defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>pixel-levelの損失関数を課した従来のdenoiserと比べ, より良い結果が得られた.</p><p>state-of-the-artな防御手法であるensemble adversarial trainingと比べ, 3つのメリットがある.</p><ol><li>target modelがwhite-boxとblack-boxの両方に対してよりロバスト.</li><li>大規模データセットでの学習が簡単.</li><li>他のtarget modelへ使い回すことが可能.</li></ol></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02976.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#143]</div><div class="timestamp">2018.5.8 12:24:05</div></div></section><section id="Customized_Image_Narrative_Generation_via_Interactive_Visual_Question_Generation_and_Answering"><div class="paper-abstract"><div class="title">Customized Image Narrative Generation via Interactive Visual Question Generation and Answering</div><div class="info"><div class="authors">Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada</div><div class="conference">CVPR 2018</div><div class="paper_id">1224</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新規の“Customized画像説明文生成”タスクを提案した．また，インタラクティブにユーザに自動的に画像に関する質問をし，回答文を収集できるような仕組みを提案した．・従来の画像説明文生成タスクにおいて，異なるユーザの性質や画像の注目領域などにより，多様な説明文を生成できることが検討されていない．このような性質に応じて，多様な質問文を生成できる仕組み及びユーザとインターアクションしユーザの個性的な回答文を収集しユーザの特徴を学習することにより，Customizedで画像説明文を生成できる仕組みを提案した．
・提案仕組みは具体的に：①画像から self Q&A modelにより，画像中のマルチリジョンを注目し(attention構造を利用した)質問文を生成し， VQAモデルにより回答する(マルチ回答がある質問文だけを保留)；②　①により生成できた質問文をユーザに提示し，回答させる；③画像リジョン・質問文・回答文の統合した画像説明文を生成する．
・画像リジョン・質問文・ユーザ特有な回答文からchoice vectorを抽出し，このベクトルを利用してほかの画像が入力された場合，ユーザの個性的な画像説明文を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Customized_Image_Narrative_Generation.png" alt="Customized_Image_Narrative_Generation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・新規な問題設定“Customized画像説明文生成”・提案手法により，画像からより多様でユーザの個性を含んだ説明文を生成できる．
・ Automatic　Image　Narrative　Generationにおいて，従来のデータセットCOCO, SIND, DenseCapなどと比べ”diversity”,”interesting”,”naturalness”,”expressivity”などの指標に対しパフォーマンスが良い
・ Interactive　Image　Narrative　Generationにおいて，ヒューマンテストで良い評価を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・ユーザの個性を学習できる仕組みは応用場面が広そう</p><ul><li><a href="https://arxiv.org/pdf/1805.00460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#144]</div><div class="timestamp">2018.5.8 12:19:18</div></div></section><section id="First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations"><div class="paper-abstract"><div class="title">First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations</div><div class="info"><div class="authors">G Garcia-Hernando et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">一人称視点動画 (RGB-D) データセットの提供．
手（21点の3D間接位置）と物体（6D姿勢）の情報に加えて，45クラスの行動ラベルが付けられている．
データ数は1175シーケンス，10万フレーム．
手の3D姿勢と行動ラベルが付いている一人称視点動画データセットはこれまでになかった．
実験では従来手法やLSTMによるベースライン手法を合わせて18個を比較した結果が議論されており，
手の姿勢情報を使う手法が高い性能を示す傾向があることが確認されている．
</div></div><div class="item2"><img src="slides/figs/First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png" alt="First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>手の3D姿勢を使った行動認識のためのデータセットを提供．</li><li>RGB, Depth, Poseといった様々な特徴を用いる各手法が詳細に議論されている．</li><li>一番良い手法で78%程度の認識率．</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1704.02463">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=U5gleNWjz44">動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#145]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="PointFusion_Deep_Sensor_Fusion_for_3D_Bounding_Box_Estimation"><div class="paper-abstract"><div class="title">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation</div><div class="info"><div class="authors">Danfei Xu, dragomir Anguelov, Ashesh Jain</div><div class="conference">CVPR 2018</div><div class="paper_id">50</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・画像と点群情報を利用した3D物体検出のフレームワークPointFusionを提案した．・従来のマルチセンサーの情報を利用した3D物体検出は前処理が必要、マルチセンサーを異なるパイプラインで処理し，他のセンサーのコンテキストをうまく利用できないなどの問題点がある．PointFusionは①異なるネットワーク構造を用いて画像(CNN)と点群情報(PointNet)を直接処理し，②デンスフュージョンネットワーク構造を提案し，画像と点群の抽出情報を統合しより精密な3D物体検出を行う．
・2種類のデンスフュージョンネットワークを提案した．①画像情報及びPointNetにより抽出したグローバル情報を統合し， 3Dボックスのコーナー位置を推定する．②画像情報及びPointNetにより抽出したグローバル情報、ポイントフィーチャーを統合し， 3Dボックスのオフセット及びconfidence scoresを予測する．最後の2つの結果を統合し，最終的な結果を予測する</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointFusion.png" alt="PointFusion"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・点群データの前処理が必要無し．・対応できるデータの形式が広い，室外環境と室内環境両方対応できる．
・多様な三次元センサーのデータを対応できる．(RGB-D, LiDar, Radar,…)
・KITTI, SUN-RGBDデータセットにおいてstate-of-the-artな結果</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・室内・外環境両方対応できるので、応用場面が広そう・将来的にend-to-endに実現できたら更に良い</p><ul><li><a href="https://arxiv.org/pdf/1711.10871.pdf">論文</a></li></ul></div></div><div class="slide_index">[#146]</div><div class="timestamp">2018.5.8 10:56:27</div></div></section><section id="Path_Aggregation_Network_for_Instance_Segmentation"><div class="paper-abstract"><div class="title">Path Aggregation Network for Instance Segmentation</div><div class="info"><div class="authors">Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia</div><div class="conference">CVPR2018, arXive:1803.01534</div><div class="paper_id">912</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Feature Pyramid Network(FPN)ベースのMask R-CNNに，下位層の特徴マップを上位層に伝播させるPath Aggregation Networkを提案．インスタンスセグメンテーションの傾向として，上位層では物体全体に強く反応するが，下位層では物体の局所的な領域に強く反応する．
そのため，Path Aggregation Networkでは，上位層と下位層の特徴マップを用いることで，インスタンスセグメンテーションの精度を向上させている．
Path Aggregation Networkは，COCOのベンチマークで2位の性能を達成しており，CityscapeとMVDでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/912_overview.png" alt="912_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Path Aggregation Networkの構造は右図のようなシンプルな構造になっている．(a)の部分はFPNと同様の構造となっており，FPNの特徴マップから(b)で新しい特徴マップを作成する．
ここで，(a)と(b)では，緑線と赤線のように短距離と長距離のショートカットを導入する．
これにより，下位層の特徴を上位層に伝播することが可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li></ul></div></div><div class="slide_index">[#147]</div><div class="timestamp">2018.5.8 02:54:08</div></div></section><section id="StarGAN_Unified_Generative_Adversarial_Networks_for_Multi_Domain_Image_to_Image_Translation"><div class="paper-abstract"><div class="title">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</div><div class="info"><div class="authors">Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</div><div class="conference">CVPR2018, arXive:1711.09020</div><div class="paper_id">872</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>1つのネットワークでマルチドメイン対応の画像変換が可能なStarGANを提案．pix2pixやCycleGANの場合，左上図のように特定の1つのドメイン変換しかできないため，複数のドメイン変換をする時には各ドメインを変換するネットワークをそれぞれ構築しなければいけない．
StarGANでは，入力する条件とロス設計を適切に設計することで，シンプルなネットワークで多ドメインな画像変換を実現している．
実験では，顔属性のCelebAと表情のRaFD Datasetを使用し，2つのデータセットでGANを学習して下図のような多様な顔画像変換を可能にしている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/872_overview.png" alt="872_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>StarGANの構造は，右上図のようになっている．ここで，入力はそれぞれのドメインの画像がランダムに入力される．
まず，real imageとfake imageでDiscriminatorを学習する．
そして，次にGeneratorを学習する．
Generatorは，生成したい顔画像の条件とreal imageを入力して，画像変換する．
ここで，変換した画像はDiscriminatorに入力される．
変換した顔画像はCycleGANのようにreal imageを再変換する．
定義するロスは，一般的なAdversarial Loss，ドメインを認識するロス，real imageと再変換したimageのL1 Lossである．
また，複数のデータセットを学習するために，各データセットのラベルとデータセットの情報が格納されたMask vectorを導入している．
これにより，多ドメインかつ複数データセットに対応したGANを構築できている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>多ドメインかつ複数データセットに対応したGAN．変換するドメインの数に依存しないので，非常に用途が広がりそう．</p><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li><li><a href="https://github.com/yunjey/StarGAN">コードリンク</a></li></ul></div></div><div class="slide_index">[#148]</div><div class="timestamp">2018.5.8 01:27:52</div></div></section><section id="Semi-parametric_Image_Synthesis"><div class="paper-abstract"><div class="title">Semi-parametric Image Synthesis</div><div class="info"><div class="authors">Xiaojuan Qi, Qifeng Chen, Jiaya Jia, Vladlen Koltun</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>意味ラベル（Semantic Layout）から写真のようにリアルな画像をSemi-parametricな手法にて生成する。Semi-parametricはNon-parametricとParametricの強みを相補的に適用する手法である。セマンティックセグメンテーションのアノテーションとその対応する画像をペアとした外的なメモリにより対応関係を学習、Canvasとしてその順番や境界面を初期ステップとして出力する。次にCanvasと意味ラベルを入力としてConv-Deconv構造のネットワークにより写真のようにリアルな画像を出力とする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507SIMS.png" alt="180507SIMS"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Cityscapes, NYU, ADE20Kデータセットとセマンティックセグメンテーションに関するラベルが付与されていれば学習/テストが可能であり、同データセットにて従来法よりもさらにリアルな画像を生成するに至った。図には従来法（Chen and Koltun, ICCV 2017）との比較があり、従来法ではエッジ付近にボケが生じているが、提案法ではボケを相殺してさらに光の度合いまでもリアルに復元できている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>意味ラベルから写真を復元することに成功した。今後、さらに生成するアピアランスや配置をコントロールする手法が登場すれば、学習データを無限に増やすことができたり、作りたい写真を再構成することが可能になる。</p><ul><li><a href="http://vladlen.info/papers/SIMS.pdf">論文</a></li><li><a href="http://vladlen.info/publications/sims/">Project</a></li><li><a href="https://github.com/xjqicuhk/SIMS">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=U4Q98lenGLQ">Video</a></li></ul></div></div><div class="slide_index">[#149]</div><div class="timestamp">2018.5.7 13:32:33</div></div></section><section id="Hierarchical_Novelty_Detection_for_Visual_Object_Recognition"><div class="paper-abstract"><div class="title">Hierarchical Novelty Detection for Visual Object Recognition</div><div class="info"><div class="authors">Kibok Lee, Kimin Lee, Kyle Min, Yuting Zhang, Jinwoo Shin, Honglak Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">131</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・最も近いスーパークラスを予測することにより階層的新規(novelty)物体識別及び検出のフレームワークを提案した．・従来，新規なunseen物体識別は”known”と"unknown"に回帰する問題として対応されている．この論文で，物体のクラスを階層的に取り扱い，unseen物体の最も近いスーパークラスを求める．提案フレームワークによりgeneralized zero-shot learningタスクに用いられる階層的エンベディングを得られる．
・2種類の階層的な新規(novelty)物体検出構造を提案した．①top-down構造ではconfidence-calibrated classifierにより物体を分布の一致性が高いスーパークラスに分類する．②flatten構造では階層的分類構造の全体を用いずに error aggregationを避ける単一的なclassifierを用いる．また，①と②を組み合わせすることにより，階層的検出精度を向上できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/hierarchical_detection.png" alt="hierarchical_detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のクローズデータセットを用いた物体検出と比べ，提案手法はオープンデータセットを対応できる．・generalized zero-shot learningタスクで提案フレームワークを用いられる
・ ImageNet, AwA2, CUBなどのデータセットで階層的新規(novelty)物体識別においてベースラインより高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00722">論文</a></li></ul></div></div><div class="slide_index">[#150]</div><div class="timestamp">2018.5.7 13:08:17</div></div></section><section id="Revisiting_Salient_Object_Detection_Simultaneous_Detection_Ranking_and_Subitizing_of_Multiple_Salient_Objects"><div class="paper-abstract"><div class="title">Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects</div><div class="info"><div class="authors">Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce</div><div class="conference">CVPR 2018</div><div class="paper_id">892</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチsalientオブジェクトおよびそれぞれのsalientランキングを同時に検出するネットワークを提案した．・従来のsalientオブジェクトタスクに，salientランキングは観測者によって異なる結果が出る性質があるため，オブジェクトのsalientランキングについてまだ検討されていない．この文章でsalientランキングを有効的に得られるネットワークを提案した．またsalientランキング手法の評価方法も提案した．
・具体的なネットワーク構造はまずencoderネットワークにより粗末な相対salientスタックを生成し，そしてStacked Convolutional Module (SCM)により粗末なsaliency mapを生成する．またrank-awareでstage-wiseなネットワークによりsalientスタックをリファインする．ヒュージョンレイヤーにより各stageのsaliency mapを統合する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Revisiting-Salient-Object-Detection.png" alt="Revisiting-Salient-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・saliency ランキングの提案・AUC, max　F-measure, median F-measure, average F-measure,MAE, and SORなどの
評価方法により，state-of-the-artなsalientオブジェクト検出性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.05082">論文</a></li></ul></div></div><div class="slide_index">[#151]</div><div class="timestamp">2018.5.7 12:45:59</div></div></section><section id="Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization"><div class="paper-abstract"><div class="title">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</div><div class="info"><div class="authors">Y. Chao et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">動画中の行動のラベル，開始・終了時刻を推定するTemporal Action Localizationの研究．
Faster R-CNNによる物体検出をベースにLocalizationをする．
ここで，スケールのバリエーションが非常に大きい，前後の行動などのコンテキストが重要，
RGBとFlowをどう統合するか，といった3点の検討が重要としてこれらに取り組んでいる．
提案手法であるTAL-Netのポイントとしては，
アンカーごとに適切なスケールの受容野を持つ異なるCNNを組み合わせて利用している点．
各問題に対する設計がそれぞれ精度向上に寄与している点を実験から確認し，
THUMOS'14でのSOTAを達成．
</div></div><div class="item2"><img src="slides/figs/Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png" alt="Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>行動の時間スケールについての検討をちゃんと行った点は新規性がある</li><li>提案手法の各要素についての実験がされていて，それぞれによる精度向上を確認できている</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1804.07667">論文 (arXiv)</a></li><li>目新しいアイデアはないように思うが，問題点に対する解法を検討してかっちりと評価している</li><li>この辺りのスケールの話は大事そうなのにこれまで意外とちゃんとやられてきてなかったところ</li></ul></div></div><div class="slide_index">[#152]</div><div class="timestamp">2018.5.7 12:44:45</div></div></section><section id="PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume"><div class="paper-abstract"><div class="title">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</div><div class="info"><div class="authors">Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コンパクトかつ効果的なオプティカルフロー推定を実現するPWC-Netを提案する。ピラミッド構造かつ学習可能な階層的処理、射影（Warping）、コストボリュームにより設計され、軽量化しながら高精度なフロー推定を実現している。図は従来法（左図）と提案法（右図）のアーキテクチャの概略を示している。従来は画像のピラミッド構造により全てのサイズを階層的にオプティカルフローの射影や最適化を行い、最後に後処理をしていたが、提案法のPWCNetではあるひとつの階層内で後処理を行い、コンテキストを考慮したネットワーク（ContextNetwork; Dilated Convによる、各階層のオプティカルフローを入力するとそれらを総合的に解釈して最良のオプティカルフローを出力する）を通り抜けることで出力する。間には{Warping, Cont Volume, Optical flow}を行う層により構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PWCNet.png" alt="180507PWCNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法であるFlowNet2よりも17分の1の軽量化モデルでありながら、MPI Sintel final pass/KITTI 2015 BenchmarkにてState-of-the-art、Sintel 1024x436の解像度にて35fpsで動作する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>オプティカルフロー/距離画像の推定など、RGBの入力から異なるチャンネルを出力する課題が登場して本論文のように精度向上やコンパクト化、処理速度向上が著しい。ただし、出力したオプティカルフローや距離画像の出力自体の正当性を保証するような評価方法が必要？特に、異なるドメイン（ドイツの道路データで学習して日本の道路データでテストするなど）での適応とその性能保証は欲しいところ。</li><li>（さすがNVIDIA！？）実験量がとても多く見える。Table1~7までびっしり実験結果が埋められている。</li><li><a href="http://xiaodongyang.org/publications/papers/pwc-cvpr18.pdf">論文</a></li><li><a href="http://research.nvidia.com/publication/2018-02_PWC-Net%3A-CNNs-for">Project</a></li><li><a href="https://github.com/deqings/PWC-Net">GitHub</a></li></ul></div></div><div class="slide_index">[#153]</div><div class="timestamp">2018.5.7 12:26:54</div></div></section><section id="LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos"><div class="paper-abstract"><div class="title">LEGO: Learning Edge with Geometry all at Once by Watching Videos</div><div class="info"><div class="authors">Z. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ラベルなし動画からの3次元幾何 (Depth, Normal) の推定．
従来研究のものだと画素ごとの誤差で最適化していたのでボケた幾何構造推定になっていたのが問題と主張．
提案手法はエッジと3次元幾何を同時に推定して最適化することで，左図 (f) のような正確な幾何構造を推定可能にした．
ベースは従来手法同様で，カメラ姿勢を推定し，それに基づくWarping結果と元のフレームとの間の誤差をとって最適化．
これに，エッジ推定と3D-ASAP (as smooth as possible in 3D) Priorを導入したところがポイント．
3D-ASAPはある2点間の間にエッジがなければその2点は同一平面上にあるという仮定に基づく提案手法．</div></div><div class="item2"><img src="slides/figs/LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png" alt="LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>3次元幾何とエッジ推定を同時にする手法の提案</li><li>3D-ASAP Priorの定式化とそれによる精度向上を実現</li><li>KITTIやCityScapesでのSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1803.05648">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=40-GAgdUwI0">結果動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#154]</div><div class="timestamp">2018.5.7 11:15:05</div></div></section><section id="DA-GAN_Instance-level_Image_Translation_by_Deep_Attention_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Network</div><div class="info"><div class="authors">Shuang Ma, Jianlong Fu, Chang Chen, Tao Mei</div><div class="conference">CVPR 2018</div><div class="paper_id">695</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・無監督インスタンスレベルのattentionを用いたImage Translationフレームワークを提案した．・従来の無監督Image Translationではセットレベルで実現され，物体パーツレベルの対応ができないため，従来手法より生成した物体画像が幾何や意味的な情報のリアル性が低い場合がある．それと比べ，提案フレームワークは①物体をはattentionを用いた高構造化latent空間に変換し，このlatent空間によりインスタンスレベルなImage Translationを可能にした．②さらに，source samplesとtranslated samplesをセマンティック的に対応させるconsistency lossを提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DA-GAN.png" alt="DA-GAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めてattentionをGANに導入したと宣言・MNIST , CUB-200-2011, SVHN , FaceScrub and AnimePlanet 1などのデータセットを用いて実験を行い，ドメンadaption，テキスト-画像合成，ポーズモーフィング，顔‐アニメーション化などのタスクにおいて，state-of-the-artな精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・attentionをGANに導入し，さらに精密で構造化した画像生成ができるので，様々なアプリで応用できそう</p><ul><li><a href="https://arxiv.org/pdf/1802.06454.pdf">論文</a></li></ul></div></div><div class="slide_index">[#155]</div><div class="timestamp">2018.5.7 10:19:19</div></div></section><section id="PhaseNet_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">PhaseNet for Video Frame Interpolation</div><div class="info"><div class="authors">Simone Meyer, et al.</div><div class="paper_id">1804.00884</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>様々なシーンに頑健かつ、大きな動きにも対処しながらビデオフレームの補間を行うPhaseNetの提案。中間のフレームにおける位相と階層構造を推定するnnのデコーダを搭載。これにより、既存の位相ベースの手法よりも広範囲に渡る動きに対応。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PhaseNet.jpg" alt="180507PhaseNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のビデオフレーム補間アプローチは、フレーム間において密な対応付けが必要であり、照明変化や被写体ブレに頑健でない。カーネルに依存した深層学習ベースの手法でもある程度緩和することはできるが不十分。ピクセル単位の位相ベースの手法ならば上手くいくことが実装されている。位相ベースでnnを用いた手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>位相のlossとノルムを組み合わせることで、チャレンジングなシーンでも視覚的に綺麗な画像を生成できる。</p><ul><li><a href="https://arxiv.org/pdf/1804.00884.pdf">論文</a></li></ul></div></div><div class="slide_index">[#156]</div></div></section><section id="Multi-scale_Location-aware_Kernel_Representation_for_Object_Detection"><div class="paper-abstract"><div class="title">Multi-scale Location-aware Kernel Representation for Object Detection</div><div class="info"><div class="authors">Hao Wang, Qilong Wang, Mingqi Gao, Peihua Li and Wangmeng Zuo</div><div class="conference">CVPR2018</div><div class="paper_id">153</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出時に特徴量の高次の統計量（high-order statistics）を獲得するためのMulti-scale Location-aware Kernel Representation（MLKP)を提案する．MLKPはSSDで用いるような，複数解像度の特徴マップを結合したマルチスケール特徴マップを用いて効果的に計算できる．マルチスケール特徴マップをMLKPに入力すると，畳み込みと要素ごとの積算を行いr次の表現Z^rを得る．このとき，location-weight networkは各位置の寄与度を学習する．その後，各次の表現を重みつき結合し，RoI Poolingへ入力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180507_mlkp1.jpg" alt="20180507_mlkp1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最近の分類メソッドでよく用いられる高次統計量を物体検出器の高精度化に用いる手法である．Faster R-CNNにMLKPを統合することで，Faster R-CNNよりも精度が4.9%(mAP, VOC2007），4.7%（mAP, VOC2012），5.0%（MSCOCO）向上した．DSSDやR-FCNと比較しても同等もしくはそれ以上の性能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>流行りのマルチスケール手法をR-CNNに昇華した感じ．R-CNNベースの手法もまだまだ煮詰める余地は十分ある．</p><ul><li><a href="https://arxiv.org/abs/1804.00428">arXiv</a></li><li><a href="https://github.com/Hwang64/MLKP">コード</a>py-faster-rcnnをベースにされている．マルチGPU版もあり</li></ul></div></div><div class="slide_index">[#157]</div><div class="timestamp">2018.5.7 01:24:41</div></div></section><section id="Self_supervised_Learning_of_Geometrically_Stable_Features_Through_Probabilistic_Introspection"><div class="paper-abstract"><div class="title">Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection</div><div class="info"><div class="authors">David Novotny et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>幾何学変換を利用したGeometrically Stable な特徴表現の獲得手法。オリジナル画像とそれに幾何学変換を施した画像を同じCNNに学習し、中間特徴マップ上で対応するpixelでの特徴量の類似度が高くなるように学習する。キーポイントマッチングなどの問題設定で教師あり学習以上の効果を発揮。Pixelによってはマッチングが困難ば場合も存在するため、不確実性を考慮した学習を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Novotny.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>ペアとなる画像を同じNNに入力し、各pixel ペアの類似度と、不確実性を表す値を算出。不確実性を考慮した損失関数を定義することで、結果的にNNはマッチング可能かつ対応するpixelに関しては高い類似度と低い不確実性を、マッチングが困難なものに関しては高い不確実性を算出するように学習される。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>定義された距離尺度において対象に直接近づける枠組みが多い通常の類似度学習と異なり、連続値である類似度を確率変数とすることで、不確実性を考慮するのは興味深い。しかし、定式化としては論文内のものよりも、不確実性利用してモデルが類似度の分布を算出しているという定式化にした方がわかりやすいのではないかと思った。</p><ul><li><a href="https://arxiv.org/abs/1804.01552">論文</a></li></ul></div></div><div class="slide_index">[#158]</div></div></section><section id="Squeeze-and-Excitation_Networks"><div class="paper-abstract"><div class="title">Squeeze-and-Excitation Networks</div><div class="info"><div class="authors">Jie Hu, Li Shen, Gang Sun</div><div class="conference">CVPR2018, arXive:1709.01507</div><div class="paper_id">891</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Residualモジュール, Inceptionモジュールに対してAttention機構を導入したネットワーク．Squeeze-and-Excitation Networks(SENet)では，生成される特徴マップのチャンネルに対してAttentionを導入している．
SENetは，ImageNetでstate-of-the-artな性能を達成している．(現在1位)
また，Place Datasetでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/891_overview.png" alt="891_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SENetには，右図のように2つのモジュールが提案されている．SE Inception moduleは，VGGやAlexNet等の順伝播ネットワークで使われるSEモジュール．
SE Residual moduleは，ResNet系のネットワークに使われるSEモジュールである．
基本的には，Global Average Poolingを施した後に，全結合層を何層か通してチャンネル毎のAttentionを生成する．
この構造は，ResNet等の様々なネットワークモデルにも適応できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attention機構を導入した物体認識法．最近，物体認識にもAttentionが流行し始めているので，その先駆けな手法になりそう．
学習モデルもGitHub上で公開．</p><ul><li><a href="https://arxiv.org/abs/1709.01507">論文リンク</a></li><li><a href="https://github.com/hujie-frank/SENet">コードリンク</a></li></ul></div></div><div class="slide_index">[#159]</div><div class="timestamp">2018.5.6 23:46:46</div></div></section><section id="ClusterNet_Detecting_Small_Objects_in_Large_Scenes_by_Exploiting_Spatio-Temporal_Information"><div class="paper-abstract"><div class="title">ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information</div><div class="info"><div class="authors">Rodney LaLonde, Dong Zhang, Mubarak Shah</div><div class="paper_id">1704.02694</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>1平方キロメートル以上の広範囲の領域を撮影できるWide Area Motion Imagery(WAMI)の映像から、車などの小さい物体を検出する手法の提案。まず、ClusterNetでビデオフレームから、CNNを使って動きと外観情報を結合し、regions of objects of interest(ROOBI)を出力。次に、FoceaNetによって、ヒートマップ推定を介して、ROOBI内の物体の重心位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506ClusterNet.jpg" alt="180506ClusterNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>WAMIを使った従来の物体検出は、アピアランスベースの分類器であまり精度が出ず、背景差分やフレーム間差分などの動き情報に依存しがち。Fast R-CNNなどにおけるこれらの問題を検証し、効率的かつ効果的な新たな2ステージCNNを提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>広範囲の情報から数百の物体を同時に検出する。他の手法では扱えない停止車両なども検出できる。</p><ul><li><a href="https://arxiv.org/pdf/1704.02694.pdf">論文</a></li><li><a href="https://www.tno.nl/en/focus-areas/defence-safety-security/roadmaps/information-sensor-systems/wide-area-motion-imagery-wami/">WAMI</a></li></ul></div></div><div class="slide_index">[#160]</div></div></section><section id="An_Analysis_of_Scale_Invariance_in_Object_Detection-SNIP"><div class="paper-abstract"><div class="title">An Analysis of Scale Invariance in Object Detection – SNIP</div><div class="info"><div class="authors">Bharat Singh, Larry S. Davis</div><div class="paper_id">1711.08189</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>極端なスケール変化に頑健な物体検出手法であるSNIPの提案。物体検出において、大きな物体と小さな物体をそれぞれ検出することは困難。そこで、学習時に異なるサイズの物体における勾配を、選択して逆伝播する。物体の幅広いスペクトルに対処し、ドメインシフトを低減する。ピラミッド型のネットワークとなっており、end-to-end学習可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506SNIP.jpg" alt="180506SNIP.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>まず、現代の物体検出手法の欠点として、スケール変化について解析している。小さい物体を検出するために“アップサンプリング画像が必要か”などを、ImageNetを使ってパフォーマンスを評価。これらの解析に基づいてSNIPを開発。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>COCO2017 challengeにおける、最優秀学生応募作品。</p><ul><li><a href="https://arxiv.org/pdf/1711.08189.pdf">論文</a></li><li><a href="http://bit.ly/2yXVg4c">コード</a></li></ul></div></div><div class="slide_index">[#161]</div></div></section><section id="The_iNaturalist_Species_Classification_and_Detection_Dataset"><div class="paper-abstract"><div class="title">The iNaturalist Species Classification and Detection Dataset</div><div class="info"><div class="authors">Grant Van Horn, el al. </div><div class="paper_id">1707.06642</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然界にける、“写真に写り易さ”を考慮した画像分類・検出タスク用データセットの提案。5000種類以上の植物や動物からの85万9000の画像で構成。世界各地の多種多様な種やシチュエーションで撮影され、様々なカメラタイプで収集することで画質の変化し、クラスの均衡が大きい。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506iNaturalist.jpg" alt="180506iNaturalist.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の画像分類・検出用データセットでは、カテゴリごとに画像数が統一されている傾向にある。しかし，写真に収め易い種と、そうでない種があるため、自然界はとても不均衡。この差に着目し、現実世界の状況に近い状況で分類・検出に挑戦するデータセットを提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはり既存の手法では精度を出すのは難しそう。このデータセットで精度を出すチャレンジングな研究をするのはアリ。</p><ul><li><a href="https://arxiv.org/abs/1707.06642">論文</a></li></ul></div></div><div class="slide_index">[#162]</div></div></section><section id="Between-class_Learning_for_Image_Classification"><div class="paper-abstract"><div class="title">Between-class Learning for Image Classification</div><div class="info"><div class="authors">Yuji Tokozume, Yoshitaka Ushiku and Tatsuya Harada</div><div class="paper_id">1711.10284</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Between-Class learning(BC learn)という画像分類タスクにおける新学習方法の提案。まず、異なるクラスの2枚の画像をランダムな比率で混合したbetween-class imageを作成。そして、画像を波形として扱うためにミキシングを行う。混合画像をモデルに入力し、学習することで混合した比率を出力する。これにより、特徴分布の形状に制約をかけることができるため、汎化性能が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506Between-class.jpg" alt="180506Between-class.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>もともとは、混合できるデジタル音声のために開発された手法。CNNは“画像を波形として扱っている”という説から、本手法を提案。2つの画像を混合する意味に疑問はあるが、実際にパフォーマンスが向上している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>混合とミキシングの提案手法によって分類精度が向上。画像の混合にどんな意味があるのかを解明してほしい。</p><ul><li><a href="https://arxiv.org/abs/1711.10284">論文</a></li></ul></div></div><div class="slide_index">[#163]</div></div></section><section id="CleanNet_Transfer_Learning_for_Scalable_Image_Classifier_Training_with_Label_Noise"><div class="paper-abstract"><div class="title">CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise</div><div class="info"><div class="authors">Kuang-Huei Lee, Xiaodong He, Lei Zhang and Linjun Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルノイズを使って、画像分類モデルを学習するCleanNetの提案。人間による“ラベルノイズの低減”という作業を低減する。事前知識として人の手で分類されたクラスの一部の情報だけを使い、ラベルノイズを他のクラスに移すことができる。また、CleanNetとCNNによるクラス分類ネットワークを1つのフレームワークとして統合。ラベルノイズ検出タスクと、統合した画像分類タスクの両方で、ノイジーなデータセットを使って精度検証。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506CleanNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>人間がラベルのアノテーションをすると時間がかかり、学習はスケーラブルじゃない。逆に人間に頼らない手法はスケーラブルだが、有効性が低い。少し人間に頼って、あとは自動的にノイズ除去をするというハイブリットな手法。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>弱教師付き学習と比較して、ノイズを41%低減。画像分類タスクにおいて、47%パフォーマンスが向上。</p><ul><li><a href="https://arxiv.org/abs/1711.07131">論文</a></li></ul></div></div><div class="slide_index">[#164]</div></div></section><section id="Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes"><div class="paper-abstract"><div class="title">Super-Resolving Very Low-Resolution Face Images with Supplementary Attributes</div><div class="info"><div class="authors">Xin Yu, Basura Fernando, Richard Hartley, Faith Porikli</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像のアトリビュートを使用することでGTとなる高解像度画像(HR)を使用せずに低解像度画像(LR)を超解像度化する研究。LRとともに顔のアトリビュートも入力として使用することで超解像化における曖昧さを解決。
ネットワークの大枠はGANを採用。
ジェネレータにおいてLRをauto encoderに噛ませる際にエンコードされた特徴量にアトリビュートを付け足してでコードを行う。
ディスクリミネータはGTのHR画像なら1を、ジェネレータによる画像or画像にアトリビュートが含まれていないと判断した際には0を返す。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180506Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は16x16画像、出力は入力画像が128x128に超解像度化された画像。</li><li>PSNR、SSIMを評価指標として既存手法と比べたところもっとも良い精度を得た。</li><li>既存手法で入力されたLRに対して一意的なHRのみしか出力することができなかった。一方提案手法では入力するアトリビュートに伴って出力するHRの見た目を変更することが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>トレーニングで使用したデータセットはCelebAであり、使用したアトリビュートはCelebAに付属する40種類のうちからgender, ageなど18種類。</li><li><a href="https://basurafernando.github.io/papers/XinYuCVPR18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#165]</div></div></section><section id="Single-Shot_Object_Detection_with_Enriched_Semantics"><div class="paper-abstract"><div class="title">Single-Shot Object Detection with Enriched Semantics</div><div class="info"><div class="authors">Z.Zhang, S.Qiao, C.Xie, W.Shen, B.Wang and A.L.Yuille</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.00433</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>Detection with Enriched Semantics (DES)というシングルショットオブジェクト検出器を提案．セマンティックセグメンテーションブランチとオブジェクト検出ブランチで構成.
セマンティックセグメンテーションブランチとグローバルアクティベーションモジュールによってオブジェクト検出の特徴であるセマンティクスを向上．
既存のSSDなどのシングルショット検出器よりも速度と精度が向上．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Single-Shot_Object_Detection_with_Enriched_Semantics.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションブランチに高レベルのオブジェクト特徴のためのオブジェクト検出特徴チャンネルとオブジェクトクラスとの意味的関係を学習するためのグローバルアクティベーションブロックを加える．</li><li>一般的なシングルショット検出器と比較して大幅に検出精度が向上，</li><li>Titan Xp GPU1台で、31.7 FPSを達成し、R-FCNやResNetベースのSSDよりも高速.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.00433.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#166]</div><div class="timestamp">2018.5.6 01:42:36</div></div></section><section id="Revisiting_Deep_Intrinsic_Image_Decompositions"><div class="paper-abstract"><div class="title">Revisiting Deep Intrinsic Image Decompositions</div><div class="info"><div class="authors">Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf</div><div class="conference">CVPR 2018 oral</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>光の反射やシェーディングなどを再計算することで自然画像の分解と再構成（Image Decomposition）を行う問題設定である。従来型の事前情報を陽に与えるフィルタリング手法とは異なり、深層学習による提案手法では（十分なラベル付きデータが存在すれば）画像の内的な情報を効果的に捉えて画像の再構成をより自然に行うことができると主張。この問題を解決するために、２種類のカテゴリに関する問い ー（１）詳細なラベル付きデータ（２）弱教師付き学習により比較的多様なラベル付きデータを学習ー を解決することができる。これにより学習データには詳細なラベル付けを行わず弱い事前知識（Loose Prior Knowledge）のみで大量のサンプルを準備することができる。手法面において、最初は荒く光の反射（Albedo）やシェーディングを推定し、次いでエッジやテクスチャ等を推定できるようにフィルタリングを学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180505DeepIntrinsic.png" alt="180505DeepIntrinsic"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>主要な画像再構成のベンチマークにおいて全てState-of-the-artの（最先端の）結果を達成した。さらに、従来まではデータセットに対してアドホックである（と思われる）が、本論文にて提供するデータや手法はよりオープンかつリアルな問題に対して汎用的に使用できる。弱い事前知識のみでリアルデータを学習できるようにしたことも新規性として挙げられる。CVPRの査読を突破できた理由として、State-of-the-artな精度を全てのデータにて達成したことや、その学習法/アーキテクチャの提案にあると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>光の反射（Albedo）や陰影（shading）を同時に推定できる技術はよりリアルな画像の生成には重要技術なのでどんどん進んで欲しい。</p><ul><li><a href="https://arxiv.org/abs/1701.02965">論文</a></li><li><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/">MIT Intrinsic Images Dataset</a></li><li><a href="http://sintel.is.tue.mpg.de/">MPI Sintel Flow Dataset</a></li><li><a href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/">Intrinsic Images in the Wild</a></li></ul></div></div><div class="slide_index">[#167]</div><div class="timestamp">2018.5.5 17:36:29</div></div></section><section id="Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz"><div class="paper-abstract"><div class="title">Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</div><div class="info"><div class="authors">Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Perez, Christian Theobalt</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>単眼顔画像からリフレクタンス、ジオメトリー、照明情報を推定する研究。トレーニングデータには上記の情報のアノテーションを必要とせず、3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
テスト時には250Hz以上で実行することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180505Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>大量のアノテーションが必要という既存手法の問題点を解決</li><li>様々な表情に対応することができ、口髭や化粧も再現することが可能。</li><li>既存のラーニングベースの手法と比較した結果、同等の実行時間でより精度の高いリコンストラクションが可能となった。最適化ベースの手法と比較すると10%ほど精度は落ちるものの、最適化ベースの手法では実行時間が120secかかるが提案手法では4msで実行可能。 </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>目元やおでこの皺の再現には至っていない</li><li><a href="https://arxiv.org/abs/1712.02859">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/CVPR18_FaceModel/page.html">Project page</a></li></ul></div></div><div class="slide_index">[#168]</div></div></section><div class="title">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</div><div class="info"><div class="authors">W.Xian,  P.Sangkloy, V. Agrawal, A.Raj, J.Lu, C.Fang, F.Yu and J.Hays</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1706.02823</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>ユーザが色，スケッチ，テクスチャから深層画像合成を行うTextureGANを提案．既存手法では，カラーやスケッチによる制御を行っているが今回の手法ではユーザがテクスチャパチをスケッチ上に配置することによってテクスチャによる制御を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TextureGAN_Controlling_Deep_Image_Synthesis_with_Texture_Patches.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>深層画像合成における細かいテクスチャ制御の妥当性を初めて実証</li><li>ユーザが特定のテクスチャをスケッチの境界に「ドラック＆ドロップ」するテクスチャインタフェースの提案.</li><li>生成ネットワークで既存のオブジェクトに見られないテキスチャであった場合でも扱うようにする局所テクスチャロスを定義．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li><p>TextureGANをローカルテクスチャで制約することにより，テクスチャとスケッチベースの画像合成の効果を実証．</p></li><li><p>別のテクスチャデータベースから抽出されたテクスチャから生成されたスケッチを用いて実験を行い、提案アルゴリズムがユーザコントロールに忠実な妥当な画像を生成されることを確認．</p></li><li><p><a href="https://arxiv.org/pdf/1706.02823.pdf">Paper</a></p></li></ul></div></div><div class="slide_index">[#169]</div><div class="timestamp">2018.5.5 01:54:38</div><section id="Learning_Deep_Models_for_Face_Anti-Spoofing"><div class="paper-abstract"><div class="title">Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision</div><div class="info"><div class="authors">Yaojie Liu, Amin Jourabloo, Xiaoming Liu</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された動画が生身の人間によるものか、あるいはそれ以外のspoofing（撮影された動画や顔のプリントなど）を判定する研究。空間的な情報として顔のデプスマップ、時間的な情報としてrPPG（信号のパルス信号）。
CNN-RNNモデルを使用しCNNでデプスマップと顔の特徴量マップを、RNNは各時刻でCNNによって推定された顔の特徴量マップを入力としてrPPGを推定する。
既存研究では様々なパターンのspoofingがあるにも関わらずCNNによるバイナリの識別問題として捉えていたため、CNNの広すぎる空間を学習してしまい結果的に過学習をしてしまっていた。
提案手法では補助的な情報としてデプスマップ、rPPGを使用することで識別精度を向上した。
更に165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180504Learning_Deep_Models_for_Face_Anti-Spoofing.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>提案手法では既存研究のようにバイナリの識別問題とはとらえず、デプスマップとrPPGを使用することで学習したパターンのspoofingを確実に検出できることを目的とした。</li><li>既存研究とAPCER、BPCER、ACER、HTER値における比較を行なった結果、提案手法優位な結果となった。識別精度は約72%、state-of-the-artの研究では約34%。</li><li>165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11097">論文</a></li></ul></div></div><div class="slide_index">[#170]</div></div></section><section id="Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection"><div class="paper-abstract"><div class="title">Adversarially Learned One-Class Classifier for Novelty Detection</div><div class="info"><div class="authors">M.Sabokrou, M.Khalooei, M.Fathy and E.Adeli</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1802.09088</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>1クラス分類の際のノベリティ検出のために2段階のネットワークを構築．1つのネットワークはノベリティの検出をし，もう1つでは，inlierを強化しoutlierを歪ませる．
画像と動画で検証．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>1クラス分類のためのend to endネットワークを導入したもの</li><li>GANを用いた手法では学習後に片方のモデルのみが使われるが，今回の手法ではテストの際に両方のモデルを掛け合わせることで効率化を図る</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>inlierとoutlierの分類は元のクラスのサンプルの決定よりも優れている．</li><li>ノベリティクラスのサンプルが無くても学習し，動画や画像の異常を検知でき，様々なアプリケーションで高いパフォーマンスを示す．</li><li><a href="https://arxiv.org/pdf/1802.09088.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#171]</div><div class="timestamp">2018.5.4 03:50:34</div></div></section><section id="Feature_Space_Transfer_for_Data_Augmentation"><div class="paper-abstract"><div class="title">Feature Space Transfer for Data Augmentation</div><div class="info"><div class="authors">Bo Liu, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像空間上ではなく、特徴空間上でデータ拡張（Data Augmentation）を行う研究である。この課題に対して著者らは特徴空間上で物体姿勢/見え方のバリエーションを多様体として考慮するFeature Transfer Network (FATTEN)を提案。従来の特徴空間上でのデータ拡張とは異なり、提案法であるFATTENはEnd-to-Endでの学習が可能であり、より効果的にデータ拡張を実行可能である。同ネットワークは姿勢やカテゴリの多タスク学習により学習を行う。図は直感的な特徴空間上での挙動を示したもので、Pose/Appearanceにおける特徴空間の動線を把握した上でデータ拡張を行うことができる。One-/Few-shot学習でも効果を発揮し、特にOne-shotでは他を大きく離して優れていることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180504FATTEN.png" alt="180504FATTEN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>新規性としては複数の属性（ここでは姿勢・アピアランス）を同時に考慮しながら特徴空間上でデータ拡張を行える点が新規性としてあげられ、さらに関連研究と異なるのはEnd-to-Endで学習できる点も優れている。直感的にはビューポイントの違いとそれに対応するアピアランスを拡張する形で特徴学習ができていると言える。FATTENを適用しModelNet/SUN-RGBDのデータセットにてデータ拡張を行った結果、はっきりとした精度向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>RotationNetとの比較や統合（RotationNet+FATTEN）が気になる。もともとこの論文で扱っている問題に対して精度が高いRotationNetに本論文のデータ拡張手法を使用するとさらに大きく精度向上するのでは？</p><ul><li><a href="https://arxiv.org/abs/1801.04356">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">RotationNet</a></li></ul></div></div><div class="slide_index">[#172]</div><div class="timestamp">2018.5.4 00:21:12</div></div></section><section id="Deep_Extreme_Cut_From_Extreme_Points_to_Object_Segmentation"><div class="paper-abstract"><div class="title">Deep Extreme Cut: From Extreme Points to Object Segmentation</div><div class="info"><div class="authors">Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool</div><div class="conference">CVPR2018, arXiv:1711.09081</div><div class="paper_id">88</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Extreme pointを用いた物体セマンティックセグメンテーション法．このExtreme pointは，セグメンテーションの上端，下端，右端，左端を使用している．
4つのExtreme pointは，物体の大まかな形状の情報を取り込みながらCNNを学習することができる．
Pascal VOC, COCO, DAVIS2016, DAVIS2017, Grabcutで評価し，どのベンチマークにおいても高い性能を示している．
また，セマンティックセグメンテーションのアノテーションツールとして応用できることも示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/88_overview.png" alt="88_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>使用するネットワークは，ResNet101をBackboneにしたDeepLab-v2である．提案手法のDeep Extreme Cutでは，Extreme pointを有効的に学習するために，点にガウシガウシアンを施してヒートマップを作成し，そのヒートマップを入力画像のチャンネルに追加している．
この学習方法は，様々なタスクのセグメンテーションに有効であり，セマンティックセグメンテーション，動画のセグメンテーション，インスタンスセグメンテーション，インタラクションセグメンテーションに応用することができる．
また，セグメンテーションのアノテーションツールにも応用でき，従来のアノテーションコストを10分の1まで削減できていることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09081">論文リンク</a></li><li><a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/">プロジェクト＆コードリンク</a></li></ul></div></div><div class="slide_index">[#173]</div><div class="timestamp">2018.5.3 23:45:06</div></div></section><section id="Detail-Preserving_Pooling_in_Deep_Networks"><div class="paper-abstract"><div class="title">Detail-Preserving Pooling in Deep Networks</div><div class="info"><div class="authors">Faraz Saeedan, Nicolas Weber, Michael Goesele, Stefan Roth</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>徐々にダウンサイジングしながらも詳細な情報は保持するという問題設定を解決するDNN、特に微分可能なプーリング手法であるDetail-Preserving Pooling（DPP）を提案する。同ネットワークでは隠れ層にて徐々にダウンスケールを行う。図にはフローチャートが示されている。このように線形ダウンスケーリングを施した画像に対して、出力が情報量をできる限り失わないように学習できるプーリングを提案することで任意の畳み込みネットに対して性能向上を見込める手法とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503DPP.png" alt="180503DPP"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>データセットにより最良なプーリングの手法が異なるという欠点を解決するべくDPPを提案した。また、グラフィクスの分野にて提案されているDPID（文献31）を参考にして微分可能（学習可能）なプーリング手法を提案した。このようにして作成されたプーリングはあらゆるネットワークに対し有効にフィットし、（max/average poolingなどより）精度向上を保証すると主張した。例として単純にResNet-101のアーキテクチャのプーリングを置き換えてもCIFAR10にてエラー率が下がっている。このように学習可能であり、汎用的に使用できて高精度が期待できるプーリング手法を提案したことが採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>本手法が汎用的に使用できるのであれば、早い段階でDLフレームワーク（e.g. PyTorch, TensorFlow）などに実装されて使用されるかも？実装面の難しさがどの程度あるか次第か。</p><ul><li><a href="https://arxiv.org/abs/1804.04076">論文</a></li><li><a href="www">Project</a></li><li><a href="https://github.com/visinf/dpp">GitHub</a></li></ul></div></div><div class="slide_index">[#174]</div><div class="timestamp">2018.5.3 23:36:27</div></div></section><section id="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations"><div class="paper-abstract"><div class="title">Learning a Single Convolutional Super-Resolution Network for Multiple Degradations</div><div class="info"><div class="authors">Kai Zhang, Wangmeng Zuo and Lei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単一画像の超解像手法では，低解像度の画像は，高解像度の画像からのバイキュービック的にダウンサンプリングされたものであるという仮定を置いている．そのため，この仮定に従わない場合，性能が低下する．さらに，複数の劣化に対処するスケーラビリティーも欠けている．本論文ではこれらの問題に対処するため，畳み込み超解像ネットーワークに低解像度画像とdegradation map（ブラーカーネルとノイズレベルから作成）を入力する方法を提案している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG"></p><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>畳み込み超解像ネットワークにブラーカーネルやノイズレベルも入力しようとすると，低解像度画像とのサイズの違いによりネットワークの設計が困難になる．本論文では，dimensionality stretcing strategyを導入することによりこの問題を解決した点が新しい．</p><p>劣化されたSet5などのデータセットに対して，従来法や提案手法を適用し，PSNRとSSIMにより評価した結果，提案手法が最も良い結果を示した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.06116.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#175]</div><div class="timestamp">2018.5.3 15:51:30</div></div></section><section id="Super-FAN_Integrated_facial_landmark_localization_and_super-resolution_of_real-world_low_resolution_faces_in_arbitrary_poses_with_GANs"><div class="paper-abstract"><div class="title">Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs</div><div class="info"><div class="authors">Adrian Bulat, Georgios Tzimiropoulos</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>任意の向きの低解像度顔画像に対して超解像度化する研究。生成された超解像度画像に対してランドマーク推定を同時に行うことで画像の精度が良くなることを主張。顔画像の高解像度化の際にランドマークを特定することは有用であることはすでに示されていたが、低解像度かつ任意の顔向きの際にはランドマークを使用して高解像度化することが難しかった。提案手法ではGANによって低解像度顔画像から超解像度化された顔画像を生成し、生成された顔画像に対してランドマークのヒートマップを推定を推定することでネットワークの学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Super-FAN.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>解像度はそれぞれ入力画像が16x16、出力画像が64x64</li><li>生成された顔画像の評価指標としてPSNR、SSIMを、ランドマーク推定の評価指標としてAUCを使用し、 顔向きが30・60・90度の顔画像に対してどちらも既存研究より良い顔画像を生成することが可能となった。</li><li>トレーニングの際に複数のロス関数を提案しているが、各ロス関数ごとの結果に関しても議論を行っている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.02765">論文</a></li></ul></div></div><div class="slide_index">[#176]</div></div></section><section id="Image_Correction_via_Deep_Reciprocating_HDR_Transfromation"><div class="paper-abstract"><div class="title">Image Correction via Deep Reciprocating HDR Transfromation</div><div class="info"><div class="authors">Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson W.H.Lau</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力されたLDR画像に対する露光量の調節をend-to-endに行う研究。２つのU-Netを使用し、LDR画像からHDR画像の推定と、推定されたHDR画像からLDRドメインへの変換、という２つ学習によって実現する。LDR画像に内包されている問題として、露光量が少ない箇所ではピクセルが黒く塗りつぶされてしまい、実際のシーンにおける色の推定が難しいという問題がある。そこで、LDR画像から一度HDR画像を生成することで、塗りつぶされた領域を修復する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Image_Correction_via_Deep_Reciprocating_HDR_Transfromation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>入力LDR画像の露光量が多い部分や少ない部分に対しても適切な画像修復が可能となった。</li><li>同様の問題を扱う最新手法と比較した結果、提案手法優位な結果となった。主な理由としてはHDR画像からLDR画像へ変換する際に画像の局所的な詳細情報を保てていることをあげている。</li><li>定量評価として画像の質を表す数値であるPSNR、SSIM、FSIM、Q-scoeによる評価を行った。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.04371">論文</a></li><li><a href="https://ybsong00.github.io/cvpr18_imgcorrect/index">Project page</a></li></ul></div></div><div class="slide_index">[#177]</div></div></section><section id="Visual_Question_Answering_with_Memory_Augmented_Networks"><div class="paper-abstract"><div class="title">Visual Question Answering with Memory-Augmented Networks</div><div class="info"><div class="authors">Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den Hengel, Ian Reid</div><div class="conference">CVPR2018, arXive: 1707.04968</div><div class="paper_id">875</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>学習サンプルに少ないような質問に対しても回答ができるような手法を提案．ベースはMemory-Augmented Network (One-shot learningを導入したMemory Network)であり，記憶ブロックとAttentionの機能により，稀に発生する質問に対しても正確に回答をすることができる．
VQA benchmark datasetとCOCOのVQAタスクで評価し，高い性能を示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/875_overview.png" alt="875_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この手法の大まかな構造はMemory-Augmented Networkになっており，特徴抽出部分が質問文と画像特徴である．画像特徴はVGGやResNetの特徴マップを使用しており，質問文はLSTMの特徴ベクトルを使用している．
この2つの特徴ベクトルは結合され，質問と画像特徴の2つのAttentionがそれぞれ与えられてAugmented memoryに格納される．
そして，Augmented memoryを用いて最終的な回答が出力される．
提案手法では，右下図のように，稀に存在する困難な質問に対しても正確な回答を得ることができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1707.04968">論文リンク</a></li></ul></div></div><div class="slide_index">[#178]</div><div class="timestamp">2018.5.2 14:29:29</div></div></section><section id="Deep_Layer_Aggregation"><div class="paper-abstract"><div class="title">Deep Layer Aggregation</div><div class="info"><div class="authors">Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell</div><div class="conference">CVPR2018, arXive: 1707.06484</div><div class="paper_id">272</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Deep Neural Networkにおける，層間の結合に関して様々な検討を行った論文．従来のネットワーク(ResNet, DenseNet, FCN, U-Net等)のスキップ結合は，”浅い”結合しか適用されていなかった．
この論文では，より”深い” 結合をネットワークに取り入れ，少パラメータかつ高精度なネットワークモデルを構築している．
画像分類をはじめ，様々な認識タスクで実験を行い，高精度化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/272_overview.png" alt="272_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この論文では，右図のような4つのモデルを検討している(c~f)．(c)のようにシンプルに特定の層を集約して連鎖的に入力していくモデルから，(d~f)のように様々な層を集約して連鎖的に集約していくモデルを検討しており，上位層と下位層の層を効率的に伝播することで，認識精度を向上させている．
また，(c)と(f)のモデルを組み合わせることで，より性能を向上させることも可能である．
画像分類，Fine-grained Recognition，物体検出，セマンティックセグメンテーションで実験を行っており，全ての認識タスクにおいて高い性能を示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Deep CNNの次期モデルを検討しているような論文．結局，画像分類，検出，セグメンテーションではスキップ結合が重要であることを再確認できる．</p><ul><li><a href="https://arxiv.org/abs/1707.06484">論文リンク</a></li></ul></div></div><div class="slide_index">[#179]</div><div class="timestamp">2018.5.2 14:05:11</div></div></section><section id="Data_Distillation_Towards_Omni_Supervised_Learning"><div class="paper-abstract"><div class="title">Data Distillation: Towards Omni-Supervised Learning</div><div class="info"><div class="authors">Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He</div><div class="conference">CVPR2018, arXive: 1712.04440</div><div class="paper_id">536</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付きとラベルなしデータを用いることで画像認識の精度を向上させるData Distillationを提案．この手法では，self-trainingとHinton先生のKnowledge distributionをベースに提案されている．
この手法は，インターネット上のラベルなしデータを大量に学習できる．
この論文では，Mask R-CNNによる人のKeypoint検出と，FPNをbackboneにしたFaster R-CNNによる物体検出で高精度化を実現している．
(COCOをラベル付き，Sports-1M statistic framesとCOCO2017unlabel imagesをラベルなしデータとして使用．)</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/536_overview.png" alt="536_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一般的なラベルなしデータを扱うModel Distillationとは異なり，Data Distillationは1つのteacher modelとstudent modelを用いる．構造としては，1つの画像を複数の単純な変形を加え，それぞれの認識結果を得る．
そして，それぞれの認識結果を統合し，統合した認識結果をラベルとしてstudent modelを学習する．
ここで，学習に使用するラベルは”soft”なラベルではなく，”hard”なラベル．COCOをベースに実験をしており，ラベルなしデータを併用することで人のKeypoint検出と物体検出で高精度化を実現している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>シンプルかつ少量データの学習にも応用できるできるので，今後これをベースにした手法が増えそう．</p><ul><li><a href="https://arxiv.org/abs/1712.04440">論文リンク</a></li></ul></div></div><div class="slide_index">[#180]</div><div class="timestamp">2018.5.2 14:10:01</div></div></section><section id="Actor_and_Observer_Joint_Modeling_of_First_and_Third-Person_Videos"><div class="paper-abstract"><div class="title">Actor and Observer: Joint Modeling of First and Third-Person Videos</div><div class="info"><div class="authors">Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称（First Person View; 頭部にカメラを装着して撮影）かつ三人称（Third Person View; 環境に設置したカメラから撮影）の視点から人物行動や操作している物体を撮影したデータセットCharades-Egoを提供する。一人称/三人称視点は互いに対応付けされており、実に157の行動カテゴリ、112人の実演、4,000の動画ペア、全8,000動画を保有するデータベースの構築に成功した。手法の側面ではTripletによる弱教師付き学習（Weakly-supervised Learning）により一人称/三人称から抽出した複数の特徴量を評価する枠組みActorObserverNetを提案する。さらには、三人称から一人称視点への知識転換（Transferring Knowledge）をZero-shot行動認識の枠組みで実行する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503CharadesEgo.png" alt="180503CharadesEgo"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一人称/三人称は従来独立に撮影されて、それぞれのデータベースを構築して来たが、ここでは同時解析することにより行動に関するより詳細な考察（e.g. 間接的に行動を観察した方が良い vs. 操作している物体で行動を認識する方が良い）を行えるようにした。また、弱教師付き学習により特徴学習できるActorObserverNetを提案した。CVPRに通った理由はなんといってもデータベース（とそのベンチマーキング）、弱教師付き学習によるものである。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Hollywood in HomesのようにAMT（クラウドソーシング）にてユーザがフリーで使用を許可した動画を収集するのはアリにしている。公開してフリーにしても良い人だけの動画を効率良く集める仕組みが今後流行ってくるか？（ただ日本だと難しいかも？）データベースに対するベンチマーキングは若干少ない印象を受けるが、データベースの意義自体が優れているため査読を突破したと思われる。</p><ul><li><a href="https://arxiv.org/abs/1804.09627">論文</a></li><li><a href="https://www.youtube.com/watch?v=JkBFE2pzJkw&amp;feature=youtu.be">YouTube</a></li><li><a href="http://www.cs.cmu.edu/~gsigurds/">著者</a></li><li><a href="http://allenai.org/plato/charades/">Project/Database</a></li><li><a href="https://github.com/gsig/charades-algorithms">GitHub</a></li></ul></div></div><div class="slide_index">[#181]</div><div class="timestamp">2018.5.3 02:45:18</div></div></section><section id="The_Best_of_Both_Worlds_Combining_CNNs_and_Geometric_Constraints_for_Hierarchical_Motion_Segmentation"><div class="paper-abstract"><div class="title">The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation</div><div class="info"><div class="authors">Pia Bideau et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モーションセグメンテーションの問題を扱う。従来のモーションセグメンテーションは幾何的制約を設けることで効果的に動作をセグメントして来たが、高次なセグメントに失敗していた。一方でCNNについては従来方とは逆の特性があった。この両者の特性を活かして、両者にとって良いところどり（The Best of Both Worlds）することでモーションセグメンテーションの性能を向上させた。手法は図に示すようにオプティカルフローを用いた剛体の動き推定（Perspective Projection Constraints）、変形可能でより複雑な物体形状を推定できるようCNNによるセマンティックセグメンテーションを実行。物体のモーションモデルを形成するために、SharpMask（論文中文献35）による物体候補も導入し物体に関する知識を導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503MotionSegmentation.png" alt="180503MotionSegmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>クラシカルなフローによる剛体モーション推定とCNNによる物体セグメンテーションを統合、両者の良い部分を引き出しているところが評価に値した。アブストラクト/図１が非常にわかりやすくこの２つで問題設定を把握できるところもグッド。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau.pdf">論文</a></li><li><a href="http://vis-www.cs.umass.edu/">UMASS CV Lab.</a></li><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau-suppl.pdf">SupplementaryMaterial</a></li></ul></div></div><div class="slide_index">[#182]</div><div class="timestamp">2018.5.3 01:36:43</div></div></section><section id="Regularizing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present"><div class="paper-abstract"><div class="title">Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present</div><div class="info"><div class="authors">Xi.Cheny, L.Mazx, W.Jiangzx, J.Yaoy and W.Liuz</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.11439</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>encorder/decorderモデルにhiden stateと過去のhiden stateを再構成することによって隣接するhiden stateの接続を強化するためのARNetを導入．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Regulari zing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法問題点</h1><ul><li>従来のRNNのtrainとinferenceの間にはexposure biasと呼ばれる相違が存在する．</li><li>decorderはの入力に依存する演算子を用いて，キャプション生成する．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>RNNにおけるtransition dynamicsの正則化を助け，シーケンス予測の不一致の緩和が見られた．</li><li>ソースコードキャプション，イメージキャプションの両方で精度の向上が見られた．</li><li><a href="https://arxiv.org/pdf/1803.11439.pdf">Paper</a></li><li><a href="https://github.com/chenxinpeng/ARNet">github</a></li></ul></div></div><div class="slide_index">[#183]</div><div class="timestamp">2018.5.2 23:09:07</div></div></section><section id="Repulsion_Loss_Detecting_Pedestrian_in_a_Crowd"><div class="paper-abstract"><div class="title">Repulsion Loss : Detecting Pedestrian in a Crowd</div><div class="info"><div class="authors">Xinlong Wang, Tete Xiau, Yuning Jiang, Shuai Shao, Jian Sun and Chunhua Shen</div><div class="conference">CVPR2018, arXive:1711.07752</div><div class="paper_id">1005</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>群衆に頑健な歩行者検出法を提案．Faster R-CNNで群衆を検出したとき，歩行者同士の間にBounding Boxが出現しやすい．
これは，Bounding Box回帰の誤差を算出する時に誤差を最小にしようとして歩行者同士の間にBounding Boxが発生してしまう．
この現象を解決するために，新たにRepulsion Lossを導入し，群衆に対しても高精度な歩行者検出を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1005_overview_repLoss.png" alt="1005_overview_repLoss.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Repulsion Lossの中身は， L1 smooth lossをベースにしたL_RepGTとL_RepBoxから構成されている．L_RepGTは，targetの歩行者付近から最も近いGTとの誤差を示しており，targetと最も近いGTにBounding Boxが検出されると誤差が大きくなるように誤差が設計されている．
L_RepBoxは，複数のBounding Boxが特定の箇所に集中するように誤差を設定している．
L_RepBoxの目的は，NMSの割合の影響を減らすためである．
歩行者検出のCaltech, CityPerson(Cityscape)でstate-of-the-artな性能を出しており，Pascal VOCにおいても有効であることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>歩行者検出のベンチマークにおいて非常に高い性能を示しており，ResNetベースのFaster R-CNNに対してDilated Conv.を導入する等のちょっとしたテクニックも色々導入されている．</p><ul><li><a href="https://arxiv.org/abs/1711.07752">論文リンク</a></li></ul></div></div><div class="slide_index">[#184]</div><div class="timestamp">2018.5.2 12:15:00</div></div></section><section id="PackNet_Adding_Multiple_Tasks_to_a_Single_Network_by_Iterative_Pruning"><div class="paper-abstract"><div class="title">PackNet : Adding Multiple Tasks to a Single Network by Iterative Pruning</div><div class="info"><div class="authors">Arun Mallya, Svetlana Lazebnik</div><div class="conference">CVPR2018, arXive:1711.05769</div><div class="paper_id">1004</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>複数のデータセットを1つのネットワークで学習する場合，通常は過去に学習したデータセットは段々と精度が低下していく．これは，全てのパラメータに対して更新するため，過去に学習したデータセットの特徴を抽出できなくなっていくのが原因である．
この論文で着目していることは，大規模なネットワークは特定のパラメータは学習をサボる傾向があるところであり，このサボっているパラメータを使って効率よく学習させて複数のデータセットを学習させている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1004_overview_packnet.png" alt="1004_overview_packnet.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手法自体は非常にシンプルであり，特定のパラメータをプルーリング(右上図の白領域)して再学習する．そして，プルーリングしたパラメータのプルーリングを解放してパラメータをアップデートする．
特定のタスク(データセット)を学習した後は同じ要領でまたプルーリングと再学習を行う．
特定のパラメータを特定のタスクに割り当てるような学習をすることで，複数タスクに対応している．
結果としては，右図のようにタスクが追加されても性能がほとんど低下していない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な手法でありながら，非常に強力な手法．図2のインパクトがすごかった．様々な応用にも繋げれそう(Transfer Learning, Domain Adaptation等)</p><ul><li><a href="https://arxiv.org/abs/1711.05769">論文リンク</a></li><li><a href="https://github.com/arunmallya/packnet">コードリンク</a></li></ul></div></div><div class="slide_index">[#185]</div><div class="timestamp">2018.5.2 13:23:59</div></div></section><section id="Tell_Me_Where_to_Look_Guided_Attention_Inference_Network"><div class="paper-abstract"><div class="title">Tell Me Where to Look : Guided Attention Inference Network</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1247</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師あり学習で得られる物体のローカライゼーションを高精度にする研究．方法としては2つ提案しており，</p><ol><li>GAPのローカライゼーションを用いて物体の領域と背景の領域を明示的に学習させる方法と，</li><li>セマンティックセグメンテーションのラベルを用いて物体の詳細な領域を学習させる方法がある．セマンティックセグメンテーションと視覚的解釈に対する評価をしており，どちらのタスクも高い性能を示している．</li></ol></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1247_overview.png" alt="1247_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>1)の方法では，2streamなCNNをベースにしており，入力はそれぞれ通常の画像と，GAPのローカライゼーションから物体領域を排除した画像を入力する．この処理により，物体と背景を明示的に学習できる．
そして，セマンティックセグメンテーションでは，
1)のネットワークに加えて，セマンティックセグメンテーションのラベルと出力したAttention mapとの誤差を算出させることで，Attention mapを最適化させる．
Pascal VOCのweakly-supervisedによるセマンティックセグメンテーションのタスクで評価し，高い性能を示している．
また，発生するAttention mapの領域に対してオリジナルのデータセットを作成して評価している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10171">論文リンク</a></li></ul></div></div><div class="slide_index">[#186]</div><div class="timestamp">2018.5.2 13:37:25</div></div></section><section id="Beyond_Trade_off_Accelerate_FCN_based_Face_Detector_with_Higher_Accuracy"><div class="paper-abstract"><div class="title">Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1003</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>一般的な顔(物体)検出法(Faster R-CNN, FPN, SSD, YOLO等)は，Backboneな部分がFCNベースで構築されているため，各ピクセルを密に畳み込んで検出結果を出力する．しかし，顔検出では背景領域を大量に含んでおり，検出に必要な領域はごく僅かである．
本論文では，顔検出を効率化するために，2つのAttentionを適応して高速化を試みており，左上図のように高い性能を維持しつつ，4倍以上の高速化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1003_overview.png" alt="1003_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>本手法で適応しているAttentionは，右上図のようなspatial attentionとscale attentionである．spatial attentionは2次元上における顔の位置を示しており，scale attentionは出力されたスケールピラミッドから最適な特徴マップをAttentionで表現している．
spatial attentionは2次元の位置のattentionから探索する領域を制限するために使用し，scale attentionは探索するスケールピラミッドを制限するために使用する．
ネットワークは下図のようになっており，2つのAttentionにより背景と判定された領域は，マスクされた状態で後段のMask FCNに入力される．
AFW, FDDB, MALFでstate-of-the-artな性能かつ，高速な検出が可能(最速で14.2ms)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attentionを計算コスト削減に適応した物体検出法．顔検出や車載系の物体検出等の背景領域を多く含む問題設定では非常に効果的に使えそうな手法．
(COCO, VOCではあまりコストに対しては言及していない)</p><ul><li><a href="https://arxiv.org/abs/1804.05197">論文リンク</a></li></ul></div></div><div class="slide_index">[#187]</div><div class="timestamp">2018.5.2 13:55:50</div></div></section><section id="Deep_Marching_Cubes_Learning_Explicit_Surface_Representations"><div class="paper-abstract"><div class="title">Deep Marching Cubes: Learning Explicit Surface Representations</div><div class="info"><div class="authors">Y. Liao, S. Donné and A. Geiger</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存の学習ベースの3D面推定方法は，End-to-Endでの学習ができないが，本研究では，end-to-endでの学習を可能にした．3D面推定手法の一つのマーチングキューブは微分不可．そこで，代替の微分可能定式化を行い，これを3DNNの最終層として追加する．
また，疎な点群で学習が行えるようにロス関数群を提案．
サブボクセル精度での3D形状を推定可能であることを確認した．
本モデルは形状エンコーダ・推論と組み合わせられる柔軟さがある．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Marching_Cubes_Learning_Explicit_Surface_Representations_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>End-to-endで行われたものはない．適用範囲が広そう．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf">論文</a></li></ul></div></div><div class="slide_index">[#188]</div><div class="timestamp">2018.5.2 14:41:51</div></div></section><div class="title">Convolutional Image Captioning</div><div class="info"><div class="authors">J.Aneja, A.Deshpande and A.Schwing</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1711.09151v1</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>近年，条件付き画像生成や機械翻訳において畳み込みニューラルネットの功績は大きい，これを画像キャプションに応用してみた．ベースラインであるLSTMモデルと同等の精度を示し，パラメータ数ごとの学習時間の短縮をすることができた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Convolutional_Image_Captioning.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法の問題提起</h1><p>*RNNは学習プロセスが逐次的*LSTM，RNNは画像の分類精度が低い</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>*RNNとCNNのアプローチを分析し，CNNを用いたアプローチは出力確率分布のエントロピーの増大，単語予測精度の向上，消失勾配の影響の低下を示すことができた．*<a href="https://arxiv.org/pdf/1711.09151.pdf">論文</a>*<a href="https://github.com/aditya12agd5/convcap">github</a></p></div></div><div class="slide_index">[#189]</div><div class="timestamp">2018.5.1 18:06:38</div><section id="Are_You_Talking_to_Me_Reasoned_Visual_Dialog_Generation_through_Adversarial_Learning"><div class="paper-abstract"><div class="title">Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning</div><div class="info"><div class="authors">Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">741</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・強化学習とGANを用いたVisual Dialog回答文を自動生成する手法の提案．・従来のVisual Dialogシステムは画像とDialog履歴に基づきMLEにより回答文の予測を行う．こういった手法では回答文が短い，バリエーションが少ないなどの問題点がある．そこで， co-attentionを利用したジョイントで画像， Dialog履歴をreasonできる回答文生成器を提案した．提案モデルはsequential co-attention生成器と回答文が“human”からか“生成された”かを弁別できる弁別で構成される．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generate_visual_dialog.png" alt="Generate_visual_dialog"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・GANを用いた提案手法はVisual Dialogタスク従来の学習データの不足，簡潔な回答しか生成できないなどの問題点を改善した．・attentionをGANと組み合わせ， 生成回答文のinterpretabilityを向上した
・ VisDial データセットにおいて,従来の手法より高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・interactive環境でVisual Dialog回答文の生成ができたら更に様々な場面で応用できる</p><ul><li><a href="https://arxiv.org/pdf/1711.07613.pdf">論文</a></li></ul></div></div><div class="slide_index">[#190]</div><div class="timestamp">2018.5.2 13:13:20</div></div></section><section id="Density_Adaptive_Point_Set_Registration"><div class="paper-abstract"><div class="title">Density Adaptive Point Set Registration</div><div class="info"><div class="authors">Felix Järemo Lawin, Martin Danelljan, Fahad Khan, Per-Erik Forssen, Michael Felsberg</div><div class="conference">CVPR 2018</div><div class="paper_id">464</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・ 三次元センサーにより取得したPoint Set の密度の変動を対応できるPoint Set Registrationの手法を提案した．・従来の三次元センサー(例Lidar)により取得できるPoint Setの密度が均一ではない，一方，従来の確率的Point Set Registrationの手法は高密度の部分を対応させ，低密度の箇所の対応が重視されない問題点がある．提案手法はシーン構造の確率分布をモデリングすることにより，密度の変化にロバストに対応できる．
・提案手法は3次元シーンの構造及びフレーム間のカメラ移動量を同時にモデリングし， EMベースなフレームワークに基づきKL divergenceを最小化によりパラメータの最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Density-Adaptive-Point-Set-Registration.png" alt="Density-Adaptive-Point-Set-Registration"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・Lidarを用いたregistrationシステムのPoint Setの密度変化をロバストで対応できた．・ DAR-ideal、 VPS and TLS ETH datasetsなどのLidarデータセットで従来の確率的マルチビューRegistration手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・deep learningを用いていない手法</p><ul><li><a href="https://arxiv.org/pdf/1804.01495.pdf">論文</a></li></ul></div></div><div class="slide_index">[#191]</div><div class="timestamp">2018.5.2 10:39:57</div></div></section><section id="pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment"><div class="paper-abstract"><div class="title">pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment</div><div class="info"><div class="authors">J. Hong and C. Zach</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>カメラ姿勢推定，3次元復元に使われるバンドル調整では，適した初期値を与える必要があるが，初期値を与える必要を無くす提案をする．</p><p>アフィンバンドル調整問題においては，任意の初期化から到達可能な使いやすいminimaがあることが知られているが，その主な要因は，収束のワイドな領域を持つことで知られているVariable Projection（VarPro）法の導入によるものである．本研究ではPseudo Object Space Error（pOSE）を提案する．これは，アフィンと射影のモデルのハイブリッドで表現される複数カメラにおける目的関数である．
この定式化で，VarPro法に適したバイリニア問題構造となり，真の射影復元と近い3D復元結果を得られる．
実験では，ランダムな初期化から高い成功率で正しい3D復元を得られることを確認した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ランダム初期値でもメトリックの正しい3D復元が行える．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://github.com/jhh37/pose/blob/master/Documents/hong_and_zach_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#192]</div><div class="timestamp">2018.5.2 10:31:48</div></div></section><section id="Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">Finding Tiny Faces in the Wild with Generative Adversarial Network</div><div class="info"><div class="authors">Yancheng Bai, Yongqiang Zhang, Mingli Ding, Bernard Ghanem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>GANを用いて画像中の顔を検出する研究。検出が難しい顔として小さくかつボケている顔が挙げられるが、これらの顔をGANによって高解像度かつはっきりとした顔にすることで検出精度を向上させる手法を提案。
generatorは高解像度にするsuper resolution network(SRN)と顔の詳細な情報を復元するrefinment network(RN)を結合したネットワークである。
discriminatorはVGG19であり、ロスとしてデータセットの顔/generatorによる顔、顔/顔ではないモノを同時に行うロスを導入。
またよりはっきりとした顔を生成するために、generatorのロスとして物体識別のロスを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>GANによって画像中の顔から高解像度かつはっきりとした顔を生成することで高精度な顔検出手法を提案。</li><li>GANの導入による精度の向上、導入したロスの有効性を確認している。</li><li>state-of-the-artと比較して、最も高い検出精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>検出精度が非常に高く、データセットではアノテーションし忘れている顔すらも検出してしまい、これによって精度が悪いように見えてしまうと主張している。</li><li>テスト時も学習時と同様に画像全体ではなくROIを与えているため、実行時間はそれなりにかかりそう。</li><li><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/Finding%20Tiny%20Faces%20in%20the%20Wild%20with%20Generative%20Adversarial%20Network.pdf">論文</a></li><li><a href="https://ivul.kaust.edu.sa/Pages/pub-tiny-faces.aspx">Project page</a></li></ul></div></div><div class="slide_index">[#193]</div></div></section><section id="Context_Encoding_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Context Encoding for Semantic Segmentation</div><div class="info"><div class="authors">Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal</div><div class="conference">CVPR 2018</div><div class="paper_id">893</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・コンテキスト情報の抽出を利用したセマンティックセグメンテーションの効率を上げられるContext Encoding Moduleを提案した．・従来の階層式シーンの高レベルから低レベル特徴の抽出を行うネットワーク(eg. PSPNet)にはシーンのコンテキスト情報の抽出がexplicitではない問題点があり，従来のグローバル特徴抽出ネットワークの知識から，シーンのコンテキスト情報を抽出することにより，セマンティックセグメンテーションの効率を上げられるモジュールを提案した．
・具体的には：Encodingによりシーンのコンテキスト情報をキャプチャーし，クラス依存の特徴マップを選択的に強調表示できるContext Encoding Moduleを提案した； Semantic Encoding Loss (SE-loss)を提案した； Context Encoding Moduleを利用したセマンティックセグメンテーションネットワークEncNetを提案した</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Context-Encoding-for-Semantic-Segmentation.png" alt="Context-Encoding-for-Semantic-Segmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ PASCAL VOC 2012において85.9% mIoUを達成した・提案ネットワークをCIFAR-10 datasetに応用し，14層だけのネットワークで100層超えのネットワークと同じレベルの精度を実現した</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・シンプルなネットワークでstate-of-the-artな精度を実現したので，将来的に広く用いられそう</p><ul><li><a href="https://arxiv.org/pdf/1803.08904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#194]</div><div class="timestamp">2018.5.1 17:39:13</div></div></section><section id="Video_Based_Reconstruction_of_3D_People_Models"><div class="paper-abstract"><div class="title">Video Based Reconstruction of 3D People Models</div><div class="info"><div class="authors">Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>人間が動いている単眼のRGB映像から、正確な3次元物体モデルと任意の人物テクスチャを得る研究。仮想現実や拡張現実、監視やゲームなどの人間の追跡にはアニメーション可能な人間行動の3Dモデルが必要である。この研究では、動的な人間のシルエットに対応するシルエット形状を見つけ出し、テクスチャや骨格を推定して、アニメーション可能なデジタルダブルを作成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803.04758_1.png" alt="1803.04758_1"><img src="slides/figs/1803.04758_2.png" alt="1803.04758_2"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性・結果</h1><p>(a). SMPLモデルを用いてポーズを計算(b). シルエットの赤で描かれていないシルエットを取り除く
(c). 正規のTポーズで被写体の形状を最適化
(d). ティクスチャを計算しパーソナライズされた好みの形状を生成
・単眼のRGBビデオから髪や衣服を含む現実的なアバターを抽出
・被服を含む4.5mmの精度で人体形状を再構成</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p><a href="http://link.com/link1/">link</a></p></div></div><div class="slide_index">[#195]</div><div class="timestamp">2018.5.1 16:26:45</div></div></section><section id="Relation_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Relation Networks for Object Detection</div><div class="info"><div class="authors">Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei</div><div class="conference">CVPR 2018</div><div class="paper_id">439</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチオブジェクトのアピアランス特徴及び幾何情報間の関係を取り扱える，様々なタスク（物体検出，VQAなど）に用いられるObject Relation Moduleを提案した．・最近attentionに関する研究が発展し，著者たちがattentionモジュールがelement間の依頼性を学習できる面から，物体検出に応用できるアテンションモジュールを提案した．
・提案モジュールを物体検出の2つの段階に応用できる：インスタンス認識段階で提案モジュールによりオブジェクト間の関係を習得でき，精度を上げられる；duplicate removal段階で提案モジュールにより有効的に物体領域を抽出できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Relation-Networks-for-Object-Detection.png" alt="Relation-Networks-for-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来の物体検出手法は物体ごとに推定を行い，物体間の関係を利用しない．提案手法はObject Relation Moduleを提案し，物体間の関係を学習することで，物体検出の精度を更に向上した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・提案モジュールが付加の監督信号不要，既存なネットワークに追加しやすい特徴があるため，様々なタスクでの応用が期待される</p><ul><li><a href="https://arxiv.org/abs/1711.11575">論文</a></li></ul></div></div><div class="slide_index">[#196]</div><div class="timestamp">2018.5.1 16:37:43</div></div></section><section id="PPFNet_Global_Context_Aware_Local_Features_for_Robust_3D_Point_Matching"><div class="paper-abstract"><div class="title">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</div><div class="info"><div class="authors">Haowen Deng, Tolga Birdal, Slobodan Ilic</div><div class="conference">CVPR2018</div><div class="paper_id">45</div></div><div class="slide_editor">Shuichi Akizuki</div><div class="item1"><div class="text"><h1>概要</h1><p>点群データから直接3Dの局所特徴量を抽出するネットワークを提案．N-Tuple loss(Triplet lossの拡張)によって，
対応点間の特徴量が近く，それ以外の特徴量間の距離が遠くなるような変換を学習する．
PPFNetの入力は局所パッチ内の点の座標，法線，Point Pair Featureをまとめたデータ．
ネットワークの内部ではPointNetを利用する．
大域的な情報を得るために，各パッチから取得した局所特徴量を
Max poolingによって大域特徴量化し，局所特徴と結合する工夫も入れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/45.png" alt="45"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>局所特徴量を生成するネットワークを構築した点，N-Tuple lossによる学習法を提案した点が新しい．
キーポイントマッチングのベンチマークでRecall rateが向上．
オーバーラップが少ないシーンでのレジストレーションも可能になっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.02669">Paper</a></li></ul></div></div><div class="slide_index">[#197]</div><div class="timestamp">2018.5.1 15:53:31</div></div></section><section id="Geometry-Aware_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">GAGAN: Geometry-Aware Generative Adversarial Networks</div><div class="info"><div class="authors">Jean Kossaifi, Linh Tran, Yannis Panagakis and Maja Pantic</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のGANでは考慮されていなかった形状や位置といった幾何学的情報をGANの生成プロセスに組み込んだGeometry-Aware Generative Adversarial Networks (GAGAN) を提案．具体的にGAGANでは，ジェネレータで統計的情報な形状モデルの確率空間から潜在関数をサンプリングする．次にジェネレータの出力値を微分可能な幾何学変換を介して標準座標系にマッピングすることで，物体の形状や位置といった情報を強制し，生成を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_gagan.png" alt="Item3Image"><img src="slides/figs/20180501_gagan2.png" alt="Item4Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GAGANのような幾何学的情報を考慮した生成モデルはなく，GAGANが初</li><li>入力画像の属性の形状に合わせて，画像を生成することが可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>今後は，(i)より大きな画像の生成，(ii)アフィン変換によって起こりうる変形を緩和するより複雑な幾何学的変換の探索およびそれによるGAGANの拡張，(iii)顔のランドマーク検出のための従来CNNアーキテクチャの拡張に取り組む予定</p><ul><li><a href="https://arxiv.org/pdf/1712.00684.pdf">論文</a></li></ul></div></div><div class="slide_index">[#198]</div><div class="timestamp">2018.5.1 14:41:53</div></div></section><section id="IQA_Visual_Question_Answering_in_Interactive_Environments"><div class="paper-abstract"><div class="title">IQA: Visual Question Answering in Interactive Environments</div><div class="info"><div class="authors">Daniel Gordon, Ali Farhadi, Aniruddha Kembhavi, Dieter Fox, Mohammad Rastegari, Joe Redmon</div><div class="conference">CVPR2018</div><div class="paper_id">533</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新たな問題設定ー動的環境とインターアクトしながら視覚質問に答える(IQA)を提案した．・具体的には， IQAには4つの設定がある：環境でナビゲートする能力；環境中のオブジェクト，アクション及びアフォーダンスの理解；環境中のオブジェクトとインターアクトする能力；質問文に応じで環境での行動を計画する能力．
・提案の問題設定を解決するために，階層的マルチレベルで行動計画及びコントロールするネットワークHIMN及び空間的かつセマンティックなメモリを実現できる新たなrecurrent layer形式Egocentric Spatial GRUを提案した．
・更に，75000質問及びCGシーンを含んだデータセットIQUAD V1を提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual-Question-Answering-in-Interactive-Environments.png" alt="Visual-Question-Answering-in-Interactive-Environments"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のVQAタスクをCGシーンでの自己ナビゲーションと組み合わせた新たな問題設定を提案した．・IQUAD V1で従来の手法よりstate-of-the-artな精度</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・従来のVQAタスクに更に環境での探索および環境中オブジェクトとのインターアクトを取り入れ，従来の問題設定より一層現実に近づいている．・質問文の自動生成にも応用できそう
・特に色々なタスクを取り扱えているので，技術の面では向上する空間がありそう</p><ul><li><a href="https://arxiv.org/pdf/1712.03316.pdf">論文</a></li></ul></div></div><div class="slide_index">[#199]</div><div class="timestamp">2018.5.1 15:29:03</div></div></section><section id="On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks"><div class="paper-abstract"><div class="title">On the Robustness of Semantic Segmentation Models to Adversarial Attacks</div><div class="info"><div class="authors">Anurag Arnab, Ondrej Miksik and Philip H.S. Torr</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>adversarial attackに対するロバスト性の評価を, semantic segmentationにおいてstate-of-the-artな性能を持つネットワークを用いて実験した.Pascal VOCとCityscapesのデータセットに対して, FGSM, Interative FGSM, FGSM II, Interative FGSM IIで攻撃したときのIoU Ratioによりロバスト性を評価した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG" alt="On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>ResNetをバックボーンに持つネットワークがロバストであることがわかった. 中でもDeeplab v2が最もロバスト.</li><li>multi-scale processingやmean field CRFによりロバストになる.</li><li>画像分類の分野で一般的なロバスト性やモデルサイズについての知識がsemantic segmentationでも有用とは限らない.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.09856.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#200]</div><div class="timestamp">2018.5.1 14:32:41</div></div></section><section id="CodeSLAM_---_Learning_a_Compact_Optimisable_Representation_for_Dense_Visual_SLAM"><div class="paper-abstract"><div class="title">CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM</div><div class="info"><div class="authors">Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew Davison</div><div class="conference">CVPR  2018</div><div class="paper_id">288</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・RGB画像の強度データと少数のパラメータを条件に，ほぼリアルタイムで行えるデンスなシーン幾何を推定手法を提案した．・提案手法UNet構造により強度画像の特徴抽出を行い，更に抽出特徴をauto-encoder構造を用いたデプス情報推定ネットワークに入力することで階層的にデプス情報推定を行う．また，カメラ移動中得られるマルチフレームに対し，フレームごとのデプス推定及びフレーム間のカメラモーションをジョイントで最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/codeSLAM.png" alt="codeSLAM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・デンスなデプス情報推定を行うことでSLAMシステムの更なる精度向上できると宣言した．・初めてのほぼリアルタイムで行えるカメラモーションとシーンのデンス幾何をジョイントで推定する研究である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・著者たちは将来のワークとして，提案手法をリアルタイムでデンスなSLAMシステムの構築に拡張すると指摘し,将来的な研究を期待している．</p><ul><li><a href="https://arxiv.org/abs/1804.00874">論文</a></li></ul></div></div><div class="slide_index">[#201]</div><div class="timestamp">2018.5.1 14:08:56</div></div></section><section id="Learning_by_asking_questions"><div class="paper-abstract"><div class="title">Learning by asking questions</div><div class="info"><div class="authors">Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, Laurens van der Maaten</div><div class="conference">CVPR 2018</div><div class="paper_id">3</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQAタスクに用いられる新たなインターアクティブ学習フレームワークを提案した．・提案フレームワークは入力画像から，question proposal moduleにより問題集を生成し，画像との相関性を基準に問題集をフィルタリングし，残った問題をVQAにより解く．予測した答え，自己の知識及び過去の知識から質問を1つ選び，oracleにより答える．
・提案フレームワークにより，効率高い学習サンプルを得られる．また，従来のVQAネットワークで用いられるstate-of-the-artな問題集を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/learning_by_asking_questions.png" alt="learning_by_asking_questions"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のあらゆるフレームワークは学習データから学習を行う．この論文で，質問文の自動生成できる及び質問を選択する構造を導入し，自動的でインターアクティブで環境から情報を獲得することを可能にした．・実験を通し，提案手法により質問を選択する規制がsampleの効率を高められる．（従来と同じ精度の場合，学習データ量を40％減らせる）</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>real-worldバージョンのLBAシステムが実現されたら，機械で学習することは更に人の学習システムに近づく．</p><ul><li><a href="https://arxiv.org/abs/1712.01238">論文</a></li></ul></div></div><div class="slide_index">[#202]</div><div class="timestamp">2018.5.1 12:10:26</div></div></section><section id="Learning_Spatial-Temporal_Regularized_Correlation_Filters_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</div><div class="info"><div class="authors">Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1353</div></div><div class="slide_editor"><Takahiro>Itazuri</Takahiro></div><div class="item1"><div class="text"><h1>概要</h1><p>Spatially Regularized Discriminative Correlation Filters (SRDCF)に空間正則化を導入した一般物体追跡手法Spatial-Temporal Regularized Correlation Filters (STRCF)を提案. SRDCFは複数学習画像を利用するため, 計算量が大きくなってしまうことに着目し, 単一学習画像に対するSRDCFにonline Passive-Aggresive learningの考えに基づいて時間正則化を導入. STRCFはADMMで直接解くことができるため, DCFの高速性を保持したまま高い精度で追跡が可能となっている.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/STRCF.png" alt="STRCF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単一学習画像に対するSRDCFに時間正則化を導入することで, 複数学習画像に対するSRDCFを近似したSRTCFを定式化</li><li>online Passive-Aggresive learningを拡張することで, STRCFは大きな見た目の変化に対して頑健である</li><li>SRTCFはADMMを用いて, 3つの部分問題に帰着させ, Eckstein-Bertsekas条件を満たし, 大域的最適解への収束性を保証している</li><li>OTB-2015, Temple-Color, VOT-2016データセットにおいてSRDCFより精度も計算速度も向上させた</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li><li><a href="https://github.com/lifeng9472/STRCF">コード</a></li></ul></div></div><div class="slide_index">[#203]</div></div></section><section id="Learning_Spatial-Aware_Regressions_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Aware Regressions for Visual Tracking</div><div class="info"><div class="authors">Chong Sun, Huchuan Lu, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1676</div></div><div class="slide_editor"><Takahiro>Itazuri</Takahiro></div><div class="item1"><div class="text"><h1>概要</h1><p>一般物体追跡手法の二大手法であるカーネルリッジ回帰（相関フィルタを含む）とCNNのハイブリッドな手法を提案した.カーネルリッジ回帰は全体的な情報に,CNNは局所的な情報に注目するように設計している.それぞれの導入がどの精度向上に結びついているかも検討している.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LSART.png" alt="LSART"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>cross-patch similarityを用いたカーネルリッジ回帰モデルを提案し,それをニューラルネットに再定式化.</li><li>spatially reguralized kernelとdistance transform pool layerを用いて,出力の各チャンネルが特定の領域に反応するようなCNN提案.</li><li>提案したカーネルリッジ回帰とCNNを相補的に用いることで,OTB-2013,OTB-2015,VOT-2016データセットでstate-of-the-artな精度を達成.		</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://lanl.arxiv.org/pdf/1706.07457v1">論文1</a></li><li><a href="https://drive.google.com/file/d/1Lls2CK-yTkeOcOarNGapmcz4h2HQj6gf/edit">論文2</a></li></ul></div></div><div class="slide_index">[#204]</div></div></section><section id="Improved_Fusion_of_Visual_and_Language_Representations_by_Dense_Symmetric_Co-Attention_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</div><div class="info"><div class="authors">Nguyen Duy Kien, Takayuki Okatani</div><div class="conference">CVPR 2018</div><div class="paper_id">739</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>VQAタスクに用いられるattentionメカニズム“Dense Co-attention Network”(DCN)を提案した．DCNはfully対称的で，階層的にスタックできるため，マルチステップで視覚及び言語特徴のインターアクションを可能にする．具体的には，まず言語から画像の注目マップ及び画像から言語の注目マップを生成し，そして連結によりマルチモデルの特徴を融合する（dense co-attention layer)．そして階層的にdense co-attention layerをスタックにより，さらにマルチモデル特徴を深く探る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Co-attention_VQA.png" alt="Co-attention_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のattention for VQAタスクより，有効的でデンスな視覚と言語モデルの特徴の融合メカニズムDCN（構造的にも簡潔で拡張しやすい）を提案し，将来の様々なVQAタスクに用いられる．・VQA, VQA2.0データセットで2017 VQA優勝したモデルより良い精度を達成した．
・定性的な実験により，提案モデルが有効的にattentionを抽出できることを証明した</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.00775.pdf">論文</a></li></ul></div></div><div class="slide_index">[#205]</div></div></section><section id="Deep_Voting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion"><div class="paper-abstract"><div class="title">DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion</div><div class="info"><div class="authors">Z. Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>画像中から物体のパーツ（車のタイヤなど）を検出するための新しい手法を提案．投票ベースの手法でオクルージョンへの頑健性を持つ．
Visual ConceptというMid-levelな特徴をベースにして，
個々のMid-level特徴から推定されるパーツの位置推定結果を積み重ねていくことでパーツを検出する．
Visual Conceptの検出とそれに基づく投票処理はConvolutionによって実装されており，
End-to-Endでの学習が可能になっているところがポイント．
Faster-RCNNといった物体検出アプローチよりもオクルージョンに頑健なことが実験的に確認できている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png" alt="DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>CNNベースのVotingによるオクルージョンに頑健なパーツ検出手法を提案</li><li>Visual Conceptの検出から投票までConvolutionで実装</li><li>人工的なオクルージョン環境下での有効性を確認</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><ul><li>投票処理までConvolutionで表現されているのが面白い</li><li><a href="https://arxiv.org/abs/1709.04577">論文</a></li><li><a href="http://lingxixie.com/PDFs/Zhang_CVPR18_DeepVoting_SuppMat.pdf">Supplementary Material</a></li></ul></div></div><div class="slide_index">[#206]</div><div class="timestamp">2018.4.23 06:07:59</div></div></section><section id="Feature_Mapping_for_Learning_Fast_and_Accurate_3D_Pose_Inference_from_Synthetic_Images"><div class="paper-abstract"><div class="title">Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images </div><div class="info"><div class="authors">Mahdi Rad   et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成データを利用した、6D pose estimationとdepth based 3D hand pose estimationの研究。</p>埋め込み空間内で、合成データから実データへのマッピング関数を学習する。その関数の学習のためには実データに対応する(grand truthが同じ)合成データが必要であるので、教師あり実データがある程度あることが前提としてある。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Mahdi.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>残差構造を持つmapping netを対応するペアを用いて学習する。従来のドメイン適応手法と比較しても提案手法の精度が良く、適応の有無による性能の差も非常に大きい。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>手法としてはかなりstraight forwardな印象。実データの量を変化させた時の精度変化の結果はあったが、合成データの量を変化させた時の精度変化が気になる。</p><ul><li><a href="https://arxiv.org/pdf/1712.03904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#207]</div></div></section><section id="Embodied_Question_Answering"><div class="paper-abstract"><div class="title">Embodied Question Answering</div><div class="info"><div class="authors">Abhishek Das et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元空間において、エージェントに質問の答え（例：車の色は？）を探させる研究。初期位置における視覚情報だけでは答えに行きつかないためにエージェントは移動しながら答えを探していく。
エージェントの移動には、どの方向（forward, rightなど)に進むかを決定するplannerとどこまで進むかを決定するcontrolerによって行う。
目的地(正解が分かる場所)にたどり着いた時点で、最後の5フレームを用いて172の選択肢から正解を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Embodied_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>LSTMを使った場合の方が目的地により近付けるという結果が得られた。強化学習なしのものは目的地により近づいている一方、ファインチューニング＋強化学習の方が正解率は高いという結果となった。
また、最短経路を与えてVQAによって答えさせる場合でも精度が悪く、答えを導くにあたってどの方向から目的地に近づくかも重要であるということが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://embodiedqa.org/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#208]</div><div class="timestamp">2018.4.23 12:59:26</div></div></section><section id="Learning_from_Synthetic_Data_Addressing_Domain_Shift_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</div><div class="info"><div class="authors">Sankaranarayanan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>GANによる画像生成の枠組みを中間的に取り入れることでSemantic segmentationにおけるドメイン適応を行う研究。</p>従来の特徴ベクトルに対する敵対的学習によって埋め込み空間におけるdomain gapを縮める手法に対して、この研究では特徴ベクトルから画像を復元し、その画像が識別器によってどのドメインからの復元か識別できないように埋め込み関数を学習させる。
合成データからのドメイン適応で最も良い精度を達成。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Swami.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p> Source(S)は教師ありデータ、Target(T)は教師なしデータ。学習のフローは以下である: 
(1)識別器(D)は入力画像に対してpixel-wiseにsource real(SR), source fake(SF), target real(TR), target fake(TF)の4値分類を学習。(2)生成器(G)は入力特徴ベクトルからDによってSからの特徴はSRに、 Sからの特徴はTRに分類されるよう学習。
(+入力との担保を取るL2Loss)(3)埋め込み関数(F)はSからの入力はTRに、Tからの入力はSRに分類されるように学習。さらにSからのサンプルに対してはFからの特徴マップを入力としてsegmentation taskを解くCNNを学習。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>論文内にこの手法がうまくいく理由の裏付け的実験や考察が詳細にはなかったが、特徴量から画像再生成を行うことによる入力情報の保存とS/T間の敵対的学習による分布の混合が一つのフローで行えていることが効いているように思えた。実際特徴量に対するS/T間の敵対的学習のみの場合よりも大きく精度が向上している。</p><ul><li><a href="https://arxiv.org/abs/1711.06969">論文</a></li></ul></div></div><div class="slide_index">[#209]</div></div></section><section id="Natural_and_Effective_Obfuscation_by_Head_Inpainting"><div class="paper-abstract"><div class="title">Natural and Effective Obfuscation by Head Inpainting</div><div class="info"><div class="authors">Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, Mario Fritz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SNSなどで共有された画像には、プライバシー保護の問題が生じる。プライバシー保護のために顔領域にぼかしや黒塗りなどの処理がされることが多いが、画像としては不自然さが残ってしまう。
そこで、塗りつぶされた領域に顔を挿入することで自然な画像ではあるが別人のためプライバシーを保護できる画像を生成する。
提案手法は、特徴点検出（生成）と顔の挿入の2つのステップに分かれる。
特徴点検出（生成）では、オリジナルの顔画像が存在する場合は既存の特徴点検出によって特徴点を検出する。
対称の画像が既に黒塗りされているなどで特徴点検出ができない場合は、GANによって特徴点を生成する。
次のステップでは、黒塗りされている顔画像と特徴点を入力し、黒塗りされた領域に顔の挿入を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Natural_and_Effective_Obfuscation_by_Head_Inpainting.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>特徴点生成器は、GANによって生成することで正解値とのノルム最小化よりも高い精度で生成することを可能にした。画像に対する処理としてぼかしと黒塗りを比較したところ、ぼかしは顔の情報が一部残るため高い精度での生成が可能である一方、元の人物の情報は黒塗りよりも多く残ることが分かった。
また、顔の形状にも個人性が含まれるためオリジナル画像から検出した特徴点よりもGANによって生成した特徴点を使用した方が個人性は損なわれることが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09001">論文</a></li></ul></div></div><div class="slide_index">[#210]</div><div class="timestamp">2018.4.18 15:45:44</div></div></section><section id="Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections"><div class="paper-abstract"><div class="title">Augmenting Crowd-Sourced 3D Reconstructions using Semantic Detections</div><div class="info"><div class="authors">T. Price, J. L. Schonberger, Z. Wei, M. Pollefeys and J.M. Frahm</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>SfMにおいて，一つの撮影にしか映らないような移動物体を考慮することで，そのシーンの絶対スケールが推定可能になるし，人混みだと見えにくい地平面の復元も成しうる．個々の撮影画像において検出された人を3次元空間に投影し，さらに物体の意味情報（本稿では背の高さの分布）から絶対スケールを推定する．
また，人検出結果を用いて地平面推定も行う．
ランダムなインターネット画像で手法をデモンストレーションし，量的評価を行う．</p><p>人検出はトルソモデルのフィッティングに基づく．画像における肩，腰の位置が推定でき，おおよその立ち位置も分かるということ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>評価点</h1><p>若干SIGGRAPH的な気風のある，面白い視点を提供する論文．過去の知見に基づく高品質な人検出などを用いて成し得た，正統なアプリケーションに感じる．
動画のインパクトも大きいので，一度視聴を勧める．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://demuc.de/papers/price2018augmenting.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=eQWXuPv5eC0">動画</a></li></ul></div></div><div class="slide_index">[#211]</div><div class="timestamp">2018.4.16 17:07:53</div></div></section><section id="Single_View_Stereo_Matching"><div class="paper-abstract"><div class="title">Single View Stereo Matching</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li and Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単眼奥行き推定法では, 推論の際に幾何的な制約を明示的に課していないことや多くのground truth labeled dataが必要といった問題があった.この研究では単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えることにより, 従来法の問題を解決する.
view synthesis問題では, 入力を左画像として捉え, view synthesis networkにより右画像を生成する. stereo matching問題では, 左画像を右画像を用いstereo matching networkにより奥行きを推定する.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/single_view_stereo_matching.PNG" alt="single_view_stereo_matching.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えた.</li><li>従来法の問題を解決.</li><li>従来のどの方法よりも精度が高い.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02612.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#212]</div><div class="timestamp">2018.4.30 18:04:11</div></div></section><section id="Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs"><div class="paper-abstract"><div class="title">Learning Face Age Progression: A Pyramid Architecture of GANs</div><div class="info"><div class="authors">Hongyu Yang, Di Huang, Yunhong Wang and Anil K. Jain</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>入力画像中の人物の老化顔をGANによって生成する手法の提案。Discriminatorには生成した画像が合成画像であるか及び目標年代の特徴を保持しているかを判定させ、それに加え元の画像とのL2ノルム及び元の顔画像と同一人物であるかをロスに加えることで、同一人物性を保持している。
その際、Discriminatorの中間層の各出力を途中で取り出すことにより（ピラミッド型ネットワーク），様々な解像度からの年齢特徴の抽出を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>年齢推定及び個人認証タスクによって有効性を確認した。従来手法では髪や額領域は変化できなかったが、提案手法によってこれらの要素を変化させることを可能とした。
Discriminatorをピラミッド型にすることにより、従来手法に比べてより詳細な老化特徴を取り出すことに成功。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10352">論文</a></li></ul></div></div><div class="slide_index">[#213]</div><div class="timestamp">2018.4.16 16:14:24</div></div></section><section id="Image_Generation_from_Scene_Graphs"><div class="paper-abstract"><div class="title">Image Generation from Scene Graphs</div><div class="info"><div class="authors">Justin Johnson et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体同士の関係を表すScene Graphsから画像を生成する手法の提案。従来のテキストから画像を生成する手法よりも物体の数が多く複雑なシーンの画像を生成することができる。
初めに、Scene Graphsを処理するネットワークによってScene Graphsを表現するベクトルを取得し、そこから画像のレイアウトを作成する。
次にレイアウトからCRN(参考文献)を用いて画像を作成する。
作成された画像は、画像全体のリアルさと各物体のリアルさを評価するDiscriminatorによってリアルな画像であるかを評価する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180411graph.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>ユーザースタディの結果、StackGANと比較して合成結果が良いと答えた人が68%、認識可能な物体を生成できてると答えた人が59%という結果が得られた。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.01622">論文</a></li><li><a href="https://arxiv.org/abs/1707.09405">CRN</a></li><li><a href="https://arxiv.org/abs/1612.03242">StackGAN</a></li></ul></div></div><div class="slide_index">[#214]</div><div class="timestamp">2018.4.11 15:58:22</div></div></section><section id="Bottom-Up_and_Top-Down_Attention_for_Image_Captioning_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</div><div class="info"><div class="authors">Peter Anderson, Xiaodong He, Chris Buehler,Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang</div><div class="conference">CVPR 2018</div><div class="paper_id">738</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>Image captioningとVQAタスクに用いられるBottom-upとtop-down attentionをコンバインするメカニズムを提案した．従来のオブジェクトレベルの領域の抽出のほか，salient 領域の抽出も行う．Faster R-CNNを利用したbottom-up的にsalient 領域を特徴ベクトルを抽出し， top-downにより特徴のウェットを決めることをベースに， Image captioningとVQAのアーキテクチャを提案し（右図），両方ともstate-of-artな性能を得られた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Bottom_up_Top_down_VQA.png" alt="Bottom_up_Top_down_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のVQAとImage captioningは主にタスクスペシフィックなtop-downタイプのattentionを用いる．この論文で，人の視覚attentionメカニズムから，タスクスペシフィックなtop-downタイプのattentionを及びsalient 領域に注目するBottom-upのattentionを用いることと主張した．・2017 VQA Challengeにおいて優勝した．VQA v2.0 test-standardにおいて70.3%の精度を達成した．また， Image captioning タスクに対しMSCOCO Karpathy testで従来の手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1707.07998.pdf">論文</a></li></ul></div></div><div class="slide_index">[#215]</div><div class="timestamp">2018.4.27 10:27:30</div></div></section><section id="Tips_and_Tricks_for_Visual_Question_Answering_Learning_from_the_2017_Challenge"><div class="paper-abstract"><div class="title">Tips and Tricks for Visual Question Answering: Learning from the 2017 Challenge</div><div class="info"><div class="authors">Damien Teney, Peter Anderson, Xiaodong He, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">547</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>2017 VQA Challengeに優勝したモデルのモデル詳細を紹介し，さらにいかにVQAモデルの精度を上げられるかのコツとテクニックを紹介した．モデルのコアなところは視覚と質問文の意味特徴をジョイントでエンベディングし，さらにマルチ-ラベル予測を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tip_Tricks_VQA.png" alt="Tip_Tricks_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>論文により，VQAの性能上げるために，以下のテクニックがある：1.sigmoid outputsを用いて，マルチアンサーをできるようにする．2．Soft scoresを用いて，分類ではなく回帰を行う．3．Bottom-up attentionから注目領域の画像特徴を用いる．4．Gated tanhを活性化関数に用いる．5．Pre-trainedウェットで初期化する．6．ミニバッチサイズを大きく設定し，training-dataにシャッフリングを用いる</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1708.02711">論文</a></li></ul></div></div><div class="slide_index">[#216]</div><div class="timestamp">2018.4.26 16:58:02</div></div></section><section id="What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets"><div class="paper-abstract"><div class="title">What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「3DCNNが実は動き特徴を捉えられていないのではないか」という考えのもと、3DCNNにおける動き特徴の影響の上界を実験的に求める。提案する工夫により、この影響のかなり低い上界を得ることができ、動き特徴を捉えているのではない(例えば実は複数フレーム入力から「重要なフレーム選択」を行っているなど)ことを示唆した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets.png"></p></div></div><div class="item3"><div class="text"><h1>検証方法</h1><p> 通常の16frames入力で学習したC3Dにおいてtest時にsub-samplingした(動き情報を無くした)設定下でできるだけ精度を上げることで結果的に動き特徴の上界を得る。Naïveにsub-samplingを行うと入力のデータ分布の明らかな違いから動き以外の精度低下への影響をもたらすと考えられるため、 sub-samplingされたclipから元clipを生成するgeneratorを構築。学習はC3Dの中間層の値をMSEで近づける。
またsampling方法によっても精度は変わるという考えから、識別confidenceが最大となるframesをsamplingする。注意として、この際動きに関しては全く考慮せずにsamplingしてきている。</p></div></div><div class="item4"><div class="text"><h5>コメント・リンク</h5><p>結果として、かなりきつい上界を求められ、論文内では3DCNNが2Dよりも精度が良いのは動き特徴ではなく、複数フレーム入力の中で最も識別しやすいフレームを選択可能になるからではと述べられている。</p><p>フレーム選択をしているという仮説は面白いし、select frameによって精度が上昇したり、動きが大きい動画はフレーム単位での推定結果の分散が大きいなどから十分ありえそう。これが本当なら、optical flowを3dCNNに導入して大きく精度が向上することともつじつまが合いそう。</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-fb.pdf">論文</a></li></ul></div></div><div class="slide_index">[#217]</div></div></section><section id="Surface_Networks"><div class="paper-abstract"><div class="title">Surface Networks</div><div class="info"><div class="authors">Ilya Kostrikov, Joan Bruna, Daniele Panozzo, Denis Zorin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>3D triangleメッシュから有用的な三次元幾何情報を抽出するネットワークSurface Networkを提案した．従来のLaplace operatorがintrinsic三次元幾何情報しか抽出できない．しかし，様々な応用場面でextrinsic情報が必要となる．この文章で主要なcurvature方向を抽出できるDirac operator を提案し，従来のLaplace operatorより幅広い場面で応用できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SurfaceNetwork_result.png" alt="SurfaceNetwork_result"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・定性的および定性的な結果によりspatial-temporal predictionsタスクにおいて，従来手法より良い結果を得られている．・variationalエンコーダーを用いたメッシュ合成手法を提案し，有効的に3次元メッシュを生成できる．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.10819">論文</a></li></ul></div></div><div class="slide_index">[#218]</div><div class="timestamp">2018.4.13 11:16:55</div></div></section><section id="SPLATNet_Sparse_Lattice_Networks_for_Point_Cloud_Processing"><div class="paper-abstract"><div class="title">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</div><div class="info"><div class="authors">Hang Su, University of Massachusetts, Amherst; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Evangelos Kalogerakis, UMass; Subhransu Maji, ; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>点群情報を直接処理できるSPLATNet（右図）を提案した．SPLATNetは直接点群から階層的な空間情報を抽出可能．また，2D情報と3D情報のマッピングも行えるので，点群とマルチ画像の両方をSPLATNetで処理可能．従来の直接点群情報を処理するネットワークはより局所的な空間情報を損失してしまう問題点がある．提案手法はこの問題を解決するために，BCLs層を用いた． BCLs層は点群をスパースなlatticeにマッピングし，さらにそのスパースなlatticeを畳み込みできる．それにより， unordered点群情報を処理できる上に点群のより局所的な情報も抽出可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SPLATNET.png" alt="SPLATNET"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Façade segmentationタスクにおいて，点群とマルチ画像のラベリングに良い処理スピードと従来手法手法より優れた精度を得られた．ShapeNet part segmentationにおいて従来手法より優れた精度（クラスmIoU：83.7%）を得られた．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.08275">論文</a></li></ul></div></div><div class="slide_index">[#219]</div><div class="timestamp">2018.4.13 10:29:26</div></div></section><section id="From_Lifestyle_Vlogs_to_Everyday_Interactions"><div class="paper-abstract"><div class="title">From Lifestyle Vlogs to Everyday Interactions</div><div class="info"><div class="authors">Fouhey et al.</div><div class="conference">CVPR 2018.</div><div class="paper_id">arXiv ID: 1712.02310</div></div><div class="item1"><h1>概要</h1><div class="text">従来のデータ取集手法（collection-by-acting）では難しいかった, バイアスの少ない, 多様で大規模な日常生活におけるインタラクションのデータベース Lifestyle VLOG dataset を公開した. </div></div><div class="item2"><img src="slides/figs/fukuhara-From-Lifestyle-Vlogs-to-Everyday-Interactions.png"></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>従来のデータセットが想定している陽的なデータ収集とは対照的に隠的なデータ収集方法を行うことで, バイアスを小さくすることに成功した.</li><li>ビデオに対してインタラクションのラベル, フレームに対してインタラクション時の手の状態のラベル付けられている.</li><li>従来のデータセットのBiasを分析するために, 従来のデータセットで訓練した手法が Lifestyle VLOG データセットに対しても上手く動作するか検証した.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02310.pdf" target="blank">[論文] From Lifestyle Vlogs to Everyday Interactions</a></li><li><a href="https://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/index.html" target="blank">Project Page</a></li><li><a href="https://github.com/dfouhey/VLOGToolkit" target="blank">GitHub</a></li></ul></div></div><div class="slide_index">[#220]</div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="timestamp">2018.4.12.00:00:00</div></div></section><section id="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching"><div class="paper-abstract"><div class="title">Seeing Voices and Hearing Faces: Cross-modal biometric matching</div><div class="info"><div class="authors">A. Nagrani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>ある音声と2人分の顔画像から，どちらの人物の声かを推定する課題と，ある顔画像と2人分の音声から，どちらの音声がその人物の声かを推定する課題の2つを解くという問題設定の研究．
異なるモダリティ間でのマッチングという課題ということ．
ある入力に対応するのがどちらの人物かという2クラス識別の問題設定として定式化．
この問題を解くために，3入力を扱う3-streamのネットワーク構造を持つモデルを提案．
音声もスペクトログラムの形式で画像のように扱い，顔画像，音声ともにConvolutionしていくモデル．
実験では80%程度の識別率を達成し，人と同等の結果が出ている．
二人分の選択肢の性別，国籍，年齢などが同じという設定にすると，60%程度の正答率になるが，こちらでは人 (57%) を上回る結果となっている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png" alt="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>人物の顔画像と音声の対応付けという新しい問題設定</li><li>人間レベルの高い精度を実現</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00326">論文 (arXiv)</a></li></ul></div></div><div class="slide_index">[#221]</div><div class="timestamp">2018.4.12 15:48:11</div></div></section><section id="Actor_and_Action_Video_Segmentation_from_a_Sentence"><div class="paper-abstract"><div class="title">Actor and Action Video Segmentation from a Sentence</div><div class="info"><div class="authors">Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>センテンスの入力から、行動者と行動（Actor and Action）を同時に特定する研究である。複数の同様の物体から特定の人物など、詳細な分類が必要になる。ここではFully-Convolutional（構造の全てが畳み込みで構成される）モデルを適用してセグメンテーションベースで出力を行うモデルを提案。図は提案モデルを示す。I3Dにより動画像のエンコーディング、自然言語側はWord2Vecの特徴をさらにCNNによりエンコーディング。その後、動画像・言語特徴を統合してDeconvを繰り返しセグメントを獲得していく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803ActorAction.png" alt="1803ActorAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>文章（と動画像）の入力から行動者と行動の位置を特定すべくセグメンテーションを実行するという問題を提起した。また、二つの有名なデータセット（A2D/J-HMDB）を拡張して7,500を超える自然言語表現を含むデータとした。同問題に対してはSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CVxNLPの問題はここにも進出して来た。画像キャプションに限らず、この手の統合は進められるはず。</p><ul><li><a href="https://arxiv.org/pdf/1803.07485.pdf">論文</a></li></ul></div></div><div class="slide_index">[#222]</div><div class="timestamp">2018.3.24 12:47:10</div></div></section><section id="Alive_Caricature_from_2D_to_3D"><div class="paper-abstract"><div class="title">Alive Caricature from 2D to 3D</div><div class="info"><div class="authors">Qianyi Wu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>2Dの似顔絵画像から3Dの似顔絵を作成するためのアルゴリズムの提案。似顔絵画像のテストデータとしてはカリカチュアを使用し、カリカチュア画像の3Dモデルとテクスチャ化された画像を生成する。データは、標準の3D顔の変形を座標系に配置(下図、 xは口の開き具合)し、金のオリジナルデータから線形結合によって白い顔を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>カリカチュアを集めたデータセットを作って学習するのではなく、標準の3D顔のデータセットから実装でき、アプリケーションの柔軟さを推している。</p><p>3DMMやFaceWareHouseなどの従来手法と比較して、形の歪みが少なく、従来のものよりも綺麗な3D顔の出力が可能。顔以外にも、概形の予測が可能なオブジェクトなら応用できる？</p><ul><li><a href="https://arxiv.org/pdf/1803.06802.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_2.jpg"></p></div></div><div class="slide_index">[#223]</div></div></section><section id="A_Minimalist_Approach_to_Type-Agnostic_Detection_of_Quadrics_in_Point_Clouds"><div class="paper-abstract"><div class="title">A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds</div><div class="info"><div class="authors">Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンが発生している場合/複雑な環境下でも簡単な形状がポイントクラウドから検出できる枠組みを提案する。手法は3D楕円形状のフィッティング、3次元空間操作、4点取得により構成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324Quadrics.png" alt="180324Quadrics"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>タイプに依存しない3次元の二次曲面（楕円球形状）検出を点群の入力から行う手法を考案した。さらに、4点探索問題を3点探索にしてRANSACベースの手法で解を求めた。モデルベースのアプローチよりはフィッティングの性能がよいが、キーポイントベースの手法よりは劣る。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>曖昧な教示のみで3次元形状探索問題が解決できるようになる？</p><ul><li><a href="https://arxiv.org/pdf/1803.07191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#224]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section id="COCO-Stuff_Thing_and_Stuff_Classes_in_Context"><div class="paper-abstract"><div class="title">COCO-Stuff: Thing and Stuff Classes in Context</div><div class="info"><div class="authors">Holger Caesar, Jasper Uijlings, Vittorio Ferrari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>MSCOCOデータセットに対してThing（もの）やStuff（材質）に関する追加アノテーションを行い、さらにコンテキスト情報も追加したCOCO-Stuffを提案した。このデータセットには主にシーンタイプ、そのものがどこに現れそうかという場所、物理的/材質的な属性などをアノテーションとして付与する。COCO2017をベースにして164Kに対して91カテゴリを付与し、スーパーピクセルを用いた効率的なアノテーションについてもトライした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329COCOStuff.png" alt="180329COCOStuff"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>材質的なアノテーションは画像キャプションに対して重要であることを確認、相対的な位置関係などデータセットのリッチなアノテーションが重要であること、セマンティックセグメンテーションベースの方法により今回のアノテーションを簡易的に行えたこと、などを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>さらにリッチなアノテーションは今後重要になる。この論文ではスーパーピクセルという弱い知識を用い、人間のアノテーションと組み合わせることでボトムアップ・トップダウンを効果的かつ効率的に組み合わせてアノテーションを行っている点が素晴らしい。ラストオーサのVittorio Ferrariは機械と人の協調によるアノテーションが得意（なので、既存データセットへのよりリッチなアノテーションを早いペースで提案できる）。</p><ul><li><a href="https://arxiv.org/pdf/1612.03716v4.pdf">論文</a></li><li><a href="https://github.com/nightrome/cocostuff10k">GitHub</a></li></ul></div></div><div class="slide_index">[#225]</div><div class="timestamp">2018.3.29 13:59:43</div></div></section><section id="Context-aware_Synthesis_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">Context-aware Synthesis for Video Frame Interpolation</div><div class="info"><div class="authors">Simon Niklaus, Feng Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>入力フレームだけでなく、ピクセル単位の文脈情報を用いて、高品質の中間フレームを補間するためのコンテキスト認識手法の提案。まず、プレトレインモデルを使用して、入力フレームのピクセルごとのコンテキスト情報を抽出。オプティカルフローを使用して、双方向フローを推定し、入力フレームとそのコンテキストマップの両方をワープする。最後にコンテキストマップをsynthesis networkに入力し、補間フレームを生成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401CaS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来のビデオフレーム補間アルゴリズムは、オプティカルフローまたはその変動を推定し、それを用いて2つのフレーム間の中間フレームを生成する。本手法では、 2つの入力フレーム間の双方向フローを推定し、コンテキスト認識という方式をとることで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>高品質のビデオフレーム補間実験において、従来を上回る性能。</p><ul><li><a href="https://arxiv.org/pdf/1803.10967.pdf">論文</a></li></ul></div></div><div class="slide_index">[#226]</div></div></section><section id="Deep_Depth_Completion_of_a_Single_RGB-D_Image"><div class="paper-abstract"><div class="title">Deep Depth Completion of a Single RGB-D Image</div><div class="info"><div class="authors">Yinda Zhang, Thomas Funkhouser</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>RGB画像から表面の法線とオクルージョン境界を予測し、 RGB-D画像と組み合わせて、欠けている奥行き情報を補完するDeep Depth Completionの提案。また、奥行き画像と対になったRGB-D画像のデータセットであるcompletion benchmark datasetを作成し、性能を評価。これは、低コストのRGB-Dカメラでキャプチャした画像と、高コストの深度センサで同時にキャプチャした画像で構成されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DDC.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>深度カメラは、光沢があり、明るく、透明で、遠い表面の深さを感知しないことが多い。 このような問題を解決するために、本手法ではRGB画像から得た情報と組み合わせて、 RGB-D画像の深度チャネルを完全なものにする。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>深さ修復および推定において従来よりも優れた性能。</p><ul><li><a href="https://arxiv.org/abs/1803.09326">論文</a></li><li><a href="http://deepcompletion.cs.princeton.edu/">Project webpage</a></li></ul></div></div><div class="slide_index">[#227]</div></div></section><section id="Detecting_and_Recognizing_Human-Object_Interactions"><div class="paper-abstract"><div class="title">Detecting and Recognizing Human-Object Interactions</div><div class="info"><div class="authors">Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物検出と同時に人物行動やその物体とのインタラクションも含めて学習を行うモデルを提案する。本論文では物体候補の中でも特にインタラクションに関係ありそうな物体に特化して認識ができるようにする。さらに、検出された<human, verb, object>のペアを用いて学習する（図の場合には<human, cut, knnife>）。さらに、その他の行動（図の場合にはstand）を同時に推定することもできる。モデルはFaster R-CNNをベースとするが、物体検出（box, class）、行動推定（action, target）、インタラクション（action）を推定して誤差を計算する。さらに、推定した人物位置に対する対象物体の方向も確率的に計算することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322HOI.png" alt="180322HOI"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>人間に特化した検出と行動推定の枠組みを提案した。V-COCO（Verbs in COCO）にて、相対的に26%精度が向上（31.8=>40.0）、HICO-DETデータセットにて27%相対的な精度向上が見られた。計算速度は135ms/imageであり、高速に計算が可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な多タスク学習ではなく、人物に特化して対象物体の位置も確率的に推定しているところがGood。</p><ul><li><a href="https://arxiv.org/pdf/1704.07333.pdf">論文</a></li><li><a href="https://gkioxari.github.io/InteractNet/index.html">Project</a></li><li><a href="https://github.com/s-gupta/v-coco">Verbs in COCO DB</a></li></ul></div></div><div class="slide_index">[#228]</div><div class="timestamp">2018.3.22 19:55:34</div></div></section><section id="Discriminative_Learning_of_Latent_Features_for_Zero-Shot_Recognition"><div class="paper-abstract"><div class="title">Discriminative Learning of Latent Features for Zero-Shot Recognition</div><div class="info"><div class="authors">Minghui Yan Li, et al</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Zero-shot learning(ZSL)における、視覚的および意味的インスタンスを別々に表現し学習するLatent Discriminative Features Learning(LDF)の提案。 (1)ズームネットワークにより差別的な領域を自動的に発見することができるネットワークの提案。(2)ユーザによって定義された属性と潜在属性の両方について、拡張空間における弁別的意味表現の学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330LDF.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ZSLは、画像表現と意味表現の間の空間を学習することによって、見えない画像カテゴリを認識する。 既存の手法では、視覚と意味空間を合わせたマッピングマトリックスを学習することが中心的課題。提案手法では、差別的に学習するとうアプローチで識別精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2つのコンポーネントによって、互いに支援しながら学習することで最先端の精度に。</p><ul><li><a href="https://arxiv.org/pdf/1803.06731.pdf">論文</a></li></ul></div></div><div class="slide_index">[#229]</div></div></section><section id="Domain_Adaptive_Faster_R-CNN_for_Object_Detection_in_the_Wild"><div class="paper-abstract"><div class="title">Domain Adaptive Faster R-CNN for Object Detection in the Wild</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ドメイン変換について、ゲームなどのCG映像から実際の交通シーンに対応して物体検出を行うための学習方法を提案する。本論文では(i) 画像レベルのドメイン変換、(ii) インスタンス（ある物体）に対してのドメイン変換、の二種類の方法を提案し、整合性をとるように正規化する（図のConsistency Regularization; Global/Localな特徴変換を考慮）。ここで、物体検出はFaster R-CNNをベースとしてドメイン変換の手法も二種類（H-divergence、敵対的学習）用意する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314DomainFRCNN.png" alt="180314DomainFRCNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>CGで学習し実環境における自動運転などで使えるドメイン変換の手法を提案した。実験はCityscapes, KITTI, SIM10Kなどで行い、ロバストな物体検出を実行することができた。例えばCityscapesとKITTIの相互ドメイン変換でベースラインのFaster R-CNNが30.2 (K->C)、53.5 (C->K)のところ、Domain Adaptive Faster R-CNNでは38.5 (K->C)、64.1 (C->K)であった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データ収集は手動から自動の時代になって来た？データを手作業で集める時代からアルゴリズムを駆使して収集する時代へ移行。</p><ul><li><a href="https://arxiv.org/pdf/1803.03243.pdf">論文</a></li><li><a href="http://www.vision.ee.ethz.ch/~liwenw/">著者</a></li></ul></div></div><div class="slide_index">[#230]</div><div class="timestamp">2018.3.14 08:43:53</div></div></section><section id="Efficient_Interactive_Annotation_of_Segmentation_Datasets_with_Polygon-RNN"><div class="paper-abstract"><div class="title">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</div><div class="info"><div class="authors">David Acuna, Huan Ling, Amlan Kar, Sanja Fidler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Polygon-RNNのアイデアを踏襲し、ヒューマン・イン・ザ・ループを使って対話的にオブジェクトのポリゴンアノテーションの生成。また、新しいCNNエンコーダアーキテクチャの設計、強化学習によるモデルの効果的な学習、 Graph Neural Networkを使用した出力解像度の向上を行う。これらのアーキテクチャをPolygon-RNN ++と呼ぶ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>アノテーション作成時の負担を軽減。より正確にアノテーションを付加できるため、雑音の多いアノテーターに対しても頑健である。</p><p>高い汎化能力となり、既存のピクセルワイズメソッドよりも大幅に改善。ドメイン外のデータセットにも適応可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.09693.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_2.jpg"></p></div></div><div class="slide_index">[#231]</div></div></section><section id="Egocentric_Basketball_Motion_Planning_from_a_Single_First-Person_Image"><div class="paper-abstract"><div class="title">Egocentric Basketball Motion Planning from a Single First-Person Image</div><div class="info"><div class="authors">Gedas Bertasius, Aaron Chan, Jianbo Shi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180307EgoBasketball.png" alt="180307EgoBasketball"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。</p><ul><li><a href="https://arxiv.org/pdf/1803.01413v1.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=wRRRl4QsUQg">YouTube</a></li></ul></div></div><div class="slide_index">[#232]</div><div class="timestamp">2018.3.7 09:04:15</div></div></section><section id="Fast_and_Accurate_Single_Image_Super-Resolution_via_Information_Distillation_Network"><div class="paper-abstract"><div class="title">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</div><div class="info"><div class="authors">Zheng Hui, Xiumei Wang, Xinbo Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>元の低解像度画像から高解像度画像を再構築するための、深くてコンパクトなCNNを提案。提案モデルは、特徴抽出ブロック、積み重ね情報蒸留ブロック、再構成ブロックの3部構成。これにより、情報量が豊富かつ効率的に特徴を徐々に抽出できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331FaASISR.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNが超解像殿画像を扱うようになってきたが、ネットワークが増大するにつれて、計算上の複雑さとメモリ消費という問題が生じる。これらの問題を解決するためのコンパクトなCNN。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PSNR、SSIM、IFCの4つのデータセットで検証し、精度向上を確認。デシジョンおよび圧縮アーチファクト低減などの他の画像修復問題にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li></ul></div></div><div class="slide_index">[#233]</div></div></section><section id="Future_Frame_Prediction_for_Anomaly_Detection_--_A_New_Baseline"><div class="paper-abstract"><div class="title">Future Frame Prediction for Anomaly Detection -- A New Baseline</div><div class="info"><div class="authors">Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>先の（未来の）フレーム予測と異常検知を同時に行う手法を提案する論文。予測したフレームと異常検知の正解値により誤差を計算して最適化を行う。図に本論文で提案するネットワークアーキテクチャの図を示す。U-Netにより画像予測やさらにオプティカルフロー推定を行い、RGB空間、オプティカルフロー空間にて誤差を計算しGANの枠組みでそれらがリアルかフェイクかを判定する。同フレームを用いて異常検知を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180315PredictionAnomaly.png" alt="180315PredictionAnomaly"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来は現在フレームを入力として異常検知を行う手法は存在したが、未来フレームを予測して異常検知を行う枠組みは本論文による初めての試みである。異常値の正解値を与えることで画像予測にもフィードバックされるため、画像予測と異常検知の相互学習に良い影響を与える。オープンデータベースにてベンチマークした結果、何れもState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成ベースで画像予測+X（Xは任意タスク）というものはSoTAが出せるくらいにはなってきた。</p><ul><li><a href="https://arxiv.org/pdf/1712.09867.pdf">論文</a></li><li><a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018">Project</a></li></ul></div></div><div class="slide_index">[#234]</div><div class="timestamp">2018.3.15 09:04:03</div></div></section><section id="Guided_Labeling_using_Convolutional_Neural_Networks"><div class="paper-abstract"><div class="title">Guided Labeling using Convolutional Neural Networks</div><div class="info"><div class="authors">Sebastian Stabinger, et al. </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルの付いていないデータに対して、どの画像にラベルを付けてデータセットを構成すればよいかを判断するguided labelingの提案。ラベル付けを行う必要があるサンプルを見定めることで、データセットの量を大幅に減らすことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313GuidedLabeling.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>大規模データセットにおいて、手動でのラベル付けは大変。選別してラベル付けを行えば、作業を最小限に抑えられる。また、ある意味良いデータを選別できるため、場合によっては精度も向上。</p></div></div><div class="item4"><div class="text"><p>MNISTは、データセットのサイズを1/16に、CIFAR10は1/2に減らすことが可能に。また、MNISTの場合は、全部使った時よりも識別精度が向上した。普遍性を妨げる不必要なデータを取り除けたことが精度向上につながった？</p><ul><li><a href="https://arxiv.org/pdf/1712.02154.pdf">論文</a></li></ul></div></div><div class="slide_index">[#235]</div></div></section><section id="HATS_Histograms_of_Averaged_Time_Surfaces_for_Robust_Event-based_Object_Classification"><div class="paper-abstract"><div class="title">HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification</div><div class="info"><div class="authors">Amos Sironi, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>イベントベースカメラにおける、識別アルゴリズムの提案。本研究では、(1)イベントベースのオブジェクト分類のための低レベル表現とアーキテクチャの欠如、(2)実世界における大きなイベントベースのデータセットの欠如、の2つの問題に取り組む。新しい機械学習アーキテクチャ、イベントベースの特徴表現(Histograms of Averaged Time Surfaces)、データセット(N-CARS)を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330NCARS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>イベントベースのカメラは、従来のフレームベースのカメラと比較して、高時間分解能、低消費電力、高ダイナミックレンジという点で優れており、様々なシーンで応用が利く。しかし、イベントベースのオブジェクト分類アルゴリズムの精度は未だ低い。特徴表現には過去時間の情報を使用。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>過去の情報を使うことで、既存のイベントベースカメラによる認識手法よりも優れた結果となった。</p><ul><li><a href="https://arxiv.org/pdf/1803.07913.pdf">論文</a></li><li><a href="http://www.prophesee.ai/dataset-n-cars/">データセット</a></li></ul></div></div><div class="slide_index">[#236]</div></div></section><section id="Improving_Object_Localization_with_Fitness_NMS_and_Bounded_IoU_Loss"><div class="paper-abstract"><div class="title">Improving Object Localization with Fitness NMS and Bounded IoU Loss</div><div class="info"><div class="authors">Lachlan Tychsen-Smith, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のNon-Max Supressionを改良したFitness NMSの提案。Soft NMSも同時に使用するとより効果的。</p><p>勾配降下法の収束特性(滑らかさ、堅牢性など)を維持しつつ、IoUを最大化するという目標により適した損失関数であるBounded IoU Loss の提案。これをRoIクラスタリングと組み合わせることで精度が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314FitnessNMS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>バウンディングボックスのスコアを算出する関数を拡張する。具体的には、グランドトゥルースとのIoUと、クラスの期待値を追加する。これにより、IoUの重なり推定値と、クラス確率の両方が高いバウンディングボックスを優先して学習することができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>MSCOCO、Titan X(Maxwell)使用時では、精度33.6％-79Hzまたは41.8％-5Hz。本論文ではDeNetでテストしたが、別の手法でも精度向上が望めるよう。</p><ul><li><a href="https://arxiv.org/pdf/1711.00164.pdf">論文</a></li><li><a href="https://github.com/lachlants/denet">ソースコード</a></li></ul></div></div><div class="slide_index">[#237]</div></div></section><section id="Independently_Recurrent_Neural_Network_IndRNN_Building_A_Longer_and_Deeper_RNN"><div class="paper-abstract"><div class="title">Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</div><div class="info"><div class="authors">Shuai Li, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>新しいRNN手法であるindependently recurrent neural network (IndRNN)の提案。一枚のレイヤ内のニューロンが独立しており、レイヤ間で接続されている。これにより、勾配消失問題や爆発問題を防ぎ、より長期的なデータを学習することができる。また、IndRNNは複数積み重ねることができるため、既存のRNNよりも深いネットワークを構築できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314IndRNN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法によって下記の従来手法の問題を解決。</p><p>RNNは、勾配の消失や爆発の問題、長期パターンの学習が困難である。LSTMやGRUは、上記のRNNの問題を解決すべく開発されたが、層の勾配が減衰してしまう問題がある。また、RNNは全てのニューロンが接続されているため、挙動の解釈が困難。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>かなり長いシーケンス(5000回以上の時間ステップ)を処理でき、かなり深いネットワーク（実験では21レイヤー）を構築できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.04831.pdf">論文</a></li></ul></div></div><div class="slide_index">[#238]</div></div></section><section id="Iterative_Visual_Reasoning_Beyond_Convolutions"><div class="paper-abstract"><div class="title">Iterative Visual Reasoning Beyond Convolutions</div><div class="info"><div class="authors">Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNのような理由を突き止める能力がない認識システムを超えた、反復的なvisual reasoningのための新しいフレームワークの提案。畳み込みベースのローカルモジュールとグラフベースのグローバルモジュールの2コアで構成。2つのモジュールのを繰返し展開し、予測結果を相互にクロスフィードして絞り込む。最後に、両方のモジュールの最高値をアテンションベースのモジュールと組み合わせてプレディクト。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401IVRBC_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>ただ畳み込むだけでなく、Spatial(空間的)およびSemanticの空間を探索することができる。下図のように、「人」は「車」を運転するというSpatialとSemanticの双方を兼ね備えた認識を行うことで精度向上を図る。</p><p>通常のCNNと比較して、ADEで8.4％、COCOで3.7％の精度向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.11189.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401IVRBC_2.jpg"></p></div></div><div class="slide_index">[#239]</div></div></section><section id="LayoutNet_Reconstructing_the_3D_Room_Layout_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</div><div class="info"><div class="authors">Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単一のパースペクティブまたはパノラマ画像から屋内3Dルームレイアウトを推定するLayoutNetの提案。最初に、消失点を分析し、水平になるように画像を整列。これにより、壁と壁の境界が垂直になり、ノイズ低減。画像からコーナー(レイアウト接合点)と境界を、エンコーダ/デコーダ構造のCNNで出力。最後に、3D Layoutパラメータを、予測したコーナーと境界に適合するように最適化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401LayoutNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>アーキテクチャはRoomNetと似ているが、消失点に基づいて画像を整列させ、複数のレイアウト要素（コーナー、境界線、サイズ、平行移動）を予測し、 “L”形の部屋のような非直方体のマンハッタンレイアウトに対しても適応できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>従来手法と比較して、処理速度と正確さにおいて性能の向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.08999.pdf">論文</a></li><li><a href="https://github.com/zouchuhang/LayoutNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#240]</div></div></section><section id="Learning_to_Localize_Sound_Source_in_Visual_Scenes"><div class="paper-abstract"><div class="title">Learning to Localize Sound Source in Visual Scenes</div><div class="info"><div class="authors">Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像と音声の入力から、音が画像のどこで鳴っているか（鳴りそうか？）を推定した研究。さらに、人の声なら人の領域、車の音なら車の領域にアテンションがあたるなど物体と音声の対応関係も学習することができる。学習には音源とその対応する物体の位置を対応づけたデータセット（144Kのペアが含まれるSound Source Localization Dataset）を準備した。さらに既存の物体認識と音声を対応づけて（？）Unsupervised/Semi-supervisedに学習することにも成功した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322LocalizeSound.png" alt="180322LocalizeSound"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>教師あり、教師なし、半教師あり、いずれの枠組みでも音声ー物体の対応関係を学習することができるようにした。音源とそれに対応する物体領域の尤度がヒートマップにて高く表示されている。結果はビデオを参照されたい。教師なし学習はTriplet-lossにより構成され、ビデオと近い/遠い音声の誤差により計算。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常に面白い問題設定、プラス誤差関数を柔軟に抽出可能というところが上手。精読しても良いと感じた論文。CVにおいてビデオの音声は今まで使用しないことも多かった（もしくは精度向上のために活用していた）が、これからは使用方法を見直してもよいと感じた。</p><ul><li><a href="https://arxiv.org/pdf/1803.03849.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=UyairkbzR_Y">YouTube</a></li></ul></div></div><div class="slide_index">[#241]</div><div class="timestamp">2018.3.22 19:18:32</div></div></section><section id="Learning_to_Segment_Every_Thing"><div class="paper-abstract"><div class="title">Learning to Segment Every Thing</div><div class="info"><div class="authors">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180303SegmentEverything.png" alt="180303SegmentEverything"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）</p><ul><li><a href="https://arxiv.org/pdf/1711.10370.pdf">論文</a></li><li><a href="http://ronghanghu.com/">著者</a></li><li><a href="http://kaiminghe.com/">Kaiming He</a></li></ul></div></div><div class="slide_index">[#242]</div><div class="timestamp">2018.3.3 10:46:40</div></div></section><section id="MakeupGAN_Makeup_Transfer_via_Cycle-Consistent_Adversarial_Networks"><div class="paper-abstract"><div class="title">MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks</div><div class="info"><div class="authors">Huiwen Chang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ソース画像のメイクをターゲット画像へ転写やメイクの除去をする研究。ターゲット画像とメイク済み画像の2枚を入力としメイクを転写するネットワークGとメイク済み画像らメイクを取り除くネットワークFを考え、2つのネットワークによって元の画像に戻るように学習していく。
その際、Fによってxに付与されたメイクがyのメイクと同じものであるかを評価するロスを加えることでメイクの特徴を捉える。
従来手法ではメイク転写・除去を独立した問題として考えていたが、この研究ではセットとして考えている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408make.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Youtubeのメイクチュートリアルの動画から、1148枚のメイクなし画像と1044枚のメイクあり画像を収集。ユーザースタディによって2つの既存手法と比較し、提案手法が一番いいと答えた人が65.7％（2番目と答えた人が31.4％）
従来手法では肌の色や表情の違いがあると上手くいかないのに対し、ソースとターゲット間でこれらが違ってもうまく転写できる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://research.adobe.com/project/makeupgan-makeup-transfer-via-cycle-consistent-adversarial-networks/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#243]</div><div class="timestamp">2018.4.8 01:45:44</div></div></section><section id="Motion-Appearance_Co-Memory_Networks_for_Video_Question_Answering"><div class="paper-abstract"><div class="title">Motion-Appearance Co-Memory Networks for Video Question Answering</div><div class="info"><div class="authors">Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ビデオQAのための、 Dynamic Memory Network(DMN) のコンセプトに基づいたmotion-appearance comemory networkの提案。本研究の特徴は次の3つである。(1)アテンションを生成するために動きと外観情報の両方を手がかりとして利用する共メモリアテンションメカニズム。(2) multi-level contextual factを生成するための時間的conv-deconv network。(3)異なる質問に対して動的な時間表現を構成するdynamic fact ensemble method。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401MACoMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法は、次のようなvideo QA特有の属性に基づいている。(1)豊富な情報を含む長い画像シーケンスを扱う。(2)動き情報と出現情報を相互に関連付け、アテンションキューを他の情報に応用できる。(3)答えを推論するために必要なフレーム数は質問によって異なる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>TGIF-QAの4つのタスクすべてにおいて、最先端技術よりも優れている。</p><ul><li><a href="https://arxiv.org/pdf/1803.10906.pdf">論文</a></li></ul></div></div><div class="slide_index">[#244]</div></div></section><section id="Multi-Frame_Quality_Enhancement_for_Compressed_Video"><div class="paper-abstract"><div class="title">Multi-Frame Quality Enhancement for Compressed Video</div><div class="info"><div class="authors">Ren Yang, Mai Xu, Zulin Wang, Tianyi Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>圧縮した動画像に対して画質を向上させる研究。Peak Quality Frames (PQFs)を用いたSVMベースの手法やMulti-Frame CNN (MF-CNN)を提案。提案法により、圧縮動画における連続フレームからアーティファクトを補正するような働きが見られた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324PQF.png" alt="180324PQF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>動画の画質改善手法においてState-of-the-art。動画に対する画質改善の結果は図を参照。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04680">論文</a></li><li><a href="https://github.com/ryangBUAA/MFQE">GitHub</a></li></ul></div></div><div class="slide_index">[#245]</div><div class="timestamp">2018.3.24 15:14:35</div></div></section><section id="Multi-Level_Factorisation_Net_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Multi-Level Factorisation Net for Person Re-Identification</div><div class="info"><div class="authors">Xiaobin Chang, Timothy M. Hospedales, Tao Xiang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の視覚的外観を、人の手によるアノテーションなしかつ、複数のセマンティックレベルで識別因子に分解する Multi-Level Factorisation Net(MLFN)の提案。 MLFNは、複数のブロックで構成されており、各ブロックには、複数の因子モジュールと、各入力画像の内容を解釈するための因子選択モジュールが含まれている。 </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331MLFN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>効果的なRe-IDを目指すには、高低のセマンティックレベルでの人の差別化かつ視界不変性をモデル化することである。 近年(2018)のdeep Re-IDモデルは、セマンティックレベルの特徴表現を学習するか、アノテーション付きデータが必要となる。MLFNではこれらを改善する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのRe-IDと、CIFAR-100の結果で最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.09132.pdf">論文</a></li></ul></div></div><div class="slide_index">[#246]</div></div></section><section id="Non-local_Neural_Networks"><div class="paper-abstract"><div class="title">Non-local Neural Networks </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="item1"><h1>概要</h1><div class="text">NLPなどで効果を発揮しているself-attentionを多次元に一般化し、2D/3DCNNに導入することで新たな「non-local block」を形成し、画像や動画での実験を行った。
行動認識＠Kineticsでは非常に高い精度を達成。Instance segmentationやkey point detectionなどのタスクでも汎用的に効果を発揮。
</div></div><div class="item2"><img src="slides/figs/non_local.png"></div><div class="item3"><h1>手法</h1><div class="text">位置jと位置iに依存してアテンションを出力する関数f(.)とjのみに依存する関数g(.)の積を入力位置jに関して和をとることによって位置iの出力値を決定する。
位置情報の保存、可変入力サイズ、などの性質を持ち、全結合、畳み込みを特殊な形として含む。またf(.)の定義の仕方によってはself-attentionと一致する。
f(.)は様々な形が提案されているが、種類によらず効果を発揮している。実際に使用する場合は図のような残差構造を使用している。

</div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><p>効果のインパクトがすごい。学習曲線からもうまくいっていることが明らか。C2Dに対してspace-timeにnon-local blockを適用すると3Dconvよりも時系列方向への拡大として効果があったのが興味深い。
結局残差を用いたnon-local blockを使用していたので、単純にnon-local layerのみでの性能もきになる。
位置情報の保存は重要でも、局所性はあまり重要ではなかったのかと感じられる。</p><ul><li><a href="https://arxiv.org/abs/1711.07971">論文</a></li></ul></div></div><div class="slide_index">[#247]</div><div class="slide_editor">Tomoyuki Suzuki</div></div></section><section id="Pose-Robust_Face_Recognition_via_Deep_Residual_Equivariant_Mapping"><div class="paper-abstract"><div class="title">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</div><div class="info"><div class="authors">Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>横顔の認識精度を高めるためにDeep Residual EquivAriant Mapping (DREAM)の提案。正面と側面の顔間のマッピングを行うことで特徴空間を対応付ける。これにより、横顔を正面の姿勢に変換して認識を単純化。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313DREAM_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・手法・リンク集</h1><p>正面と側面のトレーニング数の不均衡から、現代の顔認識モデルの多くは、正面と比べて横顔を処理するのが比較的貧弱。本手法は姿勢変動を伴う顔認識に限定されない顔認識が可能で、横顔のデータを増やさなくても精度向上。</p><p>上図より、DREAMをCNNに追加し、入力に残差を動的に追加。下図はマッピングによる姿勢変換の例。</p><ul><li><a href="https://arxiv.org/pdf/1803.00839.pdf">論文</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DREAM">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180313DREAM_2.jpg"></p></div></div><div class="slide_index">[#248]</div></div></section><section id="Pyramid_Stereo_Matching_Network"><div class="paper-abstract"><div class="title">Pyramid Stereo Matching Network</div><div class="info"><div class="authors">Jia-Ren Chang, Yong-Sheng Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>空間ピラミッドプーリングと3D CNNの2つのモジュールから構成された、ステレオ画像対からの奥行き推定を行うPyramid Stereo Matching Network(PSMNet)の提案。空間ピラミッドプーリングは、異なるスケールおよび位置でコンテキストを集約し、コストボリュームを形成する。 3D CNNは、複数のhourglass networksを重ねて、コストボリュームを規則化することを学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330PSMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>現在(2018)ではステレオ画像からの奥行き推定を、CNNの教師あり学習で解決されてきている。 コンテキスト情報を利用することで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>最先端の手法よりも優れている結果。</p><ul><li><a href="https://arxiv.org/pdf/1803.08669.pdf">論文</a></li><li><a href="https: //github.com/JiaRenChang/PSMNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#249]</div></div></section><section id="Referring_Relationships"><div class="paper-abstract"><div class="title">Referring Relationships</div><div class="info"><div class="authors">Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>referring relationshipsを利用して同カテゴリのエンティティ間の曖昧さを解消するタスクの提案。特徴抽出後、アテンションを生成。述語を使用することで、アテンションをシフトさせる。この述語シフトモジュールを介して、subjectとobjectの間でメッセージを反復的に渡すことで、2つのエンティティをローカライズ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>画像中のエンティティ間の関係にはそれぞれ意味があり、画像の理解に役立つ。例えば、図のサッカーの試合の画像では、複数の人写っているが、それぞれは異なる関係を持っている。一人はボールを蹴っており、もう一人はゴールを守っている。 <person-kicking-ball>に着目すると、述語の”kick”を理解することにより、画像内のどの人物が”ball”を蹴っているのかを正しく識別する。</p><ul><li><a href="https://arxiv.org/pdf/1803.10362.pdf">論文</a></li><li><a href="https://github.com/StanfordVL/ReferringRelationships">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_2.jpg"></p></div></div><div class="slide_index">[#250]</div></div></section><section id="Rethinking_Feature_Distribution_for_Loss_Functions_in_Image_Classification"><div class="paper-abstract"><div class="title">Rethinking Feature Distribution for Loss Functions in Image Classification</div><div class="info"><div class="authors">Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本論文ではLarge-margin Gaussian Mixture (L-GM) Lossを提案して画像識別タスクに応用する。Softmax Lossとの違いは、学習セットにおけるディープ特徴の混合ガウス分布をフォローしつつ仮説を設定するところである。識別境界や尤度正則化においてL-GM Lossは非常に高いパフォーマンスを実現している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314LGM.png" alt="180314LGM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>L-GM Lossは画像識別においてSoftmax Lossよりも精度が高いことはもちろん、特徴分布を考慮するため例えばAdversarial Examples（摂動ノイズ）などにおいても対応できる。MNIST, CIFAR, ImageNet, LFWにおける識別や摂動ノイズを加えた実験においても良好な性能を確かめた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Softmax Lossよりも有意に精度向上が見られている。導入が簡単なら取り入れて精度向上したい。</p><ul><li><a href="https://arxiv.org/pdf/1803.02988.pdf">論文</a></li></ul></div></div><div class="slide_index">[#251]</div><div class="timestamp">2018.3.14 11:04:45</div></div></section><section id="Robust_Depth_Estimation_from_Auto_Bracketed_Images"><div class="paper-abstract"><div class="title">Robust Depth Estimation from Auto Bracketed Images</div><div class="info"><div class="authors">Sunghoon Im, Hae-Gon Jeon, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>HDRの画像の明るさを補正するためのブラケット撮影からの距離画像やカメラ姿勢を同時推定する手法を提案する論文。ブラケット撮影とは通常の露出撮影以外に意図的に「少し明るめの写真」と「少し暗めの写真」を同時に撮影。距離画像推定は幾何変換をResidual-flow Networkに統合したモデルにより行う。ここでは学習ベースのMulti-view stereo手法（Deep Multi-View Stereo; DMVS）を幾何推定（Structure-from-Small-Motion; SfSM）と組み合わせる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323BracketedImages.png" alt="180323BracketedImages"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>距離画像推定において、スマートフォンやDSLRカメラなど種々のデータセットにてSoTAな精度を達成。モバイル環境でも動作するような小さなネットワークと処理速度についても同時に実現した。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.07702">論文</a></li><li><a href="https://sunghoonim.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#252]</div><div class="timestamp">2018.3.23 19:11:04</div></div></section><section id="Rotation-Sensitive_Regression_for_Oriented_Scene_Text_Detection"><div class="paper-abstract"><div class="title">Rotation-Sensitive Regression for Oriented Scene Text Detection</div><div class="info"><div class="authors">Minghui Liao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然画像から文字を検出する。単なる検出ではなく、文字の方向を考慮したバウンディングボックスによる検出手法であるRotation-sensitive Regression Detector (RRD)の提案。回帰ブランチによって、畳み込みフィルタを回転させて回転感知特徴を抽出。分類ブランチによって、回転感性特徴をプーリングすることによって回転不変特徴を抽出。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329RRD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>文字をテーマにした研究では(1)テキストの向きを無視した分類方法と，(2)向きを考慮したバウンディングボックスによる回帰がある。従来研究では、両方のタスクの共有の特徴を使用していたが、互換性がなかったためにパフォーマンスが低下(図b)。そこで、異なる2つのネットワークから抽出した、異なる特性の特徴を分類および回帰することを提案(図d,e)。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ICDAR 2015、MSRA-TD500、RCTW-17およびCOCO-Textを含む3つのシーンテキストのデータセットで最先端のパフォーマンスを達成。向きがある一般物体検出にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.05265.pdf">論文</a></li></ul></div></div><div class="slide_index">[#253]</div></div></section><section id="SketchMate_Deep_Hashing_for_Million-Scale_Human_Sketch_Retrieval"><div class="paper-abstract"><div class="title">SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval</div><div class="info"><div class="authors">Peng Xu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチ検索のためのディープハッシングフレームワークの提案。3.8mの大規模スケッチデータセットを構築。CNNでスケッチの特徴抽出。RNNでペンストロークの時間情報をモデル化。CNN-RNNでエンコードすることで、スケッチ性質に対応した新しいhashing lossを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408SkechMate.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>従来のスケッチ認識タスクに従う代わりに、より困難な問題のスケッチハッシュ検索を行う。ネットワークをスケッチ認識のために再利用することもでき、どちらも高パフォーマンス。大規模なデータセットを利用することで、従来の文献ではあまり研究されていなかった、スケッチのユニークな特性を見出す。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.01401.pdf">論文</a></li><li><a href="http://sketchx.eecs.qmul.ac.uk/downloads/">ソースコード/データセット</a></li></ul></div></div><div class="slide_index">[#254]</div></div></section><section id="Style_Aggregated_Network_for_Facial_Landmark_Detection"><div class="paper-abstract"><div class="title">Style Aggregated Network for Facial Landmark Detection</div><div class="info"><div class="authors">Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang, University of Technology Sydney, The University of Sydney</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のランドマーク検出。顔そのもののばらつきの他に、グレースケールやカラー画像、明暗などの画像スタイルが変わっても同様に検出できるStyle Aggregated Network(SAN)の提案。まず、(1)入力画像をさまざまなスタイルに変換し、スタイルを集約し、(2)顔のランドマーク予測する。(2)は、元画像とスタイルを集約した特徴の両方を入力し、融合してカスケード式のヒートマップ予測を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SAN_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>結果・リンク集</h1><p>Flickr8kとFlickr30kを使った実験において、最先端モデルと同等かそれ以上の結果。より正確で、より多様なキャプション生成。</p><ul><li><a href="https://arxiv.org/pdf/1803.04108.pdf">論文</a></li><li><a href="https://github.com/D-X-Y/SAN">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330SAN_2.jpg"></p></div></div><div class="slide_index">[#255]</div></div></section><section id="The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric"><div class="paper-abstract"><div class="title">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</div><div class="info"><div class="authors">Richard Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2枚の画像の類似度を表す指標は数多く提案されているが、その類似度は必ずしも人間の知覚と一致していない。近年はDNNにより高次の特徴を得ることが可能となっており、人間の知覚に近づいている。
そこで、既存の類似度の評価尺度とDNNベースの類似度判定を比較することでDNNベースの手法がより人間の知覚に近い類似度を表現できることを確認した。
具体的には、ある画像を異なる方法で加工したもの2つを用意し、どちらが元の画像に近いかを人間とコンピュータ両方に判定させることで検証を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408perception.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>データセットとして、画像に様々な加工を施したデータを人間に類似度を評価してもらったものを作成。加工の例としては、ノイズの付与やオートエンコーダによる画像の復元などが挙げられる。
検証の結果、ＤＮＮベースの類似度の方が既存の尺度より人間の知覚に乗っ取ってることを示した。
また、DNNのネットワーク構造そのものは重要ではないことが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://richzhang.github.io/PerceptualSimilarity/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#256]</div><div class="timestamp">2018.4.8 01:36:55</div></div></section><section id="TOM-Net_Learning_Transparent_Object_Matting_from_a_Single_Image"><div class="paper-abstract"><div class="title">TOM-Net: Learning Transparent Object Matting from a Single Image</div><div class="info"><div class="authors">Guanying Chen, Kai Han, Kwan-Yee K. Wong</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>透明物体の切り抜き（Transparent Object Matting; TOM）と反射特性を推定することが可能なネットワークTOM-Netを提案する。TOM-Netにより、物体の反射特性を保存しながら他の画像にレンダリングして、同画像のテクスチャを反映させることができる。同問題を反射フローの推定問題と捉えてDNNのモデルを構築することで解決した。荒い部分は多階層のEncoder-Decorderで推定し、詳細な部分はResidualNetで調整する。この問題を解決するために、データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324TOMNet.png" alt="180324TOMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>178Kの画像を含むデータセットを構築した。同DBには876サンプル、14の透明物体、60種の背景を含む。透明物体の推定と反射特性のレンダリングはGitHubページを参照。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.04636.pdf">論文</a></li><li><a href="http://gychen.org/TOM-Net/">Project</a></li><li><a href="https://github.com/guanyingc/TOM-Net_Rendering">GitHub</a></li></ul></div></div><div class="slide_index">[#257]</div><div class="timestamp">2018.3.24 18:05:46</div></div></section><section id="Towards_Human-Machine_CooperationSelf-supervised_Sample_Mining_for_Object_Detection"><div class="paper-abstract"><div class="title">Towards Human-Machine Cooperation:Self-supervised Sample Mining for Object Detection</div><div class="info"><div class="authors">Keze Wang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出の課題を考慮し、既存のActive Learning(AL)の欠点を改善することを目的とした、Self-Supervised Sample Mining(SSM)の提案。ラベルなし、もしくは一部ラベルのないデータを使って学習することができる。交差検証後のスコアによってサンプルを選別。低い場合にはユーザによってアノテーション、高い場合にはそのままラベルとして採用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SSM.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のAL法では主に、単一の画像コンテクスト内でサンプル選択基準を定義し、大規模な物体検出において最適ではなく、頑強性および非実用的である。SSMによって、ユーザが必要な部分にだけ介入し、アノテーションの作業を軽減。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アノテーションが少ないデータセットにおいても最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.09867.pdf">論文</a></li></ul></div></div><div class="slide_index">[#258]</div></div></section><section id="Towards_Open-Set_Identity_Preserving_Face_Synthesis"><div class="paper-abstract"><div class="title">Towards Open-Set Identity Preserving Face Synthesis</div><div class="info"><div class="authors">Jianmin Bao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からidentityとattributesを別々に再構成する、GANに基づいたOpen-Set Identity Generating Adversarial Networkの提案。 face synthesis networkは、ポーズや感情、照明、背景などをキャプチャする属性ベクトルを抽出することができる。図中の2つの入力画像AおよびBから抽出された識別を再結合することによって、A0およびB0を生成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401OSIPFS_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>顔の正面化、顔属性モーフィング、 face adversarial example detectionなど、より広範なアプリケーションに応用可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.11182.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401OSIPFS_2.jpg"></p></div></div><div class="slide_index">[#259]</div></div></section><section id="Towards_Universal_Representation_for_Unseen_Action_Recognition"><div class="paper-abstract"><div class="title">Towards Universal Representation for Unseen Action Recognition</div><div class="info"><div class="authors">Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習画像がなくても行動認識を実現する「Unseen Action Recognition (UAR)」についての研究。UARの問題をMIL（Multiple Instance Learning）の一般化（GMIL）として扱い、ActivityNetなど大規模動画データから分布推定して表現を獲得。図は提案手法であるCross-Domain UAR (CD-UAR)である。ビデオから抽出したDeep特徴はGMILによりカーネル化される。Word2Vecとの投稿によりURを獲得し、ドメイン変換により新しい概念を獲得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323UAR.png" alt="180323UAR"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来法では見た/見てないの対応関係をデータセット中に含ませていたが、本論文での提案はUniversal Representation（ユニバーサル表現）を獲得して同タスクを解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#260]</div><div class="timestamp">2018.3.23 19:40:06</div></div></section><section id="Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatial-Temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns</div><div class="info"><div class="authors">Jianming Lv, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者の時空間パターンを用いた、教師なし学習の人物再同定アルゴリズムであるTFusionを提案。既存の人物再同定アルゴリズムのほとんどは、小サイズのラベル付きデータセットを用いた教師付き学習手法である。そのため、大規模な実世界のカメラネットワークに適応することは困難である。また、そこで、ラベルなしデータセットも用いたクロスデータセット手法によって精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330TFusion.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>まず、歩行者の空間的-時間的パターンを学習するために、ラベル付きデータセットを用いて学習した視覚的分類器を、ラベルなしデータセットに転送。次に、Bayesian fusion modelによって、学習された時空間パターンを視覚的特徴と組み合わせて、分類器を改善。最後に、ラベルのないデータを用いて分類器を段階的に最適化。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>人物再同定のための、教師なしクロスデータセット学習手法の中では最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li></ul></div></div><div class="slide_index">[#261]</div></div></section><section id="Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatio-temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</div><div class="info"><div class="authors">Jianming, Lv and Weihang, Chen and Qing, Li and Can, Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなし、ドメインが異なる環境に対して人物再同定を行う手法を提案する。モデルであるTFusionは４ステップにより構築（１）教師あり学習により識別器を構築（２）ターゲットであるラベルなしデータにより時空間特徴パターン（Spatio-temporal Pattern）を学習（３）統合モデルFを学習（４）ラベルなしのターゲットデータにて徐々に識別器を学習する（１〜４は図に示されている）。Bayesian Fusionを提案して、時空間特徴パターンと人物のアピアランス特徴を統合してドメイン変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323CDReID.png" alt="180323CDReID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来の人物再同定の設定では比較的小さいデータセットであり、完全に教師ありの環境を想定していたが、本論文ではラベルなし、ドメインが異なる環境に対して人物再同定を実行するため、非常に難しい問題となる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li><li><a href="https://github.com/ahangchen/TFusion">GitHub</a></li></ul></div></div><div class="slide_index">[#262]</div><div class="timestamp">2018.3.23 20:37:22</div></div></section><section id="Unsupervised_Textual_Grounding_Linking_Words_to_Image_Concepts"><div class="paper-abstract"><div class="title">Unsupervised Textual Grounding: Linking Words to Image Concepts</div><div class="info"><div class="authors">Raymond A. Yeh, Minh N. Do, Alexander G. Schwing</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単語を検出された画像の概念に関連付けるための、仮説検定を用いた教師なしTextual grounding手法の提案。ネットワークにはVGG-16を採用し、画像内のオブジェクト/単語の空間情報やクラス情報、およびクラス外の新しい概念を学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401UTG.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>Textual grounding、すなわち画像内のオブジェクトと単語をリンクさせる既存の技法は、教師付きのディープラーニングとして定式化されており、大規模なデータセットを用いてバウンディングボックスを推定する。しかし、データセットの構築には時間やコストがかかるので教師なしの手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ReferIt GameとFlickr30kを用いたベンチマークでそれぞれ7.98％と6.96％以上の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.11185.pdf">論文</a></li></ul></div></div><div class="slide_index">[#263]</div></div></section><section id="Vision-and-Language_Navigation_Interpreting_visually-grounded_navigation_instructions_in_real_environments"><div class="paper-abstract"><div class="title">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</div><div class="info"><div class="authors">Peter Anderson, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180305R2RNavi.png" alt="180305R2RNavi"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>(1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？</p><ul><li><a href="https://arxiv.org/pdf/1711.07280.pdf">論文</a></li><li><a href="https://bringmeaspoon.org/">Project</a></li><li><a href="https://github.com/peteanderson80/Matterport3DSimulator">GitHub</a></li><li><a href="https://niessner.github.io/Matterport/">Matterport3D dataset</a></li></ul></div></div><div class="slide_index">[#264]</div><div class="timestamp">2018.3.5 19:53:46</div></div></section><section id="Weakly-Supervised_Action_Segmentation_with_Iterative_Soft_Boundary_Assignment"><div class="paper-abstract"><div class="title">Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</div><div class="info"><div class="authors">Li Ding, Chenliang Xu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時系列の行動検出/セグメンテーション（Action Segmentation）に関する問題をWeakly-Supervised（WS学習）に解いた。ここではTemporal Convolutional Feature Pyramid Network (TCFPN)とIterative Soft Boundary Assignment (ISBA)を繰り返すことで行動に関する条件学習ができてくるという仕組み。TCFPNではフレームの行動を予測し、ISBAではそれを検証、それらを繰り返して行動間の境界線を定めながらWS学習の教師としていく。さらに、WS学習を促進するためにより弱い境界として行動間の繋がりを定義することでWS学習の精度を向上させる。学習はビデオ単位の誤差を最適化することで境界についても徐々に定まる（ここがWS学習の所以）ように学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329ISBATCFN.png" alt="180329ISBATCFN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Breakfast dataset, Hollywood extended datasetにて弱教師付き学習とテストを行いState-of-the-artな精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱い教師データを大量に集めると、そろそろ（ある程度の）教師ありデータによる精度を超えそう？もっと汎用的に学習できる枠組みが必要か。</p><ul><li><a href="https://arxiv.org/pdf/1803.10699v1.pdf">論文</a></li></ul></div></div><div class="slide_index">[#265]</div><div class="timestamp">2018.3.29 14:27:12</div></div></section><section id="Who_Let_The_Dogs_Out_Modeling_Dog_Behavior_From_Visual_Data"><div class="paper-abstract"><div class="title">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</div><div class="info"><div class="authors">Kiana Ehsani, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>犬視点の大規模ビデオデータセットを作成し、このデータを使用した、犬の行動や行動計画のモデル化。次の3つの問題に焦点を当てる。(1)犬の行動予測。(2)入力された画像対から犬のような行動計画を見出す。(3)例えば、歩行可能な表面推定などのタスクについて、学習された表現を利用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DogsOut.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>視覚情報からintelligent agent(知的エージェント)を直接的にモデリングするタスク。犬の視覚情報を使うことで、行動をモデル化する斬新な取り組み。得られたモデルをAIなどに応用する。特に、歩行可能な表面推定のタスクで良い結果となる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>様々なエージェントやシナリオで使用でき、ラベルがないにもかかわらず有用な情報を学習することが可能。今後は、モデルやデーセットの拡張に挑む。</p><ul><li><a href="https://arxiv.org/pdf/1803.10827.pdf">論文</a></li></ul></div></div><div class="slide_index">[#266]</div></div></section><section id="Zero-shot_Recognition_via_Semantic_Embeddings_and_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs</div><div class="info"><div class="authors">Xiaolong Wang, Yufei Ye, Abhinav Gupta, The Robotics Institute, Carnegie Mellon University</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>カテゴリの単語の埋め込みと他のカテゴリとの関係(視覚データが提供される)を使用するだけで、学習例がないカテゴリの分類器を学習するゼロショット認識モデルを提案。 knowledge graph (KG) を入力とし、Graph Convolutional Network(GCN)を基に、セマンティック埋め込みとカテゴリの関係の両方を使用して分類器を予測する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330KG.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>学習済のKGが与えられると、各ノードに対する意味的埋め込みとして入力を得る。一連のグラフ畳み込みの後、各カテゴリの視覚的分類器を予測する。トレーニング中に、カテゴリの視覚的分類器が与えられ、GCNパラメータを学習。テスト時に、これらのフィルタを使用して、見えないカテゴリの視覚的分類器を予測する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>KGのノイズに対してロバストであり、最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.08035.pdf">論文</a></li></ul></div></div><div class="slide_index">[#267]</div></div></section><section id="Zoom_and_Learn_Generalizing_Deep_Stereo_Matching_to_Novel_Domains"><div class="paper-abstract"><div class="title">Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains</div><div class="info"><div class="authors">Jiahao Pang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>学習済みデータと新しいドメイン(ground-truthなし)の両方を用いて、ディープステレオマッチングを行うZoom and Lean(ZOLE)の提案。これにより，他のドメインに一般化できるプレトレインモデルを作成することができる。一般化に際する不具合を抑制しながらアップサンプリングを行う、反復最適化問題を定式化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330ZOLE.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ground-truthデータが不足しているため、CNNを用いたステレオマッチングでは学習済みステレオモデルを新規ドメインに一般化することが困難とされていた。CNN学習時のイテレーションごとに最適化していくイメージ。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>スマートフォンで収集したデータを従来の手法に入力すると、物体のエッジがぼやけてしまうが、提案手法のZOLEではこれらを改善できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.06641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#268]</div></div></section></div></div><script src="lib/js/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  history: true,
  center: false,
  width: '100%',
  height: '100%',
  transition: 'none',
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});</script></body></html>